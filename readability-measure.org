#+TITLE: Analysis
#+AUTHOR: Roland Coeurjoly
#+EMAIL: rolandcoeurjoly@gmail.com
* Introduction
  In this project we are going to develop a readability measure based on corpus analysis.
** What is a readability measure?
   A readability measure is a set of equations used to quantify how easy a given text is to read.
* Approach
  We are going to approach the problem from a corpus linguistics perspective. This means that we are going to analyze a lot of books and take some general conclusions about them.
  The basic premise used in this project is that vocabulary and readability are correlated positively.

  We will use a literate programming with org mode to make our results are reproducible as possible.
* Corpora
  #+BEGIN_SRC shell
for i in {1..11000}
do
    wget -O $i.epub bibliothequenumerique.tv5monde.com/download/epub/$i
done
find ~/readability-measure/corpus/bibliothequenumerique.tv5monde.com/download/epub/ -name "*.epub" -size -86k -delete
  #+END_SRC

  #+RESULTS:
The following is the perfect script for downloading Chinese books from haodoo (好讀).
It removes those in vertical format with -R "V*.epub".
  #+BEGIN_SRC shell
wget -np -P ~/readability-measure/corpus/Chinese/ -r -A .epub -R "V*.epub" http://haodoo.net/PDB/
  #+END_SRC

#+BEGIN_SRC shell
cd ~/readability-measure/corpus/Chinese/haodoo.net/PDB/
find . -mindepth 1 -type f -name "*.epub" -exec printf x \; | wc -c
find . -mindepth 1 -type f -name "*435*.epub"
#+END_SRC

#+RESULTS:
| 3699          |
| ./A/435.epub  |
| ./D/1435.epub |

#+BEGIN_SRC shell
ls -ltu ~/readability-measure/corpus/Chinese/haodoo.net/PDB/A/*435.epub
#+END_SRC

#+RESULTS:
| -rw-rw-r-- | 1 | rcl | rcl | 130599 | Feb | 23 | 00:07 | /home/rcl/readability-measure/corpus/Chinese/haodoo.net/PDB/A/V435.epub |
| -rw-rw-r-- | 1 | rcl | rcl | 130460 | Feb | 23 | 00:07 | /home/rcl/readability-measure/corpus/Chinese/haodoo.net/PDB/A/435.epub  |
* Analysis
  #+PROPERTY: session *python*
  #+PROPERTY: cache yes
  #+PROPERTY: results none
  In a first instance, we want to extract the following information from each ebook:
  - Author
  - Title
  - Length in number of words
  - Number of unique words
#+BEGIN_SRC python :noweb yes :tangle analysis.py :exports none
# imports
<<imports>>
# main function
<<epub-handling>>
#+END_SRC

#+RESULTS:
: None

** Imports
   We import some packages to make our life easier:
   - ebooklib: to process epubs
   - BeautifulSoup: to process the html in epubs
   - ntlk: to do natural language processing
#+NAME: imports
#+BEGIN_SRC python :session python :results none
import sys
import os
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import nltk
import nltk.tokenize
import codecs
#+END_SRC

** Epub reading

   We then proceed to open the epub and extract all metadata.
   As stated in the [[https://ebooklib.readthedocs.io/en/latest/tutorial.html#reading-epub][package documentation]], only creator, title and language are required metadata fields.
   The rest is optional, so we catch them with care.

   We then use BeautifulSoup to remove all html marks.
#+NAME: epub-handling
#+BEGIN_SRC python :noweb yes :session python
path = "/home/rcl/readability-measure/corpus/Chinese/haodoo.net/PDB/"
f = open("/home/rcl/readability-measure/corpus/Chinese/chinese.tsv", "w+")
i = 1
for dirpath, dirnames, files in os.walk(path):
    for ebook in files:
        print "Reading ebook " + ebook + ", number  " + str(i)
        try:
            book = epub.read_epub(dirpath + "/" + ebook)
            <<get-epub-metadata>>
            <<text-extraction>>
            <<nltk-processing>>
            print >>f, str(filesize) + "\t" + str(lexicalVariety)
            i += 1
        except:
            pass
f.close()
#+END_SRC

#+RESULTS: epub-handling

#+NAME: text-extraction
#+BEGIN_SRC python :session python :noweb yes
cleantext = ""
for item in book.get_items():
    if item.get_type() == ebooklib.ITEM_DOCUMENT:
        raw_html = item.get_content()
        <<html-cleaning>>
#+END_SRC

#+RESULTS: text-extraction

#+NAME: html-cleaning
#+BEGIN_SRC python :session python
cleantext += BeautifulSoup(raw_html, "lxml").text
#+END_SRC

#+RESULTS: html-cleaning

#+NAME: get-epub-metadata
#+BEGIN_SRC python
try:
    type = book.get_metadata('DC', 'type')[0][0].encode('utf-8')
except:
    pass
try:
    subject = book.get_metadata('DC', 'subject')[0][0].encode('utf-8')
except:
    pass
try:
    source = book.get_metadata('DC', 'source')[0][0].encode('utf-8')
except:
    pass
try:
    rights = book.get_metadata('DC', 'rights')[0][0].encode('utf-8')
except:
    pass
try:
    relation = book.get_metadata('DC', 'relation')[0][0].encode('utf-8')
except:
    pass
try:
    publisher = book.get_metadata('DC', 'publisher')[0][0].encode('utf-8')
except:
    pass
try:
    language = book.get_metadata('DC', 'language')[0][0].encode('utf-8')
except:
    pass
try:
    identifier = book.get_metadata('DC', 'identifier')[0][0].encode('utf-8')
except:
    pass
try:
    format = book.get_metadata('DC', 'format')[0][0].encode('utf-8')
except:
    pass
try:
    description = book.get_metadata('DC', 'description')[0][0].encode('utf-8')
except:
    pass
try:
    coverage = book.get_metadata('DC', 'coverage')[0][0].encode('utf-8')
except:
    pass
try:
    contributor = book.get_metadata('DC', 'contributor')[0][0].encode('utf-8')
except:
    pass
try:
    creator = book.get_metadata('DC', 'creator')[0][0].encode('utf-8')
except:
    pass
try:
    title = book.get_metadata('DC', 'title')[0][0].encode('utf-8')
except:
    pass
try:
    date = book.get_metadata('DC', 'date')[0][0].encode('utf-8')
except:
    pass
#+END_SRC
#+NAME: nltk-processing
#+BEGIN_SRC python :session python
if (language != 'zh-TW'):
    tokens = nltk.tokenize.word_tokenize(cleantext)
else:
    tokens = ''.join(c for c in cleantext if u'\u4e00' <= c <= u'\u9fff')
filesize = len(tokens)
lexicalVariety = len(set(tokens))
#+END_SRC

#+RESULTS: nltk-processing

#+NAME: test-western
#+BEGIN_SRC python :session python :results output
print "Size: " + str(filesize)
print "Lexical variety: " + str(lexicalVariety)
txt_text = codecs.open(
    str("/home/rcl/readability-measure/corpus/440.txt"),
    'r',
    'utf-8-sig',
    'ignore').read()
txt_tokens = nltk.tokenize.word_tokenize(txt_text)
txt_filesize = len(txt_tokens)
txt_lexicalVariety = len(set(txt_tokens))
print "TXT Size: " + str(txt_filesize)
print "TXT Lexical variety: " + str(txt_lexicalVariety)
#+END_SRC

#+NAME: test-chinese
#+BEGIN_SRC python :session python :results output
print "Size: " + str(filesize)
print "Lexical variety: " + str(lexicalVariety)
txt_tokens = ''.join(c for c in codecs.open(
    str("/home/rcl/readability-measure/test/17F0b.txt"),
    'r',
    'utf-8-sig',
    'ignore').read() if u'\u4e00' <= c <= u'\u9fff')
txt_filesize = len(txt_tokens)
txt_lexicalVariety = len(set(txt_tokens))
print "TXT Size: " + str(txt_filesize)
print "TXT Lexical variety: " + str(txt_lexicalVariety)
#+END_SRC

  #+RESULTS:
  : Traceback (most recent call last):
  :   File "<stdin>", line 1, in <module>
  :   File "/tmp/babel-vpxI7x/python-9FEIgK", line 1, in <module>
  :     print "Size: " + str(filesize)
  : NameError: name 'filesize' is not defined

** Trial gnuplot

#+RESULTS:
#+NAME: output
#+BEGIN_SRC shell :file output.dat replace
python ~/readability-measure/analysis.py
#+END_SRC

#+RESULTS: output
[[file:output.dat]]
68539	5664
]]

#+header: :stdin output
#+BEGIN_SRC gnuplot
plot 'lexicalVarietyVsLogLength.tsv'
plot 'lexicalVarietyVsLength.tsv'
plot '/home/rcl/readability-measure/corpus/Chinese/chinese.tsv'
#+END_SRC

#+RESULTS:

#+BEGIN_SRC gnuplot :file chinese.png
plot '/home/rcl/readability-measure/corpus/Chinese/chineseLog.tsv'
#+END_SRC

#+RESULTS:
[[file:chinese.png]]

#+BEGIN_SRC python :results none
import math
file = open("/home/rcl/readability-measure/corpus/Chinese/chinese.tsv", "r")
logfile = open("/home/rcl/readability-measure/corpus/Chinese/chineseLog.tsv", "w")
for line in file:
    print line.rsplit('\t', 1)[0]
    print math.log(float(line.rsplit('\t', 1)[0]), 10)
    logfile.write(str(math.log(float(line.rsplit('\t', 1)[0]), 2)) + "\t" + str(line.rsplit('\t', 1)[1]) + '\n')
    print '\n'
file.close()
logfile.close()
#+END_SRC
