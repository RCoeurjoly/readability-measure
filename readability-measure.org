#+TITLE: Readability measure based on analysis of lexical variety

#+AUTHOR: Roland Coeurjoly
#+EMAIL: rolandcoeurjoly@gmail.com
#+EXPORT_FILE_NAME: readability_measure

* Introduction
  In this project we are going to develop a readability measure based on corpus analysis.
** What is a readability measure?
   A readability measure is a set of equations used to quantify how easy a given text is to read.
   In this project, we focus on long texts, with more that 2000 words or tokens. This is in contrast with other readability measures which analyze sentences.
* Approach
  We are going to approach the problem from a corpus linguistics perspective. This means that we are going to analyze a lot of books and take some general conclusions about them.
  The basic premise used in this project is that vocabulary and readability are correlated positively.

  We will use a literate programming with org mode to make our results as reproducible as possible.
* Corpora
** French
  #+BEGIN_SRC shell :exports code
for i in {1..11000}
do
    wget -O $i.epub bibliothequenumerique.tv5monde.com/download/epub/$i
done
find ~/readability-measure/corpus/bibliothequenumerique.tv5monde.com/download/epub/ -name "*.epub" -size -86k -delete
  #+END_SRC

  #+RESULTS:
** Chinese
The following is the perfect script for downloading Chinese books from haodoo (好讀).
It removes those in vertical format with -R "V*.epub".
  #+BEGIN_SRC shell :exports code
wget -np -P ~/readability-measure/corpus/Chinese/ -r -A .epub -R "V*.epub" http://haodoo.net/PDB/
  #+END_SRC

#+BEGIN_SRC shell :exports code
find ~/readability-measure/corpus/Chinese/haodoo.net/PDB/ -mindepth 1 -type f -name "*.epub" -exec printf x \; | wc -c
#+END_SRC

#+RESULTS:
: 3699

#+BEGIN_SRC shell :exports code
ls -ltu ~/readability-measure/corpus/Chinese/haodoo.net/PDB/A/*435.epub
#+END_SRC

#+RESULTS:
: -rw-rw-r-- 1 rcl rcl 130460 jul 19 16:04 /home/rcl/readability-measure/corpus/Chinese/haodoo.net/PDB/A/435.epub
** Arabic
   [[http://www.hindawi.
org/][http://www.hindawi.org/]]
   #+BEGIN_SRC shell :exports code
wget -np -P ~/readability-measure/corpus/Arabic/ -r -A .epub http://www.hindawi.org/books/
   #+END_SRC
** Russian
   #+BEGIN_SRC shell :exports code
wget -np -P ~/readability-measure/corpus/Russian/ -r -A .epub https://www.rulit.me/
   #+END_SRC
** All
   The Gutenberg project is also a good source of books.
   We could create different files for each language with length and lexical variety.
   Also, a convenient feature of the Gutenberg library is that it has ebooks with images and without.
   We download without images to save bandwidth.
   #+BEGIN_SRC shell :exports code
wget -w 2 -m -H "http://www.gutenberg.org/robot/harvest?filetypes[]=epub.noimages"
   #+END_SRC
* Analysis
  #+PROPERTY: session *python*
  #+PROPERTY: cache yes
  #+PROPERTY: results none
  In a first instance, we want to extract the following information from each ebook:
  - Author
  - Title
  - Length in number of words
  - Number of unique words
  It would be nice to create a file for each language (according to metadata).
  The logic would be the following:
  Try adding the results to a file suffixed with the language code.
  If that throws an exception, create the file and add the results
#+BEGIN_SRC python :noweb yes :tangle corpus-analysis.py :exports code
# imports
<<imports>>
# main function
<<epub-handling>>
#+END_SRC

#+RESULTS:
: None

** Imports
   We import some packages to make our life easier:
   - ebooklib: to process epubs
   - BeautifulSoup: to process the html in epubs
   - langdetect to detect language. We use this because based on experience epub language tags are not very reliable
   - ntlk: to do natural language processing
#+NAME: imports
#+BEGIN_SRC python :session python :results none :exports code
# -*- coding: utf-8 -*-
import sys
import os
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import nltk
import nltk.tokenize
import codecs
from scipy.optimize import curve_fit
from scipy import log as log
import numpy as np
import pandas as pd
from langdetect import detect
from langdetect import DetectorFactory
import mysql.connector
#+END_SRC

** Epub reading

   We then proceed to open the epub and extract all metadata.
   As stated in the [[https://ebooklib.readthedocs.io/en/latest/tutorial.html#reading-epub][package documentation]], only creator, title and language are required metadata fields.
   The rest is optional, so we catch them with care.

   We then use BeautifulSoup to remove all html marks.
*** Finding ebooks
 #+NAME: epub-handling
 #+BEGIN_SRC python :noweb yes :session python :exports code
i = 1
for dirpath, dirnames, files in os.walk(str(sys.argv[1])):
    for ebook in files:
        if file.endswith(".epub"):
            print "Reading ebook " + ebook + ", number  " + str(i)
            try:
                book = epub.read_epub(dirpath + "/" + ebook)
                print "Getting epub metadata"
                <<get-epub-metadata>>
                print "Extracting text from ebook"
                <<text-extraction>>
                print "Detecting language"
                <<language-detection>>
                print "Performing tokenization"
                <<nltk-tokenization>>
                #<<lexical-sweep>>
                #<<log-writing>>
                i += 1
            except:
                pass
mydb.close()
 #+END_SRC

 #+RESULTS: epub-handling
*** Extracting text from ebook
#+NAME: text-extraction
#+BEGIN_SRC python :noweb yes :session python :exports code
cleantext = ""
for item in book.get_items():
    if item.get_type() == ebooklib.ITEM_DOCUMENT:
        raw_html = item.get_content()
        <<html-cleaning>>
#+END_SRC

#+RESULTS: text-extraction
**** Cleaning the html
#+NAME: html-cleaning
#+BEGIN_SRC python :noweb yes :session python :exports code
cleantext += BeautifulSoup(raw_html, "lxml").text
#+END_SRC

#+RESULTS: html-cleaning
**** Language detection
     According to the [[https://github.com/Mimino666/langdetect][README from the github page]]:
     #+begin_quote
     Language detection algorithm is non-deterministic, which means that if you try to run it on a text which is either too short or too ambiguous, you might get different results everytime you run it.

     To enforce consistent results, call following code before the first language detection:
     #+end_quote
#+NAME: language-detection
#+begin_src python :noweb yes :session python :exports code
DetectorFactory.seed = 0
language=detect(cleantext)
#+end_src

*** Extracting metadata
#+NAME: get-epub-metadata
#+BEGIN_SRC python :noweb yes :session python :exports code
try:
    type = book.get_metadata('DC', 'type')[0][0].encode('utf-8')
except:
    type = '-'
try:
    subject = book.get_metadata('DC', 'subject')[0][0].encode('utf-8')
except:
    subject = '-'
try:
    source = book.get_metadata('DC', 'source')[0][0].encode('utf-8')
except:
    source = '-'
try:
    rights = book.get_metadata('DC', 'rights')[0][0].encode('utf-8')
except:
    rights = '-'
try:
    relation = book.get_metadata('DC', 'relation')[0][0].encode('utf-8')
except:
    relation = '-'
try:
    publisher = book.get_metadata('DC', 'publisher')[0][0].encode('utf-8')
except:
    publisher = '-'
#try:
#    language = book.get_metadata('DC', 'language')[0][0].encode('utf-8')
#except:
#    language = 'empty'
try:
    identifier = book.get_metadata('DC', 'identifier')[0][0].encode('utf-8')
except:
    identifier = '-'
try:
    format = book.get_metadata('DC', 'format')[0][0].encode('utf-8')
except:
    format = '-'
try:
    description = book.get_metadata('DC', 'description')[0][0].encode('utf-8')
except:
    description = '-'
try:
    coverage = book.get_metadata('DC', 'coverage')[0][0].encode('utf-8')
except:
    coverage = '-'
try:
    contributor = book.get_metadata('DC', 'contributor')[0][0].encode('utf-8')
except:
    contributor = '-'
try:
    creator = book.get_metadata('DC', 'creator')[0][0].encode('utf-8')
except:
    creator = '-'
try:
    title = book.get_metadata('DC', 'title')[0][0].encode('utf-8')
except:
    title = '-'
try:
    date = book.get_metadata('DC', 'date')[0][0].encode('utf-8')
except:
    date = '-'
#+END_SRC
*** Tokenization
#+NAME: nltk-tokenization
#+BEGIN_SRC python :noweb yes :session python :exports code
if (language != 'zh-TW' or language != 'zh-tw' or language != 'zh-cn'):
    tokens = nltk.tokenize.word_tokenize(cleantext)
else:
    tokens = ''.join(c for c in cleantext if u'\u4e00' <= c <= u'\u9fff')
filesize = len(tokens)
lexicalVariety = len(set(tokens))
#+END_SRC
*** Log writing
#+NAME: log-writing
#+BEGIN_SRC python :noweb yes :session python :exports code
with open("/home/rcl/readability-measure/test/"
          + str(language)
          + ".tsv", "a+") as myfile:
    myfile.write(str(filesize) + "\t"
                 + str(lexicalVariety) + "\t"
                 + str(intercept) + "\t"
                 + str(slope) + "\t"
                 + str(language) + "\t"
                 + str(creator) + "\t"
                 + str(title) + "\t"
                 + str(type) + "\t"
                 + str(subject) + "\t"
                 + str(source) + "\t"
                 + str(rights) + "\t"
                 + str(relation) + "\t"
                 + str(publisher) + "\t"
                 + str(identifier) + "\t"
                 + str(format) + "\t"
                 # + str(description) + "\t"
                 + str(contributor) + "\t"
                 + str(date) + "\n")
#+END_SRC

** Curve fitting
   We can only do the curve fitting with books longer than 10000 tokens. This is because, to begin with, books don't exhibit logarithmic behavior until they reach around 4 thousand words.
   Moreover, we need to have enough samples to be able to feed the piece of software that does the curve fitting.
#+NAME: lexical-sweep
#+BEGIN_SRC python :noweb yes :session python :exports code
start = 5000
samples = 500

sweep_values = []
if filesize > 10000:
    for j in xrange(0, len(tokens) - start, (len(tokens) - start)/samples):
        sweep_values.append([len(tokens[0:start + j]), len(set(tokens[0:start + j]))])
    <<curve-fitting>>
else:
    intercept = '-'
    slope = '-'
#+END_SRC

#+RESULTS: lexical-sweep

   #+NAME: curve-fitting
   #+begin_src python :noweb yes :session python :exports code
t =  list(zip(*sweep_values))
xarr = t[0]
yarr = t[1]

a = 0
b = 0

def log_func(x, a, b):
    return (a + b*log(x))

popt, pcov = curve_fit(log_func,  xarr, yarr, (a,b))
intercept = popt[0]
slope = popt[1]
perr = np.sqrt(np.diag(pcov))
std_error_intercept=perr[0]
std_error_slope=perr[1]
print popt, pcov, perr
      #+end_src

      #+RESULTS:
      : a = -5813.118832427114 , b = 761.1560740930518

** Tagging
   The purpose of this section is to tag the lists containing the analysis with the canon to which they belong, if appropriate.
   #+begin_src bash :tangle canon-tagging.sh :exports code
canon="/home/rcl/readability-measure/canon/chinese.txt"
analized="/home/rcl/readability-measure/tagging/zh-TW.tsv"
list=""
while read -r author_canon title_canon; do
        list+=$author_canon
        list+=" "
done < "$canon"
unique_authors=$(tr ' ' '\n' <<< $list | sort -u)
echo $unique_authors
while read -r filesize lexicalVariety intercept slope language author_list title_list type subject source rights relation publisher identifier format contibutor date; do
    flag=0
    while read -r author_canon title_canon; do
        if [ "$author_list" == "$author_canon" ] && [ "$title_list" == "$title_canon" ]; then
            #printf '%s %s Canon match!!\n' "$author_list" "$title_list"
            flag=1
        fi
    done < "$canon"
    for word in $unique_authors; do
        if [ "$author_list" == "$word" ] && [ "$flag" != 1 ]; then
            #printf '%s %s Extended canon match!!\n' "$author_list" "$title_list"
        fi
    done
done < "$analized"
   #+end_src

   #+begin_src bash
linewriting="/home/rcl/readability-measure/linewriting.txt"
touch $linewriting
echo "roland coeurjoly" > $linewriting
echo "chun zhang" >> $linewriting

while read line; do
    if [[ $line = *"chun zhang"* ]]; then
        #echo "substring found!"
        echo
    fi
done < "$linewriting"
less $linewriting
   #+end_src
   #+begin_src python :results output
# -*- coding: utf-8 -*-
import numpy
import csv
canon_file="/home/rcl/readability-measure/canon/chinese.txt"
analysis_file="/home/rcl/readability-measure/tagging/zh-TW.tsv"
canon = numpy.array(list(csv.reader(open(canon_file, "rb"), delimiter=" "))).astype("object")
analysis = numpy.array(list(csv.reader(open(analysis_file, "rb"), delimiter="\t"))).astype("object")
print canon[90][0]
print analysis[90][5]
   #+end_src
   #+RESULTS:
   : 古龍
   : 東野圭吾

* Plotting

Perfect. It plots the first two columns and doesn't give an error about all the rest.
#+BEGIN_SRC gnuplot :exports both all_.png
set title "Lexical variety Vs Length"
set xlabel "Length in words"
set ylabel "Unique words"
set logscale x
set logscale y
es_filelist=system("ls es*.tsv")
fr_filelist=system("ls fr*.tsv")
pt_filelist=system("ls p*.tsv")
plot  for [filename in es_filelist] filename title 'Spanish' linecolor 1, \
      for [filename in fr_filelist] filename title 'French' linecolor 2, \
      for [filename in pt_filelist] filename title 'Portuguese' linecolor 3, \
      'ar.tsv' title 'Arabic' linecolor 4, \
      'zh-TW.tsv' title 'Chinese' linecolor 5
#+END_SRC

#+RESULTS:
[[file:languages.png]]

#+BEGIN_SRC gnuplot :exports both :file chinese.png
set title "Lexical variety Vs Length"
set xlabel "Length in characters"
set ylabel "Unique characters"
set logscale x
set nologscale y
plot 'zh-TW.tsv' title 'Chinese' linecolor 1
#+END_SRC

#+RESULTS:

#+BEGIN_SRC gnuplot :exports both :file arabic.png
set title "Lexical variety Vs Length"
set xlabel "Length in characters"
set ylabel "Unique characters"
set logscale x
set logscale y
plot 'ar.tsv' title 'Arabic' linecolor 1
#+END_SRC

#+BEGIN_SRC gnuplot :exports both :file all.png
set multiplot
set title "Lexical variety Vs Length"
set xlabel "Length in words"
set ylabel "Unique words"
#set logscale x
#set logscale y
set logscale x
set logscale y
filelist=system("ls *.tsv")
#plot  for [filename in filelist] filename title filename
plot 'spanish.tsv' title 'Spanish' linecolor 1, \
     'french.tsv' title 'French' linecolor 2, \
     'portuguese.tsv' title 'Portuguese' linecolor 3, \
     'ar.tsv' title 'Arabic' linecolor 4, \
     for [filename in filelist] filename title filename linecolor 5
unset multiplot
#+END_SRC

#+RESULTS:
[[file:all.png]]
* Fitting points to function
  The purpose of this section is to fit all the different points to a function
  | Minimum length (characters) |         R^2 |
  |-----------------------------+-------------|
  |                           0 | 0.743868489 |
  |                       20000 |        0.71 |
  |                             |             |
  #+BEGIN_SRC python
for i in xrange(0,lexicalVariety,1000):
  print(i)
  #+END_SRC

  #+RESULTS:

#+BEGIN_SRC gnuplot :exports both :file sweep.png
set multiplot
set encoding utf8
set title "Lexical variety Vs Length"
set xlabel "Length in characters"
set ylabel "Unique characters"
set logscale x
set nologscale y
plot '/home/rcl/readability-measure/test/0936.tsv' title 'Jipin Jiading' linecolor 1, \
     '/home/rcl/readability-measure/test/1077-4000.tsv' title 'Cixi Quanzhuan' linecolor 2
     #'/home/rcl/readability-measure/zh-TW.tsv' title 'Chinese' linecolor 3
unset multiplot
#+END_SRC

#+RESULTS:
[[file:sweep.png]]


#+BEGIN_SRC gnuplot :exports both :file test.png
set multiplot
set encoding utf8
set title "Lexical variety Vs Length"
set xlabel "Length in characters"
set ylabel "Unique characters"
set logscale x
set nologscale y
plot '/home/rcl/readability-measure/zh-TW.tsv' title 'Jipin Jiading' linecolor 1, \
     #'/home/rcl/readability-measure/zh-TW.tsv' title 'Cixi Quanzhuan' linecolor 2
     #'/home/rcl/readability-measure/zh-TW.tsv' title 'Chinese' linecolor 3
unset multiplot
#+END_SRC

#+RESULTS:
[[file:test.png]]

#+BEGIN_SRC R :file R.png :results output graphics
dat <- read.csv("~/readability-measure/zh-TW.tsv", header=FALSE, sep="\t")
x = dat[, 1]
y = dat[, 2]

Estimate = lm(y ~ x)
logEstimate = lm(y ~ log(x))

plot(x,predict(Estimate),type='l',col='blue')
lines(x,predict(logEstimate),col='red')
plot(x, y, log ="x",
        type="p",
        pch = 1,
        xlab="Length (characters)",
        ylab="Unique characters (characters)")
#+END_SRC

#+RESULTS:
[[file:R.png]]

#+begin_src R :file 3.png :results output graphics
library(lattice)
xyplot(1:10 ~ 1:10)
#+end_src

#+RESULTS:
[[file:3.png]]
* SQL DB
#+header: :engine mysql
#+header: :dbuser root
#+header: :dbpassword root
#+header: :database fiction
#+begin_src sql
SELECT DISTINCT Language FROM main;
#+end_src

#+RESULTS:
| Tables_in_fiction |
|-------------------|
| hashes            |
| main              |
| main_edited       |

#+begin_src python :noweb yes :session python :exports code
mydb = mysql.connector.connect(
  host="localhost",
  user="root",
  passwd="root",
  charset='utf8'
)

mycursor = mydb.cursor()

try:
    mycursor.execute("CREATE DATABASE library")
except:
    mycursor.execute("USE library;")

try:
    mycursor.execute("""CREATE TABLE corpus (id INT AUTO_INCREMENT PRIMARY KEY,
    title VARCHAR(255),
    author VARCHAR(255),
    slope DECIMAL(5,3),
    intercept DECIMAL(5,3),
    std_error_slope DECIMAL(5,3),
    std_error_intercept DECIMAL(5,3),
    word_count DECIMAL(10,3),
    lexical_variety DECIMAL(10,3),
    language VARCHAR(255),
    type VARCHAR(255),
    subject VARCHAR(255),
    source VARCHAR(255),
    rights VARCHAR(255),
    relation VARCHAR(255),
    publisher VARCHAR(255),
    identifier VARCHAR(255),
    format VARCHAR(255),
    description VARCHAR(510),
    contributor VARCHAR(255),
    date VARCHAR(255))""")
except:
    pass

mycursor.execute("ALTER DATABASE library CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;")
mycursor.execute("ALTER TABLE corpus CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;")

try:
    mycursor.execute("ALTER TABLE corpus ADD CONSTRAINT unique_book UNIQUE (title,author);")
except:
    pass

sql = """INSERT IGNORE corpus (title,
author,
slope,
intercept,
std_error_slope,
std_error_intercept,
word_count,
lexical_variety,
language,
type,
subject,
source,
rights,
relation,
publisher,
identifier,
format,
description,
contributor,
date
) VALUES (%s,
%s,
%d,
%d,
%d,
%d,
%d,
%d,
%s,
%s,
%s,
%s,
%s,
%s,
%s,
%s,
%s,
%s,
%s,
%s)"""
val = (title,
author,
slope,
intercept,
std_error_slope,
std_error_intercept,
word_count,
lexical_variety,
language,
type,
subject,
source,
rights,
relation,
publisher,
identifier,
format,
description,
contributor,
date)

try:
    mycursor.execute(sql, val)
    mydb.commit()
    print("1 record inserted, ID:", mycursor.lastrowid)
except:
    pass
#+end_src

#+RESULTS:
#+NAME: does-book-exist-db
#+begin_src python :noweb yes :session python :exports code
query = "SELECT * from corpus where MemberID="
tmp_query=query+loginID
mycursor.execute(tmp_query)
if cursor.rowcount==1:
        ## Do your thing here
        break
#+end_src
