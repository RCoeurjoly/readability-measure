#+TITLE: Readability measure based on analysis of lexical variety

#+AUTHOR: Roland Coeurjoly
#+EMAIL: rolandcoeurjoly@gmail.com
#+EXPORT_FILE_NAME: readability_measure

* Introduction
  In this project we are going to develop a readability measure based on corpus analysis.
** What is a readability measure?
   A readability measure is a set of equations used to quantify how easy a given text is to read.
   In this project, we focus on long texts, with more that 2000 words or tokens. This is in contrast with other readability measures which analyze sentences.
* Literature review
  Alexander Arguelles proposes the following:

* Approach
  We are going to approach the problem from a corpus linguistics perspective. This means that we are going to analyze a lot of books and take some general conclusions about them.
  The basic premise used in this project is that vocabulary and readability are correlated negatively, i.e., the larger the vocabulary, the more difficult to understand.

  We will use a literate programming approach using GNU/Emacs org mode to make our results as reproducible as possible.
* Corpora
** French
  #+BEGIN_SRC shell :exports code :tangle french-corpus.sh
for i in {1..11000}
do
    wget -O $1$i.epub bibliothequenumerique.tv5monde.com/download/epub/$i
    find ${1} -name "*.epub" -size -88k -delete
done
  #+END_SRC

  #+RESULTS:
** Chinese
The following is the perfect script for downloading Chinese books from haodoo (好讀).
It removes those in vertical format with -R "V*.epub".
  #+BEGIN_SRC shell :exports code :tangle chinese-corpus.sh
wget -np -P $1 -r -A .epub -R "V*.epub" http://haodoo.net/PDB/
  #+END_SRC

#+BEGIN_SRC shell :exports code
find ~/readability-measure/corpus/Chinese/haodoo.net/PDB/ -mindepth 1 -type f -name "*.epub" -exec printf x \; | wc -c
#+END_SRC

#+RESULTS:
: 3699

#+BEGIN_SRC shell :exports code
ls -ltu ~/readability-measure/corpus/Chinese/haodoo.net/PDB/A/*435.epub
#+END_SRC

#+RESULTS:
: -rw-rw-r-- 1 rcl rcl 130460 jul 19 16:04 /home/rcl/readability-measure/corpus/Chinese/haodoo.net/PDB/A/435.epub
** Arabic
   [[http://www.hindawi.
org/][http://www.hindawi.org/]]
   #+BEGIN_SRC shell :exports code :tangle arabic-corpus.sh
wget -np -P $1 -r -A .epub http://www.hindawi.org/books/
   #+END_SRC
** Russian
   #+BEGIN_SRC shell :exports code
wget -np -P ~/readability-measure/corpus/Russian/ -r -A .epub https://www.rulit.me/
   #+END_SRC
** All
   The Gutenberg project is also a good source of books.
   We could create different files for each language with length and lexical variety.
   Also, a convenient feature of the Gutenberg library is that it has ebooks with images and without.
   We download without images to save bandwidth.
   #+BEGIN_SRC shell :exports code
wget -w 2 -m -H "http://www.gutenberg.org/robot/harvest?filetypes[]=epub.noimages"
   #+END_SRC
* Analysis
  #+PROPERTY: session *python*
  #+PROPERTY: cache yes
  #+PROPERTY: results none
  In a first instance, we want to extract the following information from each ebook:
  - Author
  - Title
  - Length in number of words
  - Number of unique words
  It would be nice to create a file for each language (according to metadata).
  The logic would be the following:
  Try adding the results to a file suffixed with the language code.
  If that throws an exception, create the file and add the results
#+BEGIN_SRC python :noweb yes :tangle corpus_analysis.py :exports code
# -*- coding: utf-8 -*-
'''
corpus-analysis.py: readability measure for epub ebooks.
Version 1.0
Copyright (C) 2019  Roland Coeurjoly <rolandcoeurjoly@gmail.com>
'''
# imports
<<imports>>
# main function
<<epub-handling>>
#+END_SRC

#+RESULTS:
: None

** Imports
   We import some packages to make our life easier:
   - ebooklib: to process epubs
   - BeautifulSoup: to process the html in epubs
   - langdetect to detect language. We use this because based on experience epub language tags are not very reliable
   - ntlk: to do natural language processing
#+NAME: imports
#+BEGIN_SRC python :session python :results none :exports code
import unicodedata
import sys
import os
import math
import subprocess
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
from scipy.optimize import curve_fit
from scipy import log as log
import numpy as np
import mysql.connector
from polyglot.text import Text
#+END_SRC

** Epub reading

   We then proceed to open the epub and extract all metadata.
   As stated in the [[https://ebooklib.readthedocs.io/en/latest/tutorial.html#reading-epub][package documentation]], only creator, title and language are required metadata fields.
   The rest is optional, so we catch them with care.

   We then use BeautifulSoup to remove all html marks.
*** Finding ebooks
 #+NAME: epub-handling
 #+BEGIN_SRC python :noweb yes :session python :exports code
i = 1
<<fit-functions>>
<<curve-fitting>>
<<db-connection>>
<<db-backup>>
<<utf8-cleaning>>
for dirpath, dirnames, files in os.walk(str(sys.argv[1])):
    for ebook in files:
        if ebook.endswith(".epub"):
            print "Reading ebook " + ebook + ", number  " + str(i)
            try:
                book = epub.read_epub(dirpath + "/" + ebook)
            except Exception as ex:
                print ex
                raise
            print "Getting epub metadata"
            <<get-epub-metadata>>
            print "Checking if book exists in database"
            <<does-book-exist-db>>
            print "Extracting text from ebook"
            <<text-extraction>>
            print "Detecting language"
            <<language-detection>>
            print "Language detected: " + str(language_detected)
            print "Performing tokenization"
            <<tokenization>>
            print "Lexical sweep"
            <<lexical-sweep>>
            print "Writing to database"
            <<database-insertion>>
            i += 1
            runbackup("localhost", "root", "root")
MY_DB.close()
 #+END_SRC

 #+RESULTS: epub-handling
*** Extracting text from ebook
#+NAME: text-extraction
#+BEGIN_SRC python :noweb yes :session python :exports code
cleantext = ""
for item in book.get_items():
    if item.get_type() == ebooklib.ITEM_DOCUMENT:
        raw_html = item.get_content()
        <<html-cleaning>>
#+END_SRC

#+RESULTS: text-extraction
**** Cleaning the html
#+NAME: html-cleaning
#+BEGIN_SRC python :noweb yes :session python :exports code
cleantext += BeautifulSoup(raw_html, "lxml").text
#+END_SRC

#+RESULTS: html-cleaning
**** Removing invalid utf-8

#+NAME: utf8-cleaning
#+BEGIN_SRC python :noweb yes :session python :exports code
PRINTABLE = {
    #'Cc',
    'Cf',
    'Cn',
    'Co',
    'Cs',
    'LC',
    'Ll',
    'Lm',
    'Lo',
    'Lt',
    'Lu',
    'Mc',
    'Me',
    'Mn',
    'Nd',
    'Nl',
    'No',
    'Pc',
    'Pd',
    'Pe',
    'Pf',
    'Pi',
    'Po',
    'Ps',
    'Sc',
    'Sk',
    'Sm',
    'So',
    'Zl',
    'Zp',
    'Zs'
}
def filter_non_printable(text):
    '''
    Remove all non printable characters from string.
    '''
    return ''.join(character for character in text if unicodedata.category(character) in PRINTABLE)
#+END_SRC
**** Language detection
#+NAME: language-detection
#+begin_src python :noweb yes :session python :exports code
cleantext = filter_non_printable(cleantext)
language_detected = Text(cleantext).language.code
#+end_src

*** TODO Extracting metadata
#+NAME: get-epub-metadata
#+BEGIN_SRC python :noweb yes :session python :exports code
try:
    epubType = book.get_metadata('DC', 'type')[0][0].encode('utf-8')
except IndexError:
    epubType = ''
try:
    subject = book.get_metadata('DC', 'subject')[0][0].encode('utf-8')
except IndexError:
    subject = ''
try:
    source = book.get_metadata('DC', 'source')[0][0].encode('utf-8')
except IndexError:
    source = ''
try:
    rights = book.get_metadata('DC', 'rights')[0][0].encode('utf-8')
except IndexError:
    rights = ''
try:
    relation = book.get_metadata('DC', 'relation')[0][0].encode('utf-8')
except IndexError:
    relation = ''
try:
    publisher = book.get_metadata('DC', 'publisher')[0][0].encode('utf-8')
except IndexError:
    publisher = ''
#try:
#    language = book.get_metadata('DC', 'language')[0][0].encode('utf-8')
#except IndexError:
#    language = 'empty'
try:
    identifier = book.get_metadata('DC', 'identifier')[0][0].encode('utf-8')
except IndexError:
    identifier = ''
try:
    epubFormat = book.get_metadata('DC', 'format')[0][0].encode('utf-8')
except IndexError:
    epubFormat = ''
try:
    description = book.get_metadata('DC', 'description')[0][0].encode('utf-8')
except IndexError:
    description = ''
try:
    coverage = book.get_metadata('DC', 'coverage')[0][0].encode('utf-8')
except IndexError:
    coverage = ''
try:
    contributor = book.get_metadata('DC', 'contributor')[0][0].encode('utf-8')
except IndexError:
    contributor = ''
try:
    author = book.get_metadata('DC', 'creator')[0][0].encode('utf-8')
except IndexError:
    author = ''
try:
    title = book.get_metadata('DC', 'title')[0][0].encode('utf-8')
except IndexError:
    title = ''
try:
    date = book.get_metadata('DC', 'date')[0][0].encode('utf-8')
except IndexError:
    date = ''
#+END_SRC
*** Tokenization
    If the language is Chinese, appart from doing the tokenization, we also measure individual characters.
#+NAME: tokenization
#+BEGIN_SRC python :noweb yes :session python :exports code
character_count = int()
unique_characters = int()
if language_detected == 'zh' or language_detected == 'zh_Hant':
    zh_characters = ''.join(c for c in cleantext if u'\u4e00' <= c <= u'\u9fff')
    character_count = len(zh_characters)
    unique_characters = len(set(zh_characters))
tokens = Text(cleantext).words
word_count = len(tokens)
unique_words = len(set(tokens))
#+END_SRC
*** Log writing
#+NAME: log-writing
#+BEGIN_SRC python :noweb yes :session python :exports code
with open("/home/rcl/readability-measure/test/"
          + str(language_detected)
          + ".tsv", "w") as myfile:
    myfile.write(str(wordCount) + "\t"
                 + str(uniqueWords) + "\t"
                 #+ str(intercept) + "\t"
                 #+ str(slope) + "\t"
                 + str(language_detected) + "\t"
                 + str(author) + "\t"
                 + str(title) + "\t"
                 + str(epubType) + "\t"
                 + str(subject) + "\t"
                 + str(source) + "\t"
                 + str(rights) + "\t"
                 + str(relation) + "\t"
                 + str(publisher) + "\t"
                 + str(identifier) + "\t"
                 + str(epubFormat) + "\t"
                 # + str(description) + "\t"
                 + str(contributor) + "\t"
                 + str(date) + "\n")
#+END_SRC

** Curve fitting
   We can only do the curve fitting with books longer than 10000 tokens. This is because, to begin with, books don't exhibit logarithmic behavior until they reach around 4 thousand words.
   Moreover, we need to have enough samples to be able to feed the piece of software that does the curve fitting.
#+NAME: lexical-sweep
#+BEGIN_SRC python :noweb yes :session python :exports code
#Temporary value for speed. Before it was 500
samples = 10
log_behaviour_start = 5000
sweep_values = []
zhsweep_values = []
log_behaviour_range = len(tokens) - log_behaviour_start
log_step = log_behaviour_range/samples
if word_count > 10000:
    for sample_size in xrange(
            log_behaviour_start,
            log_behaviour_range,
            log_step):
        x_sample = log(len(tokens[0:sample_size]))
        y_sample = log(len(set(tokens[0:sample_size])))
        sweep_values.append([x_sample, y_sample])
    popt, pcov = fit_values(linear_func, sweep_values)
    book_intercept = popt[0]
    book_slope = popt[1]
    perr = np.sqrt(np.diag(pcov))
    std_error_intercept = perr[0]
    std_error_slope = perr[1]
    if language_detected == 'zh' or language_detected == 'zh_Hant':
        zhlog_behaviour_range = len(zh_characters) - log_behaviour_start
        zhlog_step = zhlog_behaviour_range/samples
        for sample_size in xrange(
                log_behaviour_start,
                zhlog_behaviour_range,
                zhlog_step):
            x_sample = len(zh_characters[0:sample_size])
            y_sample = log(len(set(zh_characters[0:sample_size])))
            zhsweep_values.append([x_sample, y_sample])
        zhpopt, zhpcov = fit_values(linear_func, zhsweep_values)
        zhintercept = zhpopt[0]
        zhslope = zhpopt[1]
        zhperr = np.sqrt(np.diag(zhpcov))
        zhstd_error_intercept = zhperr[0]
        zhstd_error_slope = zhperr[1]
    else:
        zhintercept = int()
        zhslope = int()
        zhstd_error_intercept = int()
        zhstd_error_slope = int()
else:
    book_intercept = int()
    book_slope = int()
    std_error_intercept = int()
    std_error_slope = int()
#+END_SRC

#+RESULTS: lexical-sweep

Empirically, We have found
#+NAME: fit-functions
#+begin_src python :noweb yes :session python :exports code
def linear_func(variable, slope, y_intercept):
    '''
    Linear model.
    '''
    return slope*variable + y_intercept

def log_func(variable, coefficient, x_intercept):
    '''
    Logarithmic model.
    '''
    return coefficient*log(variable) + x_intercept

def log_log_func(variable, coefficient, intercept):
    '''
    Log-log model.
    '''
    return math.e**(coefficient*log(variable) + intercept)
#+end_src

   #+NAME: curve-fitting
   #+begin_src python :noweb yes :session python :exports code
def fit_values(function, values):
    '''
    Fit values to a given model.
    '''
    array = list(zip(*values))
    xarr = array[0]
    yarr = array[1]

    return curve_fit(function, xarr, yarr)
      #+end_src

      #+RESULTS:
      : a = -5813.118832427114 , b = 761.1560740930518

** Tagging
   The purpose of this section is to tag the lists containing the analysis with the canon to which they belong, if appropriate.
   #+begin_src bash :tangle canon-tagging.sh :exports code
canon="/home/rcl/readability-measure/canon/chinese.txt"
analized="/home/rcl/readability-measure/tagging/zh-TW.tsv"
list=""
while read -r author_canon title_canon; do
        list+=$author_canon
        list+=" "
done < "$canon"
unique_authors=$(tr ' ' '\n' <<< $list | sort -u)
echo $unique_authors
while read -r filesize lexicalVariety intercept slope language author_list title_list type subject source rights relation publisher identifier format contibutor date; do
    flag=0
    while read -r author_canon title_canon; do
        if [ "$author_list" == "$author_canon" ] && [ "$title_list" == "$title_canon" ]; then
            #printf '%s %s Canon match!!\n' "$author_list" "$title_list"
            flag=1
        fi
    done < "$canon"
    for word in $unique_authors; do
        if [ "$author_list" == "$word" ] && [ "$flag" != 1 ]; then
            #printf '%s %s Extended canon match!!\n' "$author_list" "$title_list"
        fi
    done
done < "$analized"
   #+end_src

   #+begin_src bash
linewriting="/home/rcl/readability-measure/linewriting.txt"
touch $linewriting
echo "roland coeurjoly" > $linewriting
echo "chun zhang" >> $linewriting

while read line; do
    if [[ $line = *"chun zhang"* ]]; then
        #echo "substring found!"
        echo
    fi
done < "$linewriting"
less $linewriting
   #+end_src
   #+begin_src python :results output
# -*- coding: utf-8 -*-
import numpy
import csv
canon_file="/home/rcl/readability-measure/canon/chinese.txt"
analysis_file="/home/rcl/readability-measure/tagging/zh-TW.tsv"
canon = numpy.array(list(csv.reader(open(canon_file, "rb"), delimiter=" "))).astype("object")
analysis = numpy.array(list(csv.reader(open(analysis_file, "rb"), delimiter="\t"))).astype("object")
print canon[90][0]
print analysis[90][5]
   #+end_src
   #+RESULTS:
   : 古龍
   : 東野圭吾

* Plotting

Perfect. It plots the first two columns and doesn't give an error about all the rest.
#+BEGIN_SRC gnuplot :exports both all_.png
set title "Lexical variety Vs Length"
set xlabel "Length in words"
set ylabel "Unique words"
set logscale x
set logscale y
es_filelist=system("ls es*.tsv")
fr_filelist=system("ls fr*.tsv")
pt_filelist=system("ls p*.tsv")
plot  for [filename in es_filelist] filename title 'Spanish' linecolor 1, \
      for [filename in fr_filelist] filename title 'French' linecolor 2, \
      for [filename in pt_filelist] filename title 'Portuguese' linecolor 3, \
      'ar.tsv' title 'Arabic' linecolor 4, \
      'zh-TW.tsv' title 'Chinese' linecolor 5
#+END_SRC

#+RESULTS:
[[file:languages.png]]

#+BEGIN_SRC gnuplot :exports both :file chinese.png
set title "Lexical variety Vs Length"
set xlabel "Length in characters"
set ylabel "Unique characters"
set logscale x
set nologscale y
plot 'zh-TW.tsv' title 'Chinese' linecolor 1
#+END_SRC

#+RESULTS:

#+BEGIN_SRC gnuplot :exports both :file arabic.png
set title "Lexical variety Vs Length"
set xlabel "Length in characters"
set ylabel "Unique characters"
set logscale x
set logscale y
plot 'ar.tsv' title 'Arabic' linecolor 1
#+END_SRC

#+BEGIN_SRC gnuplot :exports both :file all.png
set multiplot
set title "Lexical variety Vs Length"
set xlabel "Length in words"
set ylabel "Unique words"
#set logscale x
#set logscale y
set logscale x
set logscale y
filelist=system("ls *.tsv")
#plot  for [filename in filelist] filename title filename
plot 'spanish.tsv' title 'Spanish' linecolor 1, \
     'french.tsv' title 'French' linecolor 2, \
     'portuguese.tsv' title 'Portuguese' linecolor 3, \
     'ar.tsv' title 'Arabic' linecolor 4, \
     for [filename in filelist] filename title filename linecolor 5
unset multiplot
#+END_SRC

#+RESULTS:
[[file:all.png]]
* Fitting points to function
  The purpose of this section is to fit all the different points to a function
  | Minimum length (characters) |         R^2 |
  |-----------------------------+-------------|
  |                           0 | 0.743868489 |
  |                       20000 |        0.71 |
  |                             |             |
  #+BEGIN_SRC python
for i in xrange(0,lexicalVariety,1000):
  print(i)
  #+END_SRC

  #+RESULTS:

#+BEGIN_SRC gnuplot :exports both :file sweep.png
set multiplot
set encoding utf8
set title "Lexical variety Vs Length"
set xlabel "Length in characters"
set ylabel "Unique characters"
set logscale x
set nologscale y
plot '/home/rcl/readability-measure/test/0936.tsv' title 'Jipin Jiading' linecolor 1, \
     '/home/rcl/readability-measure/test/1077-4000.tsv' title 'Cixi Quanzhuan' linecolor 2
     #'/home/rcl/readability-measure/zh-TW.tsv' title 'Chinese' linecolor 3
unset multiplot
#+END_SRC

#+RESULTS:
[[file:sweep.png]]


#+BEGIN_SRC gnuplot :exports both :file test.png
set multiplot
set encoding utf8
set title "Lexical variety Vs Length"
set xlabel "Length in characters"
set ylabel "Unique characters"
set logscale x
set nologscale y
plot '/home/rcl/readability-measure/zh-TW.tsv' title 'Jipin Jiading' linecolor 1, \
     #'/home/rcl/readability-measure/zh-TW.tsv' title 'Cixi Quanzhuan' linecolor 2
     #'/home/rcl/readability-measure/zh-TW.tsv' title 'Chinese' linecolor 3
unset multiplot
#+END_SRC

#+RESULTS:
[[file:test.png]]

#+BEGIN_SRC R :file R.png :results output graphics
dat <- read.csv("~/readability-measure/zh-TW.tsv", header=FALSE, sep="\t")
x = dat[, 1]
y = dat[, 2]

Estimate = lm(y ~ x)
logEstimate = lm(y ~ log(x))

plot(x,predict(Estimate),type='l',col='blue')
lines(x,predict(logEstimate),col='red')
plot(x, y, log ="x",
        type="p",
        pch = 1,
        xlab="Length (characters)",
        ylab="Unique characters (characters)")
#+END_SRC

#+RESULTS:
[[file:R.png]]

#+begin_src R :file 3.png :results output graphics
library(lattice)
xyplot(1:10 ~ 1:10)
#+end_src

#+RESULTS:
[[file:3.png]]
* SQL DB
#+header: :engine mysql
#+header: :dbuser root
#+header: :dbpassword root
#+header: :database fiction
#+begin_src sql
SELECT DISTINCT Language FROM main;
#+end_src

#+RESULTS:
| Tables_in_fiction |
|-------------------|
| hashes            |
| main              |
| main_edited       |

#+NAME: db-connection
#+begin_src python :noweb yes :session python :exports code
MY_DB = mysql.connector.connect(
    host="localhost",
    user="root",
    passwd="root",
    charset='utf8'
)
#+end_src

#+Name: database-insertion
#+begin_src python :noweb yes :session python :exports code
mycursor = MY_DB.cursor()
print "Gotten cursor"
mycursor.execute("CREATE DATABASE IF NOT EXISTS library;")
mycursor.execute("use library;")
print "Gotten library"
mycursor.execute(
    """ CREATE TABLE IF NOT EXISTS corpus (id INT AUTO_INCREMENT PRIMARY KEY,
    title VARCHAR(255),
    author VARCHAR(255),
    slope DECIMAL(10,5),
    intercept DECIMAL(10,5),
    std_error_slope DECIMAL(10,5),
    std_error_intercept DECIMAL(10,5),
    word_count DECIMAL(20,1),
    unique_words DECIMAL(20,1),
    zhslope DECIMAL(10,5),
    zhintercept DECIMAL(10,5),
    zhstd_error_slope DECIMAL(10,5),
    zhstd_error_intercept DECIMAL(10,5),
    character_count DECIMAL(15,1),
    unique_characters DECIMAL(15,1),
    language VARCHAR(255),
    epubType VARCHAR(255),
    subject VARCHAR(255),
    source VARCHAR(255),
    rights VARCHAR(255),
    relation VARCHAR(255),
    publisher VARCHAR(255),
    identifier VARCHAR(255),
    epubFormat VARCHAR(255),
    description VARCHAR(510),
    contributor VARCHAR(255),
    date VARCHAR(255)) """)
print "Check table exists"
mycursor.execute(
    "ALTER DATABASE library CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;")
mycursor.execute(
    "ALTER TABLE corpus CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;")
print "DB and table utf8"
try:
    mycursor.execute(
        "ALTER TABLE corpus ADD CONSTRAINT unique_book UNIQUE (title,author);")
except Exception as ex:
    print ex
    raise
print "Add constraint"
sql = """INSERT IGNORE corpus (title,
author,
slope,
intercept,
std_error_slope,
std_error_intercept,
word_count,
unique_words,
zhslope,
zhintercept,
zhstd_error_slope,
zhstd_error_intercept,
character_count,
unique_characters,
language,
epubType,
subject,
source,
rights,
relation,
publisher,
identifier,
epubFormat,
description,
contributor,
date
) VALUES (%s,
%s,
%s,
%s,
%s,
%s,
%s,
%s,
%s,
%s,
%s,
%s,
%s,
%s,
%s,
%s,
%s,
%s,
%s,
%s,
%s,
%s,
%s,
%s,
%s,
%s)"""
val = (title,
       author,
       float(book_slope),
       float(book_intercept),
       float(std_error_slope),
       float(std_error_intercept),
       float(word_count),
       float(unique_words),
       float(zhslope),
       float(zhintercept),
       float(zhstd_error_slope),
       float(zhstd_error_intercept),
       float(character_count),
       float(unique_characters),
       language_detected,
       epubType,
       subject,
       source,
       rights,
       relation,
       publisher,
       identifier,
       epubFormat,
       description,
       contributor,
       date)
mycursor.execute(sql, val)
print "executed insert"
MY_DB.commit()
print("1 record inserted, ID:", mycursor.lastrowid)
#+end_src

#+RESULTS:
#+NAME: does-book-exist-db
#+begin_src python :noweb yes :session python :exports code :results output
mycursor = MY_DB.cursor()
try:
    mycursor.execute("CREATE DATABASE library")
except mysql.connector.errors.DatabaseError as ex:
    print ex
    mycursor.execute("USE library;")
try:
    query = ('SELECT * from corpus where title="' + str(title)
             + '" and author="' + str(author) + '"')
    mycursor.execute(query)
    myresult = mycursor.fetchall()
    if mycursor.rowcount == 1:
        print ("Book " + str(title)
               + ", by " + str(author)
               + " already in database. Next.")
        continue
except Exception as ex:
    print ex
    raise
#+end_src

#+RESULTS: does-book-exist-db
: ELECT * from corpus where title="opus" and author="paco"
: 1
: Book opus, by paco already in database. Next.
#+NAME: db-backup
#+begin_src python :noweb yes :session python :exports code
def runbackup(hostname, mysql_user, mysql_pw):
    '''
    Write sql file.
    '''
    try:
        backup = subprocess.Popen("mysqldump -h"
                                  + hostname + " -u"
                                  + mysql_user + " -p'"
                                  + mysql_pw + "' --databases library > "
                                  + "/media/root/terabyte/Metatron/library.sql", shell=True)
        # Wait for completion
        backup.communicate()
        print("Backup done for", hostname)
    except Exception as ex:
        # Check for errors
        print ex
        if backup.returncode != 0:
            print("Backup failed for", hostname)
        raise
#+end_src
