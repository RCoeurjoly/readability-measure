#+TITLE: Readability measure based on analysis of lexical variety
#+AUTHOR: Roland Coeurjoly
#+EMAIL: rolandcoeurjoly@gmail.com
#+EXPORT_FILE_NAME: crazy_readability

* Introduction
  In this project we are going to develop a readability measure based on corpus analysis.
** What is a readability measure?
   A readability measure is a set of equations used to quantify how easy a given text is to read.
* Approach
  We are going to approach the problem from a corpus linguistics perspective. This means that we are going to analyze a lot of books and take some general conclusions about them.
  The basic premise used in this project is that vocabulary and readability are correlated positively.

  We will use a literate programming with org mode to make our results are reproducible as possible.
* Corpora
** French
  #+BEGIN_SRC shell :exports code
for i in {1..11000}
do
    wget -O $i.epub bibliothequenumerique.tv5monde.com/download/epub/$i
done
find ~/readability-measure/corpus/bibliothequenumerique.tv5monde.com/download/epub/ -name "*.epub" -size -86k -delete
  #+END_SRC

  #+RESULTS:
** Chinese
The following is the perfect script for downloading Chinese books from haodoo (好讀).
It removes those in vertical format with -R "V*.epub".
  #+BEGIN_SRC shell :exports code
wget -np -P ~/readability-measure/corpus/Chinese/ -r -A .epub -R "V*.epub" http://haodoo.net/PDB/
  #+END_SRC

#+BEGIN_SRC shell :exports code
cd ~/readability-measure/corpus/Chinese/haodoo.net/PDB/
find . -mindepth 1 -type f -name "*.epub" -exec printf x \; | wc -c
find . -mindepth 1 -type f -name "*435*.epub"
#+END_SRC

#+RESULTS:
| 3699          |
| ./A/435.epub  |
| ./D/1435.epub |

#+BEGIN_SRC shell :exports code
ls -ltu ~/readability-measure/corpus/Chinese/haodoo.net/PDB/A/*435.epub
#+END_SRC

#+RESULTS:
| -rw-rw-r-- | 1 | rcl | rcl | 130599 | Feb | 23 | 00:07 | /home/rcl/readability-measure/corpus/Chinese/haodoo.net/PDB/A/V435.epub |
| -rw-rw-r-- | 1 | rcl | rcl | 130460 | Feb | 23 | 00:07 | /home/rcl/readability-measure/corpus/Chinese/haodoo.net/PDB/A/435.epub  |
|            |   |     |     |        |     |    |       |                                                                         |
** Arabic
   [[http://www.hindawi.
org/][http://www.hindawi.org/]]
   #+BEGIN_SRC shell :exports code
wget -np -P ~/readability-measure/corpus/Arabic/ -r -A .epub http://www.hindawi.org/books/
   #+END_SRC
** Russian
   #+BEGIN_SRC shell :exports code
wget -np -P ~/readability-measure/corpus/Russian/ -r -A .epub https://www.rulit.me/
   #+END_SRC
** All
   The Gutenberg project is also a good source of books.
   We could create different files for each language with length and lexical variety.
   Also, a convenient feature of the Gutenberg library is that it has ebooks with images and without.
   We download without images to save bandwidth.
   #+BEGIN_SRC shell :exports code
wget -w 2 -m -H "http://www.gutenberg.org/robot/harvest?filetypes[]=epub.noimages"
   #+END_SRC
* Analysis
  #+PROPERTY: session *python*
  #+PROPERTY: cache yes
  #+PROPERTY: results none
  In a first instance, we want to extract the following information from each ebook:
  - Author
  - Title
  - Length in number of words
  - Number of unique words
  It would be nice to create a file for each language (according to metadata).
  The logic would be the following:
  Try adding the results to a file suffixed with the language code.
  If that throws an exception, create the file and add the results
#+BEGIN_SRC python :noweb yes :tangle analysis-sweep.py :exports code
# imports
<<imports>>
# main function
<<epub-handling>>
#+END_SRC

#+RESULTS:
: None

** Imports
   We import some packages to make our life easier:
   - ebooklib: to process epubs
   - BeautifulSoup: to process the html in epubs
   - ntlk: to do natural language processing
#+NAME: imports
#+BEGIN_SRC python :session python :results none :exports code
import sys
import os
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import nltk
import nltk.tokenize
import codecs
#+END_SRC

** Epub reading

   We then proceed to open the epub and extract all metadata.
   As stated in the [[https://ebooklib.readthedocs.io/en/latest/tutorial.html#reading-epub][package documentation]], only creator, title and language are required metadata fields.
   The rest is optional, so we catch them with care.

   We then use BeautifulSoup to remove all html marks.
#+NAME: epub-handling
#+BEGIN_SRC python :noweb yes :session python :exports code
path = "/home/rcl/readability-measure/test/"
i = 1
for dirpath, dirnames, files in os.walk(path):
    for ebook in files:
        print "Reading ebook " + ebook + ", number  " + str(i)
        try:
            book = epub.read_epub(dirpath + "/" + ebook)
            <<get-epub-metadata>>
            <<text-extraction>>
            <<nltk-processing>>
            <<file-writing-sweep>>
            i += 1
        except:
            pass
#+END_SRC

#+RESULTS: epub-handling

#+NAME: file-writing
#+BEGIN_SRC python :exports code
with open("/home/rcl/readability-measure/test/" + ebook + ".tsv", "a") as myfile:
    myfile.write(str(filesize) + "\t"
                 + str(lexicalVariety) + "\t"
                 + str(language) + "\t"
                 + str(creator) + "\t"
                 + str(title) + "\t"
                 + str(type) + "\t"
                 + str(subject) + "\t"
                 + str(source) + "\t"
                 + str(rights) + "\t"
                 + str(relation) + "\t"
                 + str(publisher) + "\t"
                 + str(identifier) + "\t"
                 + str(format) + "\t"
                 # + str(description) + "\t"
                 + str(contributor) + "\t"
                 + str(date) + "\n")
#+END_SRC

#+NAME: text-extraction
#+BEGIN_SRC python :session python :noweb yes :exports code
cleantext = ""
for item in book.get_items():
    if item.get_type() == ebooklib.ITEM_DOCUMENT:
        raw_html = item.get_content()
        <<html-cleaning>>
#+END_SRC

#+RESULTS: text-extraction

#+NAME: html-cleaning
#+BEGIN_SRC python :session python :exports code
cleantext += BeautifulSoup(raw_html, "lxml").text
#+END_SRC

#+RESULTS: html-cleaning

#+NAME: get-epub-metadata
#+BEGIN_SRC python :exports code
try:
    type = book.get_metadata('DC', 'type')[0][0].encode('utf-8')
except:
    type = '-'
try:
    subject = book.get_metadata('DC', 'subject')[0][0].encode('utf-8')
except:
    subject = '-'
try:
    source = book.get_metadata('DC', 'source')[0][0].encode('utf-8')
except:
    source = '-'
try:
    rights = book.get_metadata('DC', 'rights')[0][0].encode('utf-8')
except:
    rights = '-'
try:
    relation = book.get_metadata('DC', 'relation')[0][0].encode('utf-8')
except:
    relation = '-'
try:
    publisher = book.get_metadata('DC', 'publisher')[0][0].encode('utf-8')
except:
    publisher = '-'
try:
    language = book.get_metadata('DC', 'language')[0][0].encode('utf-8')
except:
    language = 'empty'
try:
    identifier = book.get_metadata('DC', 'identifier')[0][0].encode('utf-8')
except:
    identifier = '-'
try:
    format = book.get_metadata('DC', 'format')[0][0].encode('utf-8')
except:
    format = '-'
try:
    description = book.get_metadata('DC', 'description')[0][0].encode('utf-8')
except:
    description = '-'
try:
    coverage = book.get_metadata('DC', 'coverage')[0][0].encode('utf-8')
except:
    coverage = '-'
try:
    contributor = book.get_metadata('DC', 'contributor')[0][0].encode('utf-8')
except:
    contributor = '-'
try:
    creator = book.get_metadata('DC', 'creator')[0][0].encode('utf-8')
except:
    creator = '-'
try:
    title = book.get_metadata('DC', 'title')[0][0].encode('utf-8')
except:
    title = '-'
try:
    date = book.get_metadata('DC', 'date')[0][0].encode('utf-8')
except:
    date = '-'
#+END_SRC
#+NAME: nltk-processing
#+BEGIN_SRC python :session python :exports code
if (language != 'zh-TW'):
    tokens = nltk.tokenize.word_tokenize(cleantext)
else:
    tokens = ''.join(c for c in cleantext if u'\u4e00' <= c <= u'\u9fff')
filesize = len(tokens)
lexicalVariety = len(set(tokens))
#+END_SRC

#+RESULTS: nltk-processing

#+NAME: test-western
#+BEGIN_SRC python :session python :results output :exports code
print "Size: " + str(filesize)
print "Lexical variety: " + str(lexicalVariety)
txt_text = codecs.open(
    str("/home/rcl/readability-measure/corpus/440.txt"),
    'r',
    'utf-8-sig',
    'ignore').read()
txt_tokens = nltk.tokenize.word_tokenize(txt_text)
txt_filesize = len(txt_tokens)
txt_lexicalVariety = len(set(txt_tokens))
print "TXT Size: " + str(txt_filesize)
print "TXT Lexical variety: " + str(txt_lexicalVariety)
#+END_SRC

#+NAME: test-chinese
#+BEGIN_SRC python :session python :results output :exports code
print "Size: " + str(filesize)
print "Lexical variety: " + str(lexicalVariety)
txt_tokens = ''.join(c for c in codecs.open(
    str("/home/rcl/readability-measure/test/17F0b.txt"),
    'r',
    'utf-8-sig',
    'ignore').read() if u'\u4e00' <= c <= u'\u9fff')
txt_filesize = len(txt_tokens)
txt_lexicalVariety = len(set(txt_tokens))
print "TXT Size: " + str(txt_filesize)
print "TXT Lexical variety: " + str(txt_lexicalVariety)
#+END_SRC

  #+RESULTS:
  : Traceback (most recent call last):
  :   File "<stdin>", line 1, in <module>
  :   File "/tmp/babel-vpxI7x/python-9FEIgK", line 1, in <module>
  :     print "Size: " + str(filesize)
  : NameError: name 'filesize' is not defined

* Plotting

Perfect. It plots the first two columns and doesn't give an error about all the rest.
#+BEGIN_SRC gnuplot :exports both :file chinese.png
set title "Lexical variety Vs Length"
set xlabel "Length in words"
set ylabel "Unique words"
set logscale x
set logscale y
es_filelist=system("ls es*.tsv")
fr_filelist=system("ls fr*.tsv")
pt_filelist=system("ls p*.tsv")
plot  for [filename in es_filelist] filename title 'Spanish' linecolor 1, \
      for [filename in fr_filelist] filename title 'French' linecolor 2, \
      for [filename in pt_filelist] filename title 'Portuguese' linecolor 3, \
      'ar.tsv' title 'Arabic' linecolor 4, \
      'zh-TW.tsv' title 'Chinese' linecolor 5
#+END_SRC

#+RESULTS:
[[file:chinese.png]]

#+BEGIN_SRC gnuplot :exports both :file chinese.png
set title "Lexical variety Vs Length"
set xlabel "Length in characters"
set ylabel "Unique characters"
set logscale x
set nologscale y
plot 'zh-TW.tsv' title 'Chinese' linecolor 1
#+END_SRC

#+RESULTS:
[[file:chinese.png]]

#+BEGIN_SRC gnuplot :exports both :file arabic.png
set title "Lexical variety Vs Length"
set xlabel "Length in characters"
set ylabel "Unique characters"
set logscale x
set logscale y
plot 'ar.tsv' title 'Arabic' linecolor 1
#+END_SRC

#+RESULTS:
[[file:arabic.png]]

#+BEGIN_SRC gnuplot :exports both :file all.png
set multiplot
set title "Lexical variety Vs Length"
set xlabel "Length in words"
set ylabel "Unique words"
#set logscale x
#set logscale y
set logscale x
set logscale y
filelist=system("ls *.tsv")
#plot  for [filename in filelist] filename title filename
plot 'spanish.tsv' title 'Spanish' linecolor 1, \
     'french.tsv' title 'French' linecolor 2, \
     'portuguese.tsv' title 'Portuguese' linecolor 3, \
     'ar.tsv' title 'Arabic' linecolor 4, \
     for [filename in filelist] filename title filename linecolor 5
unset multiplot
#+END_SRC

#+RESULTS:
[[file:all.png]]
* Fitting points to function
  The purpose of this section is to fit all the different points to a function
  | Minimum length (characters) |         R^2 |
  |-----------------------------+-------------|
  |                           0 | 0.743868489 |
  |                       20000 |        0.71 |
  |                             |             |
  #+BEGIN_SRC python
for i in xrange(0,lexicalVariety,1000):
  print(i)
  #+END_SRC



#+NAME: file-writing-sweep
#+BEGIN_SRC python :exports code
with open("/home/rcl/readability-measure/test/" + ebook[0:4] + ".tsv", "a") as myfile:
    for j in xrange(0,5000,1):
        myfile.write(str(len(tokens[5000:5000 + j])) + "\t"
                     + str(len(set(tokens[5000:5000 + j]))) + "\n")
        print "Analizing first " + str(len(tokens[0:j])) + " characters."
#+END_SRC

#+BEGIN_SRC gnuplot :exports both :file sweep.png
set multiplot
set encoding utf8
set title "Lexical variety Vs Length"
set xlabel "Length in characters"
set ylabel "Unique characters"
set logscale xR
set nologscale y
plot '/home/rcl/readability-measure/test/0936.tsv' title 'Jipin Jiading' linecolor 1, \
     '/home/rcl/readability-measure/test/1077-4000.tsv' title 'Cixi Quanzhuan' linecolor 2
unset multiplot
#+END_SRC

#+RESULTS:
[[file:sweep.png]]
