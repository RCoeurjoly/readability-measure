#+TITLE: Analysis
#+AUTHOR: Roland Coeurjoly
#+EMAIL: rolandcoeurjoly@gmail.com
* Introduction
  In this project we are going to develop a readability measure based on corpus analysis.
** What is a readability measure?
   A readability measure is a set of equations used to quantify how easy a given text is to read.
* Approach
  We are going to approach the problem from a corpus linguistics perspective. This means that we are going to analyze a lot of books and take some general conclusions about them.
  The basic premise used in this project is that vocabulary and readability are correlated positively.

  We will use a literate programming with org mode to make our results are reproducible as possible.
* Corpora
** French
  #+BEGIN_SRC shell
for i in {1..11000}
do
    wget -O $i.epub bibliothequenumerique.tv5monde.com/download/epub/$i
done
find ~/readability-measure/corpus/bibliothequenumerique.tv5monde.com/download/epub/ -name "*.epub" -size -86k -delete
  #+END_SRC

  #+RESULTS:
** Chinese
The following is the perfect script for downloading Chinese books from haodoo (好讀).
It removes those in vertical format with -R "V*.epub".
  #+BEGIN_SRC shell
wget -np -P ~/readability-measure/corpus/Chinese/ -r -A .epub -R "V*.epub" http://haodoo.net/PDB/
  #+END_SRC

#+BEGIN_SRC shell
cd ~/readability-measure/corpus/Chinese/haodoo.net/PDB/
find . -mindepth 1 -type f -name "*.epub" -exec printf x \; | wc -c
find . -mindepth 1 -type f -name "*435*.epub"
#+END_SRC

#+RESULTS:
| 3699          |
| ./A/435.epub  |
| ./D/1435.epub |

#+BEGIN_SRC shell
ls -ltu ~/readability-measure/corpus/Chinese/haodoo.net/PDB/A/*435.epub
#+END_SRC

#+RESULTS:
| -rw-rw-r-- | 1 | rcl | rcl | 130599 | Feb | 23 | 00:07 | /home/rcl/readability-measure/corpus/Chinese/haodoo.net/PDB/A/V435.epub |
| -rw-rw-r-- | 1 | rcl | rcl | 130460 | Feb | 23 | 00:07 | /home/rcl/readability-measure/corpus/Chinese/haodoo.net/PDB/A/435.epub  |
** All
   The Gutenberg project is also a good source of books.
   We could create different files for each language with length and lexical variety.
   Also, a convenient feature of the Gutenberg library is that it has ebooks with images and without.
   We download without images to save bandwidth.
   #+BEGIN_SRC shell
wget -w 2 -m -H "http://www.gutenberg.org/robot/harvest?filetypes[]=epub.noimages"
   #+END_SRC
* Analysis
  #+PROPERTY: session *python*
  #+PROPERTY: cache yes
  #+PROPERTY: results none
  In a first instance, we want to extract the following information from each ebook:
  - Author
  - Title
  - Length in number of words
  - Number of unique words
  It would be nice to create a file for each language (according to metadata).
  The logic would be the following:
  Try adding the results to a file suffixed with the language code.
  If that throws an exception, create the file and add the results
#+BEGIN_SRC python :noweb yes :tangle analysis.py :exports none
# imports
<<imports>>
# main function
<<epub-handling>>
#+END_SRC

#+RESULTS:
: None

** Imports
   We import some packages to make our life easier:
   - ebooklib: to process epubs
   - BeautifulSoup: to process the html in epubs
   - ntlk: to do natural language processing
#+NAME: imports
#+BEGIN_SRC python :session python :results none
import sys
import os
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import nltk
import nltk.tokenize
import codecs
#+END_SRC

** Epub reading

   We then proceed to open the epub and extract all metadata.
   As stated in the [[https://ebooklib.readthedocs.io/en/latest/tutorial.html#reading-epub][package documentation]], only creator, title and language are required metadata fields.
   The rest is optional, so we catch them with care.

   We then use BeautifulSoup to remove all html marks.
#+NAME: epub-handling
#+BEGIN_SRC python :noweb yes :session python
path = "/home/rcl/readability-measure/corpus/"
i = 1
for dirpath, dirnames, files in os.walk(path):
    for ebook in files:
        print "Reading ebook " + ebook + ", number  " + str(i)
        try:
            book = epub.read_epub(dirpath + "/" + ebook)
            <<get-epub-metadata>>
            <<text-extraction>>
            <<nltk-processing>>
            <<file-writing>>
            i += 1
        except:
            pass
#+END_SRC

#+RESULTS: epub-handling

<<<<<<< cf368f97fbfcf1869d69b64042193ed25b54a73d
We extract all chapters in html format.

=======
#+NAME: file-writing
#+BEGIN_SRC python
with open("/home/rcl/readability-measure/" + language + ".tsv", "a") as myfile:
    myfile.write(str(filesize) + "\t"
                 + str(lexicalVariety) + "\t"
                 + str(language) + "\t"
                 + str(creator) + "\t"
                 + str(title) + "\t"
                 + str(type) + "\t"
                 + str(subject) + "\t"
                 + str(source) + "\t"
                 + str(rights) + "\t"
                 + str(relation) + "\t"
                 + str(publisher) + "\t"
                 + str(identifier) + "\t"
                 + str(format) + "\t"
                 # + str(description) + "\t"
                 + str(contributor) + "\t"
                 + str(date) + "\n")
#+END_SRC
>>>>>>> Add tsv files
#+NAME: text-extraction
#+BEGIN_SRC python :session python :noweb yes
cleantext = ""
for item in book.get_items():
    if item.get_type() == ebooklib.ITEM_DOCUMENT:
        raw_html = item.get_content()
        <<html-cleaning>>
#+END_SRC

#+RESULTS: text-extraction

#+NAME: html-cleaning
#+BEGIN_SRC python :session python
cleantext += BeautifulSoup(raw_html, "lxml").text
#+END_SRC

#+RESULTS: html-cleaning

#+NAME: get-epub-metadata
#+BEGIN_SRC python
try:
    type = book.get_metadata('DC', 'type')[0][0].encode('utf-8')
except:
    type = '-'
try:
    subject = book.get_metadata('DC', 'subject')[0][0].encode('utf-8')
except:
    subject = '-'
try:
    source = book.get_metadata('DC', 'source')[0][0].encode('utf-8')
except:
    source = '-'
try:
    rights = book.get_metadata('DC', 'rights')[0][0].encode('utf-8')
except:
    rights = '-'
try:
    relation = book.get_metadata('DC', 'relation')[0][0].encode('utf-8')
except:
    relation = '-'
try:
    publisher = book.get_metadata('DC', 'publisher')[0][0].encode('utf-8')
except:
    publisher = '-'
try:
    language = book.get_metadata('DC', 'language')[0][0].encode('utf-8')
except:
    language = '-'
try:
    identifier = book.get_metadata('DC', 'identifier')[0][0].encode('utf-8')
except:
    identifier = '-'
try:
    format = book.get_metadata('DC', 'format')[0][0].encode('utf-8')
except:
    format = '-'
try:
    description = book.get_metadata('DC', 'description')[0][0].encode('utf-8')
except:
    description = '-'
try:
    coverage = book.get_metadata('DC', 'coverage')[0][0].encode('utf-8')
except:
    coverage = '-'
try:
    contributor = book.get_metadata('DC', 'contributor')[0][0].encode('utf-8')
except:
    contributor = '-'
try:
    creator = book.get_metadata('DC', 'creator')[0][0].encode('utf-8')
except:
    creator = '-'
try:
    title = book.get_metadata('DC', 'title')[0][0].encode('utf-8')
except:
    title = '-'
try:
    date = book.get_metadata('DC', 'date')[0][0].encode('utf-8')
except:
    date = '-'
#+END_SRC
#+NAME: nltk-processing
#+BEGIN_SRC python :session python
if (language != 'zh-TW'):
    tokens = nltk.tokenize.word_tokenize(cleantext)
else:
    tokens = ''.join(c for c in cleantext if u'\u4e00' <= c <= u'\u9fff')
filesize = len(tokens)
lexicalVariety = len(set(tokens))
#+END_SRC

#+RESULTS: nltk-processing

#+NAME: test-western
#+BEGIN_SRC python :session python :results output
print "Size: " + str(filesize)
print "Lexical variety: " + str(lexicalVariety)
txt_text = codecs.open(
    str("/home/rcl/readability-measure/corpus/440.txt"),
    'r',
    'utf-8-sig',
    'ignore').read()
txt_tokens = nltk.tokenize.word_tokenize(txt_text)
txt_filesize = len(txt_tokens)
txt_lexicalVariety = len(set(txt_tokens))
print "TXT Size: " + str(txt_filesize)
print "TXT Lexical variety: " + str(txt_lexicalVariety)
#+END_SRC

#+NAME: test-chinese
#+BEGIN_SRC python :session python :results output
print "Size: " + str(filesize)
print "Lexical variety: " + str(lexicalVariety)
txt_tokens = ''.join(c for c in codecs.open(
    str("/home/rcl/readability-measure/test/17F0b.txt"),
    'r',
    'utf-8-sig',
    'ignore').read() if u'\u4e00' <= c <= u'\u9fff')
txt_filesize = len(txt_tokens)
txt_lexicalVariety = len(set(txt_tokens))
print "TXT Size: " + str(txt_filesize)
print "TXT Lexical variety: " + str(txt_lexicalVariety)
#+END_SRC

  #+RESULTS:
  : Traceback (most recent call last):
  :   File "<stdin>", line 1, in <module>
  :   File "/tmp/babel-vpxI7x/python-9FEIgK", line 1, in <module>
  :     print "Size: " + str(filesize)
  : NameError: name 'filesize' is not defined

** Trial gnuplot

#+RESULTS:
#+NAME: output
#+BEGIN_SRC shell :file output.dat replace
python ~/readability-measure/analysis.py
#+END_SRC

#+RESULTS: output
[[file:output.dat]]
68539	5664
]]

Perfect. It plots the first two columns and doesn't give an error about all the rest.
#+BEGIN_SRC gnuplot
plot '/home/rcl/readability-measure/zh-TW.tsv'
#+END_SRC

#+RESULTS:

#+BEGIN_SRC gnuplot :file chinese.png
plot '/home/rcl/readability-measure/corpus/bibliothequenumerique.tv5monde.com/lexicalVarietyVsLength.tsv'
#+END_SRC

#+RESULTS:
[[file:chinese.png]]

#+BEGIN_SRC python :results none
import math
file = open("/home/rcl/readability-measure/corpus/Chinese/chinese.tsv", "r")
logfile = open("/home/rcl/readability-measure/corpus/Chinese/chineseLog.tsv", "w")
for line in file:
    print line.rsplit('\t', 1)[0]
    print math.log(float(line.rsplit('\t', 1)[0]), 10)
    logfile.write(str(math.log(float(line.rsplit('\t', 1)[0]), 2)) + "\t" + str(line.rsplit('\t', 1)[1]) + '\n')
    print '\n'
file.close()
logfile.close()
#+END_SRC
