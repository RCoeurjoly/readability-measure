#+TITLE: Readability measure based on analysis of lexical variety

#+AUTHOR: Roland Coeurjoly
#+EMAIL: rolandcoeurjoly@gmail.com
#+EXPORT_FILE_NAME: readability_measure

* Introduction
  In this project we are going to develop a readability measure based on corpus analysis.
** What is a readability measure?
   A readability measure is a set of equations used to quantify how easy a given text is to read.
   In this project, we focus on long texts, with more that 2000 words or tokens. This is in contrast with other readability measures which analyze sentences.
* Literature review
  Alexander Arguelles proposes the following:

* Approach
  We are going to approach the problem from a corpus linguistics perspective. This means that we are going to analyze a lot of books and take some general conclusions about them.
  The basic premise used in this project is that vocabulary and readability are correlated negatively, i.e., the larger the vocabulary, the more difficult to understand.

  We will use a literate programming approach using GNU/Emacs org mode to make our results as reproducible as possible.
* Corpora
** French
  #+BEGIN_SRC shell :exports code :tangle french-corpus.sh
for i in {1..11000}
do
    wget -O $1$i.epub bibliothequenumerique.tv5monde.com/download/epub/$i
    find ${1} -name "*.epub" -size -88k -delete
done
  #+END_SRC

  #+RESULTS:
** Chinese
The following is the perfect script for downloading Chinese books from haodoo (好讀).
It removes those in vertical format with -R "V*.epub".
  #+BEGIN_SRC shell :exports code :tangle chinese-corpus.sh
wget -np -P $1 -r -A .epub -R "V*.epub" http://haodoo.net/PDB/
  #+END_SRC

#+BEGIN_SRC shell :exports code
find ~/readability-measure/corpus/Chinese/haodoo.net/PDB/ -mindepth 1 -type f -name "*.epub" -exec printf x \; | wc -c
#+END_SRC

#+RESULTS:
: 3699

#+BEGIN_SRC shell :exports code
ls -ltu ~/readability-measure/corpus/Chinese/haodoo.net/PDB/A/*435.epub
#+END_SRC

#+RESULTS:
: -rw-rw-r-- 1 rcl rcl 130460 jul 19 16:04 /home/rcl/readability-measure/corpus/Chinese/haodoo.net/PDB/A/435.epub
** Arabic
   [[http://www.hindawi.
org/][http://www.hindawi.org/]]
   #+BEGIN_SRC shell :exports code :tangle arabic-corpus.sh
wget -np -P $1 -r -A .epub http://www.hindawi.org/books/
   #+END_SRC
** Russian
   #+BEGIN_SRC shell :exports code
wget -np -P ~/readability-measure/corpus/Russian/ -r -A .epub https://www.rulit.me/
   #+END_SRC
** All
   The Gutenberg project is also a good source of books.
   We could create different files for each language with length and lexical variety.
   Also, a convenient feature of the Gutenberg library is that it has ebooks with images and without.
   We download without images to save bandwidth.
   #+BEGIN_SRC shell :exports code
wget -w 2 -m -H "http://www.gutenberg.org/robot/harvest?filetypes[]=epub.noimages"
   #+END_SRC
* Analysis
  #+PROPERTY: session *python*
  #+PROPERTY: cache yes
  #+PROPERTY: results none
  In a first instance, we want to extract the following information from each ebook:
  - Author
  - Title
  - Length in number of words
  - Number of unique words
  It would be nice to create a file for each language (according to metadata).
  The logic would be the following:
  Try adding the results to a file suffixed with the language code.
  If that throws an exception, create the file and add the results
#+BEGIN_SRC python :noweb yes :tangle corpus_analysis.py :exports code
# -*- coding: utf-8 -*-
'''
corpus-analysis.py: readability measure for epub ebooks.
Version 1.0
Copyright (C) 2019  Roland Coeurjoly <rolandcoeurjoly@gmail.com>
'''
# Imports
<<imports>>
# Constants
<<printable-set>>
# Book Class
<<book-class>>
# Functions
<<utf8-cleaning>>
## Curve fitting functions
<<curve-fit>>
<<lexical-sweep>>
<<fit-functions>>
## Database functions
<<db-connection>>
<<database-insertion>>
<<database-creation>>
<<is-book-in-db>>
<<db-backup>>
# Main function
<<main>>
#+END_SRC

#+RESULTS:
: None

** Imports
   We import some packages to make our life easier:
   - ebooklib: to process epubs
   - BeautifulSoup: to process the html in epubs
   - langdetect to detect language. We use this because based on experience epub language tags are not very reliable
   - ntlk: to do natural language processing
#+NAME: imports
#+BEGIN_SRC python :session python :results none :exports code
import unicodedata
import sys
import os
import math
import subprocess
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
from scipy.optimize import curve_fit
from scipy import log as log
import numpy as np
import mysql.connector
from polyglot.text import Text
#+END_SRC

** Ebook handling

   We then proceed to open the epub and extract all metadata.
   As stated in the [[https://ebooklib.readthedocs.io/en/latest/tutorial.html#reading-epub][package documentation]], only creator, title and language are required metadata fields.
   The rest is optional, so we catch them with care.

   We then use BeautifulSoup to remove all html marks.
*** Class book
#+NAME: book-class
#+BEGIN_SRC python :noweb yes :session python :exports code
class Book(object):
    '''
    Book class
    '''
    # pylint: disable=too-many-instance-attributes
    # There is a lot of metadata but it is repetitive and non problematic.
    <<constructor>>
    <<tokenization>>
    <<text-extraction>>
    <<language-detection>>
    #+end_src

*** Extracting text from ebook
#+NAME: text-extraction
#+BEGIN_SRC python :noweb yes :session python :exports code
def extract_text(self, book):
    '''
    Extract all text from the book.
    '''
    cleantext = ""
    html_filtered = ""
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            raw_html = item.get_content()
            <<html-filtering>>
    cleantext = clean_non_printable(html_filtered)
    self.text = cleantext
#+END_SRC

#+RESULTS: text-extraction
**** Cleaning the html
#+NAME: html-filtering
#+BEGIN_SRC python :noweb yes :session python :exports code
html_filtered += BeautifulSoup(raw_html, "lxml").text
#+END_SRC

#+RESULTS: html-cleaning
**** Removing invalid utf-8

#+NAME: printable-set
#+BEGIN_SRC python :noweb yes :session python :exports code
PRINTABLE = {
    #'Cc',
    'Cf',
    'Cn',
    'Co',
    'Cs',
    'LC',
    'Ll',
    'Lm',
    'Lo',
    'Lt',
    'Lu',
    'Mc',
    'Me',
    'Mn',
    'Nd',
    'Nl',
    'No',
    'Pc',
    'Pd',
    'Pe',
    'Pf',
    'Pi',
    'Po',
    'Ps',
    'Sc',
    'Sk',
    'Sm',
    'So',
    'Zl',
    'Zp',
    'Zs'}
     #+end_src

#+NAME: utf8-cleaning
#+BEGIN_SRC python :noweb yes :session python :exports code
def clean_non_printable(text):
    '''
    Remove all non printable characters from string.
    '''
    return ''.join(character for character in text if unicodedata.category(character) in PRINTABLE)
#+END_SRC
**** Language detection
#+NAME: language-detection
#+begin_src python :noweb yes :session python :exports code
def detect_language(self):
    '''
    We don't trust the epub metadata regarding language tags
    so we do our own language detection
    '''
    self.language = Text(self.text).language.code
#+end_src

*** Extracting metadata
    We don't extract all text in constructor because it is expensive and we want to check first if it exists in database.
#+NAME: constructor
#+BEGIN_SRC python :noweb yes :session python :exports code
def __init__(self, epub_file):
    '''
    Init.
    '''
    # pylint: disable=too-many-statements
    # There is a lot of metadata but it is repetitive and non problematic.
    try:
        self.epub_type = epub_file.get_metadata('DC', 'type')[0][0].encode('utf-8')
    except IndexError:
        self.epub_type = ''
    try:
        self.subject = epub_file.get_metadata('DC', 'subject')[0][0].encode('utf-8')
    except IndexError:
        self.subject = ''
    try:
        self.source = epub_file.get_metadata('DC', 'source')[0][0].encode('utf-8')
    except IndexError:
        self.source = ''
    try:
        self.rights = epub_file.get_metadata('DC', 'rights')[0][0].encode('utf-8')
    except IndexError:
        self.rights = ''
    try:
        self.relation = epub_file.get_metadata('DC', 'relation')[0][0].encode('utf-8')
    except IndexError:
        self.relation = ''
    try:
        self.publisher = epub_file.get_metadata('DC', 'publisher')[0][0].encode('utf-8')
    except IndexError:
        self.publisher = ''
    #try:
    #    self.language = epub_file.get_metadata('DC', 'language')[0][0].encode('utf-8')
    #except IndexError:
    #    self.language = 'empty'
    try:
        self.identifier = epub_file.get_metadata('DC', 'identifier')[0][0].encode('utf-8')
    except IndexError:
        self.identifier = ''
    try:
        self.epub_format = epub_file.get_metadata('DC', 'format')[0][0].encode('utf-8')
    except IndexError:
        self.epub_format = ''
    try:
        self.description = epub_file.get_metadata('DC', 'description')[0][0].encode('utf-8')
    except IndexError:
        self.description = ''
    try:
        self.coverage = epub_file.get_metadata('DC', 'coverage')[0][0].encode('utf-8')
    except IndexError:
        self.coverage = ''
    try:
        self.contributor = epub_file.get_metadata('DC', 'contributor')[0][0].encode('utf-8')
    except IndexError:
        self.contributor = ''
    self.author = epub_file.get_metadata('DC', 'creator')[0][0].encode('utf-8')
    self.title = epub_file.get_metadata('DC', 'title')[0][0].encode('utf-8')
    try:
        self.date = epub_file.get_metadata('DC', 'date')[0][0].encode('utf-8')
    except IndexError:
        self.date = ''
    self.language = str()
    self.zh_characters = str()
    self.character_count = int()
    self.unique_characters = int()
    self.tokens = str()
    self.word_count = int()
    self.unique_words = int()
    self.text = str()
#+END_SRC
*** Tokenization
    If the language is Chinese, appart from doing the tokenization, we also measure individual characters.
#+NAME: tokenization
#+BEGIN_SRC python :noweb yes :session python :exports code
def tokenize(self):
    '''
    Tokenization.
    '''
    if self.language == 'zh' or self.language == 'zh_Hant':
        self.zh_characters = ''.join(character for character in self.text
                                     if u'\u4e00' <= character <= u'\u9fff')
        self.character_count = len(self.zh_characters)
        self.unique_characters = len(set(self.zh_characters))
    else:
        self.zh_characters = str()
        self.character_count = int()
        self.unique_characters = int()
    self.tokens = Text(self.text).words
    self.word_count = len(self.tokens)
    self.unique_words = len(set(self.tokens))
#+END_SRC
*** Log writing
#+NAME: log-writing
#+BEGIN_SRC python :noweb yes :session python :exports code
with open("/home/rcl/readability-measure/test/"
          + str(language_detected)
          + ".tsv", "w") as myfile:
    myfile.write(str(wordCount) + "\t"
                 + str(uniqueWords) + "\t"
                 #+ str(intercept) + "\t"
                 #+ str(slope) + "\t"
                 + str(language_detected) + "\t"
                 + str(author) + "\t"
                 + str(title) + "\t"
                 + str(epub_type) + "\t"
                 + str(subject) + "\t"
                 + str(source) + "\t"
                 + str(rights) + "\t"
                 + str(relation) + "\t"
                 + str(publisher) + "\t"
                 + str(identifier) + "\t"
                 + str(epubFormat) + "\t"
                 # + str(description) + "\t"
                 + str(contributor) + "\t"
                 + str(date) + "\n")
#+END_SRC

** Curve fitting
   We can only do the curve fitting with books longer than 10000 tokens. This is because, to begin with, books don't exhibit logarithmic behavior until they reach around 4 thousand words.
   Moreover, we need to have enough samples to be able to feed the piece of software that does the curve fitting.
#+NAME: lexical-sweep
#+BEGIN_SRC python :noweb yes :session python :exports code
def lexical_sweep(text, samples=10):
    '''
    Lexical sweep.
    '''
    #Temporary value for speed. Before it was 500
    log_behaviour_start = 5000
    sweep_values = []
    log_behaviour_range = len(text) - log_behaviour_start
    log_step = log_behaviour_range/samples
    if len(text) > 10000:
        for sample_size in xrange(
                log_behaviour_start,
                log_behaviour_range,
                log_step):
            x_sample = log(len(text[0:sample_size]))
            y_sample = log(len(set(text[0:sample_size])))
            sweep_values.append([x_sample, y_sample])
        return sweep_values
    return False
#+END_SRC

#+NAME: curve-fit
#+BEGIN_SRC python :noweb yes :session python :exports code
def extract_fit_parameters(function, sweep_values):
    '''
    Curve fit.
    '''
    if sweep_values:
        array = list(zip(*sweep_values))
        xarr = array[0]
        yarr = array[1]

        popt, pcov = curve_fit(function, xarr, yarr)
        intercept = popt[0]
        slope = popt[1]
        perr = np.sqrt(np.diag(pcov))
        std_error_intercept = perr[0]
        std_error_slope = perr[1]
        return {'intercept': intercept,
                'slope': slope,
                'std_error_intercept': std_error_intercept,
                'std_error_slope': std_error_slope}
    return {'intercept': int(),
            'slope': int(),
            'std_error_intercept': int(),
            'std_error_slope': int()}
#+END_SRC

#+RESULTS: lexical-sweep

Empirically, We have found
#+NAME: fit-functions
#+begin_src python :noweb yes :session python :exports code
def linear_func(variable, slope, y_intercept):
    '''
    Linear model.
    '''
    return slope*variable + y_intercept

def log_func(variable, coefficient, x_intercept):
    '''
    Logarithmic model.
    '''
    return coefficient*log(variable) + x_intercept

def log_log_func(variable, coefficient, intercept):
    '''
    Log-log model.
    '''
    return math.e**(coefficient*log(variable) + intercept)
#+end_src

** Main
 #+NAME: main
 #+BEGIN_SRC python :noweb yes :session python :exports code
def analyze_books(argv):
    '''
    Main function: open and read all epub files in directory.
    Analyze them and populate data in database
    :param argv: command line args.
    '''
    books_analyzed = 1
    for dirpath, __, files in os.walk(str(argv[1])):
        for ebook in files:
            if ebook.endswith(".epub"):
                print "Reading ebook " + ebook + ", number  " + str(books_analyzed)
                try:
                    epub_file = epub.read_epub(dirpath + "/" + ebook)
                except Exception as ex:
                    print ex
                    continue
                print "Getting epub metadata"
                my_book = Book(epub_file)
                print "Checking if book exists in database"
                if is_book_in_db(my_book.title, my_book.author):
                    continue
                print "Extracting text from ebook"
                my_book.extract_text(epub_file)
                print "Detecting language"
                my_book.detect_language()
                print "Language detected: " + str(my_book.language)
                print "Performing tokenization"
                my_book.tokenize()
                print "Lexical sweeps"
                sweep_values = lexical_sweep(my_book.tokens, samples=10)
                word_curve_fit = extract_fit_parameters(log_func, sweep_values)
                sweep_values = lexical_sweep(my_book.zh_characters, samples=10)
                zh_character_curve_fit = extract_fit_parameters(log_log_func, sweep_values)
                sweep_values = []
                print "Writing to database"
                insert_book_db(my_book, word_curve_fit, zh_character_curve_fit)
                books_analyzed += 1
                if len(argv) == 3:
                    runbackup("localhost", "root", "root", str(argv[2]))
                else:
                    runbackup("localhost", "root", "root")
    MY_DB.close()
if __name__ == '__main__':
    analyze_books(sys.argv)
 #+END_SRC

 #+RESULTS: epub-handling

** Tagging
   The purpose of this section is to tag the lists containing the analysis with the canon to which they belong, if appropriate.
   #+begin_src bash :tangle canon-tagging.sh :exports code
canon="/home/rcl/readability-measure/canon/chinese.txt"
analized="/home/rcl/readability-measure/tagging/zh-TW.tsv"
list=""
while read -r author_canon title_canon; do
        list+=$author_canon
        list+=" "
done < "$canon"
unique_authors=$(tr ' ' '\n' <<< $list | sort -u)
echo $unique_authors
while read -r filesize lexicalVariety intercept slope language author_list title_list type subject source rights relation publisher identifier format contibutor date; do
    flag=0
    while read -r author_canon title_canon; do
        if [ "$author_list" == "$author_canon" ] && [ "$title_list" == "$title_canon" ]; then
            #printf '%s %s Canon match!!\n' "$author_list" "$title_list"
            flag=1
        fi
    done < "$canon"
    for word in $unique_authors; do
        if [ "$author_list" == "$word" ] && [ "$flag" != 1 ]; then
            #printf '%s %s Extended canon match!!\n' "$author_list" "$title_list"
        fi
    done
done < "$analized"
   #+end_src

   #+begin_src bash
linewriting="/home/rcl/readability-measure/linewriting.txt"
touch $linewriting
echo "roland coeurjoly" > $linewriting
echo "chun zhang" >> $linewriting

while read line; do
    if [[ $line = *"chun zhang"* ]]; then
        #echo "substring found!"
        echo
    fi
done < "$linewriting"
less $linewriting
   #+end_src
   #+begin_src python :results output
# -*- coding: utf-8 -*-
import numpy
import csv
canon_file="/home/rcl/readability-measure/canon/chinese.txt"
analysis_file="/home/rcl/readability-measure/tagging/zh-TW.tsv"
canon = numpy.array(list(csv.reader(open(canon_file, "rb"), delimiter=" "))).astype("object")
analysis = numpy.array(list(csv.reader(open(analysis_file, "rb"), delimiter="\t"))).astype("object")
print canon[90][0]
print analysis[90][5]
   #+end_src
   #+RESULTS:
   : 古龍
   : 東野圭吾

* SQL DB
#+header: :engine mysql
#+header: :dbuser root
#+header: :dbpassword root
#+header: :database fiction
#+begin_src sql
SELECT DISTINCT Language FROM main;
#+end_src

#+RESULTS:
| Tables_in_fiction |
|-------------------|
| hashes            |
| main              |
| main_edited       |

#+NAME: db-connection
#+begin_src python :noweb yes :session python :exports code
MY_DB = mysql.connector.connect(
    host="localhost",
    user="root",
    passwd="root",
    charset='utf8'
)
#+end_src

#+Name: database-insertion
#+begin_src python :noweb yes :session python :exports code
def insert_book_db(book, word_curve_fit, zh_character_curve_fit):
    '''
    Insert data into db
    '''
    mycursor = MY_DB.cursor()
    mycursor.execute("use library;")
    sql = """INSERT IGNORE corpus (title,
    author,
    slope,
    intercept,
    std_error_slope,
    std_error_intercept,
    word_count,
    unique_words,
    zhslope,
    zhintercept,
    zhstd_error_slope,
    zhstd_error_intercept,
    character_count,
    unique_characters,
    language,
    epubType,
    subject,
    source,
    rights,
    relation,
    publisher,
    identifier,
    epubFormat,
    description,
    contributor,
    date
    ) VALUES (%s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s)"""
    val = (book.title,
           book.author,
           float(word_curve_fit['slope']),
           float(word_curve_fit['intercept']),
           float(word_curve_fit['std_error_slope']),
           float(word_curve_fit['std_error_intercept']),
           float(book.word_count),
           float(book.unique_words),
           float(zh_character_curve_fit['slope']),
           float(zh_character_curve_fit['intercept']),
           float(zh_character_curve_fit['std_error_slope']),
           float(zh_character_curve_fit['std_error_intercept']),
           float(book.character_count),
           float(book.unique_characters),
           book.language,
           book.epub_type,
           book.subject,
           book.source,
           book.rights,
           book.relation,
           book.publisher,
           book.identifier,
           book.epub_format,
           book.description,
           book.contributor,
           book.date)
    mycursor.execute(sql, val)
    MY_DB.commit()
    print("1 record inserted, ID:", mycursor.lastrowid)
#+end_src

#+RESULTS:
#+Name: database-creation
#+begin_src python :noweb yes :session python :exports code
def create_database():
    '''
    Create database if it doesn't exists yet.
    '''
    mycursor = MY_DB.cursor()
    mycursor.execute("CREATE DATABASE IF NOT EXISTS library;")
    mycursor.execute(
        """ CREATE TABLE IF NOT EXISTS corpus (id INT AUTO_INCREMENT PRIMARY KEY,
        title VARCHAR(255),
        author VARCHAR(255),
        slope DECIMAL(10,5),
        intercept DECIMAL(10,5),
        std_error_slope DECIMAL(10,5),
        std_error_intercept DECIMAL(10,5),
        word_count DECIMAL(20,1),
        unique_words DECIMAL(20,1),
        zhslope DECIMAL(10,5),
        zhintercept DECIMAL(10,5),
        zhstd_error_slope DECIMAL(10,5),
        zhstd_error_intercept DECIMAL(10,5),
        character_count DECIMAL(15,1),
        unique_characters DECIMAL(15,1),
        language VARCHAR(255),
        epub_type VARCHAR(255),
        subject VARCHAR(255),
        source VARCHAR(255),
        rights VARCHAR(255),
        relation VARCHAR(255),
        publisher VARCHAR(255),
        identifier VARCHAR(255),
        epubFormat VARCHAR(255),
        description VARCHAR(510),
        contributor VARCHAR(255),
        date VARCHAR(255)) """)
    mycursor.execute(
        "ALTER DATABASE library CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;")
    mycursor.execute(
        "ALTER TABLE corpus CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;")
    try:
        mycursor.execute(
            "ALTER TABLE corpus ADD CONSTRAINT unique_book UNIQUE (title,author);")
    except Exception as ex:
        print ex
#+end_src

#+NAME: is-book-in-db
#+begin_src python :noweb yes :session python :exports code :results output
def is_book_in_db(title, author):
    '''
    Check if book is in database.
    '''
    mycursor = MY_DB.cursor()
    mycursor.execute("CREATE DATABASE IF NOT EXISTS library;")
    mycursor.execute("USE library;")
    query = ('SELECT * from corpus where title="' + str(title)
             + '" and author="' + str(author) + '"')
    mycursor.execute(query)
    mycursor.fetchall()
    if mycursor.rowcount == 1:
        print ("Book " + str(title)
               + ", by " + str(author)
               + " already in database. Next.")
        return True
    return False
#+end_src

#+RESULTS: does-book-exist-db
: ELECT * from corpus where title="opus" and author="paco"
: 1
: Book opus, by paco already in database. Next.
#+NAME: db-backup
#+begin_src python :noweb yes :session python :exports code
def runbackup(hostname,
              mysql_user,
              mysql_password,
              db_loc="/media/root/terabyte/Metatron/library.sql"):
    '''
    Write sql file.
    '''
    try:
        backup = subprocess.Popen("mysqldump -h"
                                  + hostname + " -u"
                                  + mysql_user + " -p'"
                                  + mysql_password + "' --databases library > "
                                  + db_loc, shell=True)
        # Wait for completion
        backup.communicate()
        if backup.returncode != 0:
            sys.exit(1)
        else:
            print("Backup done for", hostname)
    except Exception as ex:
        # Check for errors
        print ex
        print("Backup failed for", hostname)
#+end_src
* Fitting points to function
  The purpose of this section is to fit all the different points to a function
  | Minimum length (characters) |         R^2 |
  |-----------------------------+-------------|
  |                           0 | 0.743868489 |
  |                       20000 |        0.71 |
  |                             |             |
  #+BEGIN_SRC python
for i in xrange(0,lexicalVariety,1000):
  print(i)
  #+END_SRC

  #+RESULTS:

#+BEGIN_SRC gnuplot :exports both :file sweep.png
set multiplot
set encoding utf8
set title "Lexical variety Vs Length"
set xlabel "Length in characters"
set ylabel "Unique characters"
set logscale x
set nologscale y
plot '/home/rcl/readability-measure/test/0936.tsv' title 'Jipin Jiading' linecolor 1, \
     '/home/rcl/readability-measure/test/1077-4000.tsv' title 'Cixi Quanzhuan' linecolor 2
     #'/home/rcl/readability-measure/zh-TW.tsv' title 'Chinese' linecolor 3
unset multiplot
#+END_SRC

#+RESULTS:
[[file:sweep.png]]


#+BEGIN_SRC gnuplot :exports both :file test.png
set multiplot
set encoding utf8
set title "Lexical variety Vs Length"
set xlabel "Length in characters"
set ylabel "Unique characters"
set logscale x
set nologscale y
plot '/home/rcl/readability-measure/zh-TW.tsv' title 'Jipin Jiading' linecolor 1, \
     #'/home/rcl/readability-measure/zh-TW.tsv' title 'Cixi Quanzhuan' linecolor 2
     #'/home/rcl/readability-measure/zh-TW.tsv' title 'Chinese' linecolor 3
unset multiplot
#+END_SRC

#+RESULTS:
[[file:test.png]]

#+BEGIN_SRC R :file R.png :results output graphics
dat <- read.csv("~/readability-measure/zh-TW.tsv", header=FALSE, sep="\t")
x = dat[, 1]
y = dat[, 2]

Estimate = lm(y ~ x)
logEstimate = lm(y ~ log(x))

plot(x,predict(Estimate),type='l',col='blue')
lines(x,predict(logEstimate),col='red')
plot(x, y, log ="x",
        type="p",
        pch = 1,
        xlab="Length (characters)",
        ylab="Unique characters (characters)")
#+END_SRC

#+RESULTS:
[[file:R.png]]

#+begin_src R :file 3.png :results output graphics
library(lattice)
xyplot(1:10 ~ 1:10)
#+end_src

#+RESULTS:
[[file:3.png]]
* Plotting

Perfect. It plots the first two columns and doesn't give an error about all the rest.
#+BEGIN_SRC gnuplot :exports both all_.png
set title "Lexical variety Vs Length"
set xlabel "Length in words"
set ylabel "Unique words"
set logscale x
set logscale y
es_filelist=system("ls es*.tsv")
fr_filelist=system("ls fr*.tsv")
pt_filelist=system("ls p*.tsv")
plot  for [filename in es_filelist] filename title 'Spanish' linecolor 1, \
      for [filename in fr_filelist] filename title 'French' linecolor 2, \
      for [filename in pt_filelist] filename title 'Portuguese' linecolor 3, \
      'ar.tsv' title 'Arabic' linecolor 4, \
      'zh-TW.tsv' title 'Chinese' linecolor 5
#+END_SRC

#+RESULTS:
[[file:languages.png]]

#+BEGIN_SRC gnuplot :exports both :file chinese.png
set title "Lexical variety Vs Length"
set xlabel "Length in characters"
set ylabel "Unique characters"
set logscale x
set nologscale y
plot 'zh-TW.tsv' title 'Chinese' linecolor 1
#+END_SRC

#+RESULTS:

#+BEGIN_SRC gnuplot :exports both :file arabic.png
set title "Lexical variety Vs Length"
set xlabel "Length in characters"
set ylabel "Unique characters"
set logscale x
set logscale y
plot 'ar.tsv' title 'Arabic' linecolor 1
#+END_SRC

#+BEGIN_SRC gnuplot :exports both :file all.png
set multiplot
set title "Lexical variety Vs Length"
set xlabel "Length in words"
set ylabel "Unique words"
#set logscale x
#set logscale y
set logscale x
set logscale y
filelist=system("ls *.tsv")
#plot  for [filename in filelist] filename title filename
plot 'spanish.tsv' title 'Spanish' linecolor 1, \
     'french.tsv' title 'French' linecolor 2, \
     'portuguese.tsv' title 'Portuguese' linecolor 3, \
     'ar.tsv' title 'Arabic' linecolor 4, \
     for [filename in filelist] filename title filename linecolor 5
unset multiplot
#+END_SRC

#+RESULTS:
[[file:all.png]]
