#+TITLE: Readability metric based on analysis of lexical variety
#+AUTHOR: Roland Coeurjoly
#+EMAIL: rolandcoeurjoly@gmail.com
#+Date:
#+OPTIONS: ^:nil toc:nil H:4
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usepackage{attrib}
#+LATEX_HEADER: \Plainauthor{Roland Coeurjoly}
#+LATEX_HEADER: \author{Roland Coeurjoly}
#+LATEX_HEADER: \title{Readability metric based on analysis of lexical variety}
#+LATEX_HEADER: \Shorttitle{Pending}
#+LATEX_HEADER: \Keywords{readability metric, readability test, readability, formula, comprehensible input, extensive reading, vocabulary, literate programming, reproducible research, emacs}
#+LATEX_HEADER: \Address{Pending}
#+LATEX_HEADER: \Abstract{We present a readability metric, capable of being applied to books written in multiple languages and easy to compute, therefore lending itself to be applied to large corpora composed of thousands of books. It uses length of text (metricd in words) versus unique words to compute the rate at which the author introduces new vocabulary in a certain book. This rate can then be used to rank the book with respect to others. This readability metric is only suitable to texts of at least 10.000 (ten thousand) words. It is therefore used primarely for the analysis of }
#+STARTUP: oddeven
* Summary
  The main idea of this paper is that the rate at which an author introduces new words into a book can function as a readability metric.
  This rate takes into account the number of unique words and the length in words of a book.
  So, in a simplified case, between two books with the same length, the one with fewer unique words is easiest to read, all other factors being equal.
  This readability metric is valuable to people who, having a large corpus of books, want to find those that are easiest to read in comparison with the rest of the corpus.
  In other words, helps you find comprehensible input.
  In this project we are going to develop a readability metric based on corpus analysis.
** What is a readability metric?
   A readability metric is a set of equations used to quantify how easy a given text is to read.
   In this project, we focus on long texts, with more that 2000 words or tokens. This is in contrast with other readability metrics which analyze sentences.
* Literature review
  Alexander Arguelles proposes the following:
* Approach
  We are going to approach the problem from a corpus linguistics perspective. This means that we are going to analyze a lot of books and take some general conclusions about them.
  The basic premise used in this project is that vocabulary and readability are correlated negatively, i.e., the larger the vocabulary, the more difficult to understand.

  We will use a literate programming approach using GNU/Emacs org mode to make our results as reproducible as possible.
* Corpora
** French
  #+BEGIN_SRC shell :exports code :tangle french-corpus.sh
for i in {1..11000}
do
    wget -O $1$i.epub bibliothequenumerique.tv5monde.com/download/epub/$i
    find ${1} -name "*.epub" -size -88k -delete
done
  #+END_SRC

  #+RESULTS:
** Chinese
The following is the perfect script for downloading Chinese books from haodoo (好讀).
It removes those in vertical format with -R "V*.epub".
  #+BEGIN_SRC shell :exports code :tangle chinese-corpus.sh
wget -np -P $1 -r -A .epub -R "V*.epub" http://haodoo.net/PDB/
  #+END_SRC

#+BEGIN_SRC shell :exports code
find ~/readability-metric/corpus/Chinese/haodoo.net/PDB/ -mindepth 1 -type f -name "*.epub" -exec printf x \; | wc -c
#+END_SRC

#+RESULTS:
: 3699

#+BEGIN_SRC shell :exports code
ls -ltu ~/readability-metric/corpus/Chinese/haodoo.net/PDB/A/*435.epub
#+END_SRC

#+RESULTS:
: -rw-rw-r-- 1 rcl rcl 130460 jul 19 16:04 /home/rcl/readability-metric/corpus/Chinese/haodoo.net/PDB/A/435.epub
** Arabic
   [[http://www.hindawi.
org/][http://www.hindawi.org/]]
   #+BEGIN_SRC shell :exports code :tangle arabic-corpus.sh
wget -np -P $1 -r -A .epub http://www.hindawi.org/books/
   #+END_SRC
** Russian
   #+BEGIN_SRC shell :exports code
wget -np -P ~/readability-metric/corpus/Russian/ -r -A .epub https://www.rulit.me/
   #+END_SRC
** All
   The Gutenberg project is also a good source of books.
   We could create different files for each language with length and lexical variety.
   Also, a convenient feature of the Gutenberg library is that it has ebooks with images and without.
   We download without images to save bandwidth.
   #+BEGIN_SRC shell :exports code
wget -w 2 -m -H "http://www.gutenberg.org/robot/harvest?filetypes[]=epub.noimages"
   #+END_SRC
* Python module
** Lexical sweep and curve fitting
   We can only do the curve fitting with books longer than 10000 tokens. This is because, to begin with, books don't exhibit logarithmic behavior until they reach around 4 thousand words.
   Moreover, we need to have enough samples to be able to feed the piece of software that does the curve fitting.
#+NAME: lexical-sweep
#+BEGIN_SRC python :noweb yes :session python :exports code
def lexical_sweep(text, samples=10, log_x=False, log_y=False):
    '''
    Lexical sweep.
    '''
    log_behaviour_start = 5000
    sweep_values = []
    log_behaviour_range = len(text) - log_behaviour_start
    log_step = log_behaviour_range/(samples - 1)
    if len(text) > 10000 and samples >= 2:
        for sample_size in xrange(
                log_behaviour_start,
                len(text) - 1,
                log_step):
            if log_x:
                x_sample = log(sample_size)
            else:
                x_sample = sample_size
            if log_y:
                y_sample = log(len(set(text[0:sample_size])))
            else:
                y_sample = len(set(text[0:sample_size]))
            sweep_values.append([x_sample, y_sample])
        return sweep_values
    return False
#+END_SRC

#+NAME: curve-fit
#+BEGIN_SRC python :noweb yes :session python :exports code
def extract_fit_parameters(function, sweep_values):
    '''
    Curve fit.
    '''
    if sweep_values:
        array = list(zip(*sweep_values))
        xarr = array[0]
        yarr = array[1]
        initial_a = 0
        initial_b = 0
        popt, pcov = curve_fit(function, xarr, yarr, (initial_a, initial_b))
        slope = popt[0]
        intercept = popt[1]
        perr = np.sqrt(np.diag(pcov))
        std_error_slope = perr[0]
        std_error_intercept = perr[1]
        return {'intercept': intercept,
                'slope': slope,
                'std_error_intercept': std_error_intercept,
                'std_error_slope': std_error_slope}
    return {'intercept': int(),
            'slope': int(),
            'std_error_intercept': int(),
            'std_error_slope': int()}
#+END_SRC

#+RESULTS: lexical-sweep

Empirically, We have found
#+NAME: fit-functions
#+begin_src python :noweb yes :session python :exports code
def linear_func(variable, slope, y_intercept):
    '''
    Linear model.
    '''
    return slope*variable + y_intercept

def log_func(variable, coefficient, x_intercept):
    '''
    Logarithmic model.
    '''
    return coefficient*log(variable) + x_intercept

def log_log_func(variable, coefficient, intercept):
    '''
    Log-log model.
    '''
    return math.e**(coefficient*log(variable) + intercept)
#+end_src

** Ebook handling

   We then proceed to open the epub and extract all metadata.
   As stated in the [[https://ebooklib.readthedocs.io/en/latest/tutorial.html#reading-epub][package documentation]], only creator, title and language are required metadata fields.
   The rest is optional, so we catch them with care.

   We then use BeautifulSoup to remove all html marks.
*** Class book
#+NAME: book-class
#+BEGIN_SRC python :noweb yes :session python :exports code
class Book(object):
    '''
    Book class
    '''
    # pylint: disable=too-many-instance-attributes
    # There is a lot of metadata but it is repetitive and non problematic.
    <<constructor>>
    <<tokenization>>
    <<text-extraction>>
    <<language-detection>>
    <<release-text>>
    <<release-zh-characters>>
    <<release-tokens>>
    #+end_src

*** Extracting metadata
    We don't extract all text in constructor because it is expensive and we want to check first if it exists in database.
#+NAME: constructor
#+BEGIN_SRC python :noweb yes :session python :exports code
def __init__(self, epub_filename):
    '''
    Init.
    '''
    # pylint: disable=too-many-statements
    # There is a lot of metadata but it is repetitive and non problematic.
    self.filename = epub_filename
    epub_file = epub.read_epub(epub_filename)
    try:
        self.epub_type = epub_file.get_metadata('DC', 'type')[0][0].encode('utf-8')
    except (IndexError, AttributeError):
        self.epub_type = ''
    try:
        self.subject = epub_file.get_metadata('DC', 'subject')[0][0].encode('utf-8')
    except (IndexError, AttributeError):
        self.subject = ''
    try:
        self.source = epub_file.get_metadata('DC', 'source')[0][0].encode('utf-8')
    except (IndexError, AttributeError):
        self.source = ''
    try:
        self.rights = epub_file.get_metadata('DC', 'rights')[0][0].encode('utf-8')
    except (IndexError, AttributeError):
        self.rights = ''
    try:
        self.relation = epub_file.get_metadata('DC', 'relation')[0][0].encode('utf-8')
    except (IndexError, AttributeError):
        self.relation = ''
    try:
        self.publisher = epub_file.get_metadata('DC', 'publisher')[0][0].encode('utf-8')
    except (IndexError, AttributeError):
        self.publisher = ''
    #try:
    #    self.language = epub_file.get_metadata('DC', 'language')[0][0].encode('utf-8')
    #except (IndexError, AttributeError):
    #    self.language = 'empty'
    try:
        self.identifier = epub_file.get_metadata('DC', 'identifier')[0][0].encode('utf-8')
    except (IndexError, AttributeError):
        self.identifier = ''
    try:
        self.epub_format = epub_file.get_metadata('DC', 'format')[0][0].encode('utf-8')
    except (IndexError, AttributeError):
        self.epub_format = ''
    try:
        self.description = epub_file.get_metadata('DC', 'description')[0][0].encode('utf-8')
    except (IndexError, AttributeError):
        self.description = ''
    try:
        self.coverage = epub_file.get_metadata('DC', 'coverage')[0][0].encode('utf-8')
    except (IndexError, AttributeError):
        self.coverage = ''
    try:
        self.contributor = epub_file.get_metadata('DC', 'contributor')[0][0].encode('utf-8')
    except (IndexError, AttributeError):
        self.contributor = ''
    self.author = epub_file.get_metadata('DC', 'creator')[0][0].encode('utf-8')
    self.title = epub_file.get_metadata('DC', 'title')[0][0].encode('utf-8')
    try:
        self.date = epub_file.get_metadata('DC', 'date')[0][0].encode('utf-8')
    except (IndexError, AttributeError):
        self.date = ''
    self.language = str()
    self.zh_characters = str()
    self.character_count = int()
    self.unique_characters = int()
    self.tokens = str()
    self.word_count = int()
    self.unique_words = int()
    self.text = str()
#+END_SRC
*** Extracting text from ebook
#+NAME: text-extraction
#+BEGIN_SRC python :noweb yes :session python :exports code
def extract_text(self):
    '''
    Extract all text from the book.
    '''
    book = epub.read_epub(self.filename)
    cleantext = ""
    html_filtered = ""
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            raw_html = item.get_content()
            <<html-filtering>>
    cleantext = clean_non_printable(html_filtered)
    self.text = cleantext
#+END_SRC

#+RESULTS: text-extraction
**** Cleaning the html
#+NAME: html-filtering
#+BEGIN_SRC python :noweb yes :session python :exports code
html_filtered += BeautifulSoup(raw_html, "lxml").text
#+END_SRC

#+RESULTS: html-cleaning
**** Removing invalid utf-8

#+NAME: printable-set
#+BEGIN_SRC python :noweb yes :session python :exports code
PRINTABLE = {
    #'Cc',
    'Cf',
    'Cn',
    'Co',
    'Cs',
    'LC',
    'Ll',
    'Lm',
    'Lo',
    'Lt',
    'Lu',
    'Mc',
    'Me',
    'Mn',
    'Nd',
    'Nl',
    'No',
    'Pc',
    'Pd',
    'Pe',
    'Pf',
    'Pi',
    'Po',
    'Ps',
    'Sc',
    'Sk',
    'Sm',
    'So',
    'Zl',
    'Zp',
    'Zs'}
     #+end_src

#+NAME: utf8-cleaning
#+BEGIN_SRC python :noweb yes :session python :exports code
def clean_non_printable(text):
    '''
    Remove all non printable characters from string.
    '''
    return ''.join(character for character in text if unicodedata.category(character) in PRINTABLE)
#+END_SRC
**** Language detection
#+NAME: language-detection
#+begin_src python :noweb yes :session python :exports code
def detect_language(self):
    '''
    We don't trust the epub metadata regarding language tags
    so we do our own language detection
    '''
    if not self.tokens:
        self.extract_text()
    self.language = Text(self.text).language.code
#+end_src

*** Tokenization
    If the language is Chinese, appart from doing the tokenization, we also metric individual characters.
#+NAME: tokenization
#+BEGIN_SRC python :noweb yes :session python :exports code
def tokenize(self):
    '''
    Tokenization.
    '''
    if not self.tokens:
        self.extract_text()
    if self.language == 'zh' or self.language == 'zh_Hant':
        self.zh_characters = ''.join(character for character in self.text
                                     if u'\u4e00' <= character <= u'\u9fff')
        self.character_count = len(self.zh_characters)
        self.unique_characters = len(set(self.zh_characters))
    else:
        self.zh_characters = str()
        self.character_count = int()
        self.unique_characters = int()
    self.tokens = Text(self.text).words
    self.word_count = len(self.tokens)
    self.unique_words = len(set(self.tokens))
#+END_SRC
*** Release text
    I conjecturize that holding a lot of text in memory is very expensive.
#+NAME: release-text
#+BEGIN_SRC python :noweb yes :session python :exports code
def release_text(self):
    '''
    Release text.
    '''
    self.text = str()
#+END_SRC
#+NAME: release-zh-characters
#+BEGIN_SRC python :noweb yes :session python :exports code
def release_zh_characters(self):
    '''
    Release Chinese characters.
    '''
    self.zh_characters = str()
    #+END_SRC
#+NAME: release-tokens
#+BEGIN_SRC python :noweb yes :session python :exports code
def release_tokens(self):
    '''
    Release tokens.
    '''
    self.tokens = str()
#+END_SRC

** Vocabulary coverage
   #+NAME: vocabulary_coverage
   #+begin_src python :noweb yes :exports code :session sahj :tangle vocabulary_coverage.py :results output
'''
Random
'''
from nltk import FreqDist
import corpus_analysis


MY_BOOK = corpus_analysis.Book("./test/pinocchio.epub")
MY_BOOK.tokenize()
MY_FREQDIST = FreqDist(MY_BOOK.tokens)
print MY_BOOK.word_count
percentage = 0
cumulative_word_count = 0
coverage = 1
print MY_FREQDIST.most_common(coverage)[coverage - 1][1]
margin_unknowable_list = MY_FREQDIST.most_common(MY_BOOK.word_count - 1) - MY_FREQDIST.most_common(int(round((MY_BOOK.word_count - 1)*0.98)))
last_word_frequency = MY_FREQDIST.most_common(coverage)[coverage - 1][1]
coverage += 1
cumulative_word_count += last_word_frequency
percentage = (cumulative_word_count*100/MY_BOOK.word_count)
print margin_unknowable_list
   #+end_src

   #+RESULTS: vocabulary_coverage
   : 52544
   : 3345
   : Traceback (most recent call last):
   :   File "<stdin>", line 1, in <module>
   :   File "/tmp/babel-2FdH2m/python-VLLu9V", line 16, in <module>
   :     margin_unknowable_list = MY_FREQDIST.most_common(MY_BOOK.word_count - 1) - MY_FREQDIST.most_common(int(round((MY_BOOK.word_count - 1)*0.98)))
   : TypeError: unsupported operand type(s) for -: 'list' and 'list'

** Learnable words
** Main
 #+NAME: main
 #+BEGIN_SRC python :noweb yes :session python :exports code
def analyse_book(ebook, samples=10):
    '''
    Analyse individual book.
    You can insert into db or into json afterwards
    '''
    my_book = Book(ebook)
    my_book.extract_text()
    my_book.detect_language()
    my_book.tokenize()
    sweep_values = lexical_sweep(my_book.tokens,
                                 samples,
                                 log_x=True,
                                 log_y=True)
    try:
        word_curve_fit = extract_fit_parameters(linear_func, sweep_values)
    except TypeError as ex:
        print ex
        return False
    sweep_values = lexical_sweep(my_book.zh_characters,
                                 samples,
                                 log_x=True,
                                 log_y=False)
    try:
        zh_character_curve_fit = extract_fit_parameters(linear_func, sweep_values)
    except TypeError as ex:
        print ex
        return False
    return my_book, word_curve_fit, zh_character_curve_fit

def analyse_books(argv, db):
    '''
    Main function: open and read all epub files in directory.
    Analyze them and populate data in database
    :param argv: command line args.
    '''
    create_database(db)
    books_analyzed = 0
    for dirpath, __, files in os.walk(str(argv[1])):
        for ebook in files:
            if ebook.endswith(".epub"):
                try:
                    my_book = Book(dirpath + '/' + ebook)
                except KeyError as ex:
                    print ex
                    continue
                if db == "library":
                    print "Checking if book exists in database"
                    if is_book_in_db(my_book.title, my_book.author):
                        continue
                try:
                    result = analyse_book(dirpath + '/' + ebook)
                    if not result:
                        continue
                except TypeError as ex:
                    print ex
                    continue
                my_book, word_curve_fit, zh_character_curve_fit = result[0], result[1], result[2]
                print "Reading ebook " + ebook + ", number  " + str(books_analyzed)
                print "Writing to database"
                insert_book_db(my_book, word_curve_fit, zh_character_curve_fit, db)
                books_analyzed += 1
                if len(sys.argv) == 3:
                    if str(sys.argv[2]).endswith(".sql"):
                        runbackup("localhost", "root", "root", str(sys.argv[2]))
                else:
                    runbackup("localhost", "root", "root")
    MY_DB.close()

if __name__ == '__main__':
    analyse_books(sys.argv, "library")

 #+END_SRC

 #+RESULTS: epub-handling
** Imports
   We import some packages to make our life easier:
   - ebooklib: to process epubs
   - BeautifulSoup: to process the html in epubs
   - langdetect to detect language. We use this because based on experience epub language tags are not very reliable
   - ntlk: to do natural language processing
#+NAME: imports
#+BEGIN_SRC python :session python :results none :exports code
import unicodedata
import sys
import os
import math
import subprocess
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
from scipy.optimize import curve_fit
from scipy import log as log
import numpy as np
import mysql.connector
from polyglot.text import Text
#+END_SRC

** Architecture
   In a first instance, we want to extract the following information from each ebook:
  - Author
  - Title
  - Length in number of words
  - Number of unique words
  It would be nice to create a file for each language (according to metadata).
  The logic would be the following:
  Try adding the results to a file suffixed with the language code.
  If that throws an exception, create the file and add the results
#+BEGIN_SRC python :noweb yes :tangle corpus_analysis.py :exports code
# -*- coding: utf-8 -*-
'''
corpus-analysis.py: readability metric for epub ebooks.
Version 1.0
Copyright (C) 2019  Roland Coeurjoly <rolandcoeurjoly@gmail.com>
'''
# Imports
<<imports>>
# Constants
<<printable-set>>
# Classes
## Book Class
<<book-class>>
# Functions
<<utf8-cleaning>>
## Curve fitting functions
<<curve-fit>>
<<lexical-sweep>>
<<fit-functions>>
## Database functions
<<db-connection>>
<<database-insertion>>
<<database-creation>>
<<is-book-in-db>>
<<db-backup>>
# Main function
<<main>>
#+END_SRC

#+RESULTS:
: None

* Arguelles Analysis
** Python
*** Whole length
#+NAME: moby
#+BEGIN_SRC python :exports code :session readability_metric :results output
from corpus_analysis import Book

moby_dick = Book("test/moby.epub")
moby_dick.extract_text()
moby_dick.tokenize()
print moby_dick.title
with open('moby.tsv', 'w+') as my_file:
    my_file.write(str(moby_dick.word_count) + "\t" + str(moby_dick.unique_words) + "\n")
#+END_SRC

#+RESULTS: moby
: Moby Dick; Or, The Whale

#+NAME: pinocchio
#+BEGIN_SRC python :exports code :session readability_metric :results output
from corpus_analysis import Book

pinocchio = Book("test/pinocchio.epub")
pinocchio.extract_text()
pinocchio.tokenize()
print pinocchio.title

with open('pinocchio.tsv', 'w') as my_file:
    my_file.write(str(pinocchio.word_count) + "\t" + str(pinocchio.unique_words) + "\n")
#+END_SRC
#+RESULTS: pinocchio
: The Adventures of Pinocchio
*** Sweep
#+NAME: moby_sweep
#+BEGIN_SRC python :exports code :session readability_metric :results output
from corpus_analysis import Book

moby_dick = Book("test/moby.epub")
moby_dick.extract_text()
moby_dick.tokenize()
print moby_dick.title
sweep_values = lexical_sweep(moby_dick.tokens, samples=100, log_x=False, log_y=False)
with open('moby_sweep.tsv', 'w') as my_file:
    for sweep_value in sweep_values:
        my_file.write(str(sweep_value[0]) + "\t" + str(sweep_value[1]) + "\n")
#+END_SRC

#+RESULTS: moby_sweep
: Moby Dick; Or, The Whale

#+NAME: pinocchio_sweep
#+BEGIN_SRC python :exports code :session readability_metric :results output
from corpus_analysis import Book, lexical_sweep

pinocchio = Book("test/pinocchio.epub")
pinocchio.extract_text()
pinocchio.tokenize()
print pinocchio.title
sweep_values = lexical_sweep(pinocchio.tokens, samples=100, log_x=False, log_y=False)
with open('pinocchio_sweep.tsv', 'w') as my_file:
    for sweep_value in sweep_values:
        my_file.write(str(sweep_value[0]) + "\t" + str(sweep_value[1]) + "\n")
#+END_SRC

#+RESULTS: pinocchio_sweep
: The Adventures of Pinocchio
** Plot
#+NAME: moby_pinocchio_plot
#+BEGIN_SRC gnuplot :var pinocchio_title=pinocchio moby_title=moby :exports both moby_pinocchio.png
reset
set xrange [0:300000]
set yrange [0:25000]
set key autotitle columnhead
set style line 1 lw 4 lc rgb '#990042' ps 2 pt 6 pi 5
set style line 2 lw 3 lc rgb '#31f120' ps 2 pt 12 pi 3
set title "Lexical variety Vs Length"
set title pinocchio_title
set xlabel "Length in words"
set ylabel "Unique words"
plot "moby.tsv" ls 1 title moby_title, \
     "pinocchio.tsv" ls 2 title pinocchio_title
#+END_SRC

#+RESULTS: moby_pinocchio_plot

#+RESULTS:

#+NAME: moby_pinocchio_sweep_plot
#+BEGIN_SRC gnuplot :var pinocchio_title=pinocchio_sweep moby_title=moby_sweep :exports both :file moby_pinocchio.png
reset
set xrange [4000:400000]
set yrange [1000:40000]
set logscale x
set logscale y
set style line 1 lw 4 lc rgb '#990042' ps 2 pt 6 pi 5
set style line 2 lw 3 lc rgb '#31f120' ps 2 pt 12 pi 3
set title pinocchio_title
set title "Pinocchio and Moby Dick comparison"
#set title "Lexical variety Vs Length"
set xlabel "Length in words"
set ylabel "Unique words"
plot "moby_sweep.tsv" ls 1 title moby_title, \
     "pinocchio_sweep.tsv" ls 2 title pinocchio_title
#+END_SRC

#+RESULTS: moby_pinocchio_sweep_plot
[[file:moby_pinocchio.png]]

#+RESULTS:

#+begin_src gnuplot :exports both file.png
reset

set title "Putting it All Together"

set xlabel "X"
set xrange [-8:8]
set xtics -8,2,8


set ylabel "Y"
set yrange [-20:70]
set ytics -20,10,70

2f(x) = x**2
g(x) = x**3
h(x) = 10*sqrt(abs(x))
i(x) = 15*sin(x)

plot f(x) w lp lw 1, g(x) w p lw 2, h(x) w l lw 3, i(x) w l lw 4
#+end_src

#+RESULTS:

* Testing
#+BEGIN_SRC python :exports code :noweb yes :tangle test_corpus_analysis.py
# -*- coding: utf-8 -*-
'''
Unit testing for the corpus analysis
'''
import timeout_decorator
import unittest
import json
import mysql
from decimal import *
from ebooklib import epub
from corpus_analysis import Book, lexical_sweep, extract_fit_parameters, linear_func, analyse_books

class MyTest(unittest.TestCase):
    '''
    Class
    '''
    @timeout_decorator.timeout(1)
    def test_metadata(self):
        '''
        Given a certain book, test metadata
        '''
        with open("benchmarks.json", "r") as test_cases:
            benchmarks = json.load(test_cases)
            for benchmark in benchmarks['books']:
                my_book = Book(benchmark['path'].encode('utf-8'))
                self.assertEqual(my_book.author, benchmark['author'].encode('utf-8'))
                self.assertEqual(my_book.title, benchmark['title'].encode('utf-8'))
                self.assertEqual(my_book.epub_type, benchmark['epub_type'].encode('utf-8'))
                self.assertEqual(my_book.subject, benchmark['subject'].encode('utf-8'))
                self.assertEqual(my_book.rights, benchmark['rights'].encode('utf-8'))
                self.assertEqual(my_book.relation, benchmark['relation'].encode('utf-8'))
                self.assertEqual(my_book.publisher, benchmark['publisher'].encode('utf-8'))
                self.assertEqual(my_book.identifier, benchmark['identifier'].encode('utf-8'))
                self.assertEqual(my_book.epub_format, benchmark['epub_format'].encode('utf-8'))
                self.assertEqual(my_book.description, benchmark['description'].encode('utf-8'))
                self.assertEqual(my_book.contributor, benchmark['contributor'].encode('utf-8'))
                self.assertEqual(my_book.date, benchmark['date'].encode('utf-8'))
                print "Metadata for " + benchmark['title'].encode('utf-8') + " OK"

    @timeout_decorator.timeout(19)
    def test_language(self):
        '''
        Given a certain book, test language
        '''
        with open("benchmarks.json", "r") as test_cases:
            benchmarks = json.load(test_cases)
            for benchmark in benchmarks['books']:
                my_book = Book(benchmark['path'].encode('utf-8'))
                my_book.extract_text()
                my_book.detect_language()
                self.assertEqual(my_book.language, benchmark['language'].encode('utf-8'))
                print "Language for " + benchmark['title'].encode('utf-8') + " OK"

    @timeout_decorator.timeout(50)
    def test_tokens(self):
        '''
        Given a certain book, test language
        '''
        with open("benchmarks.json", "r") as test_cases:
            benchmarks = json.load(test_cases)
            for benchmark in benchmarks['books']:
                my_book = Book(benchmark['path'].encode('utf-8'))
                my_book.extract_text()
                my_book.detect_language()
                my_book.tokenize()
                self.assertEqual(my_book.word_count, benchmark['word_count'])
                self.assertEqual(my_book.unique_words, benchmark['unique_words'])
                self.assertEqual(my_book.character_count, benchmark['zh_character_count'])
                self.assertEqual(my_book.unique_characters, benchmark['unique_zh_characters'])
                print "Tokens for " + benchmark['title'].encode('utf-8') + " OK"

    @timeout_decorator.timeout(145)
    def test_fit(self):
        '''
        Given a certain book, test language
        '''
        with open("benchmarks.json", "r") as test_cases:
            benchmarks = json.load(test_cases)
            for benchmark in benchmarks['books']:
                my_book = Book(benchmark['path'].encode('utf-8'))
                my_book.extract_text()
                my_book.detect_language()
                my_book.tokenize()
                sweep_values = lexical_sweep(my_book.tokens, samples=10, log_x=True, log_y=True)
                word_curve_fit = extract_fit_parameters(linear_func, sweep_values)
                sweep_values = lexical_sweep(my_book.zh_characters, samples=10, log_x=True)
                zh_character_curve_fit = extract_fit_parameters(linear_func, sweep_values)
                self.assertEqual(float(word_curve_fit['slope']),
                                 benchmark['word_curve_fit_slope'])
                self.assertEqual(float(word_curve_fit['intercept']),
                                 benchmark['word_curve_fit_intercept'])
                self.assertEqual(float(word_curve_fit['std_error_slope']),
                                 benchmark['word_curve_fit_std_error_slope'])
                self.assertEqual(float(word_curve_fit['std_error_intercept']),
                                 benchmark['word_curve_fit_std_error_intercept'])
                self.assertEqual(float(zh_character_curve_fit['slope']),
                                 benchmark['zh_character_curve_fit_slope'])
                self.assertEqual(float(zh_character_curve_fit['intercept']),
                                 benchmark['zh_character_curve_fit_intercept'])
                self.assertEqual(float(zh_character_curve_fit['std_error_slope']),
                                 benchmark['zh_character_curve_fit_std_error_slope'])
                self.assertEqual(float(zh_character_curve_fit['std_error_intercept']),
                                 benchmark['zh_character_curve_fit_std_error_intercept'])
                print "Fit for " + benchmark['title'].encode('utf-8') + " OK"

    @timeout_decorator.timeout(260)
    def test_db_writing(self):
        '''
        Write all books to database
        '''
        query_pattern = """Select title,
        author,
        slope,
        intercept,
        std_error_slope,
        std_error_intercept,
        word_count,
        unique_words,
        zhslope,
        zhintercept,
        zhstd_error_slope,
        zhstd_error_intercept,
        character_count,
        unique_characters,
        language,
        epub_type,
        subject,
        source,
        rights,
        relation,
        publisher,
        identifier,
        epub_format,
        description,
        contributor,
        date from corpus
        """
        expected_result_Xueqin = [(u'\u7d05\u6a13\u5922', u'Xueqin Cao',
                                   Decimal('0.49438'), Decimal('3.36368'),
                                   Decimal('0.01678'), Decimal('0.20654'),
                                   Decimal('662992.0'), Decimal('21113.0'),
                                   Decimal('636.13906'), Decimal('-4277.28846'),
                                   Decimal('5.36047'), Decimal('66.41762'),
                                   Decimal('724567.0'), Decimal('4263.0'),
                                   u'zh_Hant', u'',
                                   u'China -- History -- Qing dynasty, 1644-1912 -- Fiction',
                                   u'http://www.gutenberg.orgfiles/24264/24264-0.txt',
                                   u'Public domain in the USA.', u'', u'',
                                   u'http://www.gutenberg.org/ebooks/24264', u'', u'', u'',
                                   u'2008-01-12')]

        expected_result_Collodi = [(u'The Adventures of Pinocchio',
                                    u'Carlo Collodi', Decimal('0.56476'),
                                    Decimal('2.29671'), Decimal('0.01358'),
                                    Decimal('0.13704'), Decimal('52544.0'),
                                    Decimal('4945.0'), Decimal('0.00000'),
                                    Decimal('0.00000'), Decimal('0.00000'),
                                    Decimal('0.00000'), Decimal('0.0'), Decimal('0.0'),
                                    u'en', u'', u'Fairy tales',
                                    u'http://www.gutenberg.org/files/500/500-h/500-h.htm',
                                    u'Public domain in the USA.', u'', u'',
                                    u'http://www.gutenberg.org/ebooks/500', u'', u'',
                                    u'Carol Della Chiesa', u'2006-01-12')]

        expected_result_Goethe = [(u'Faust: Eine Trag\xf6die', u'Johann Wolfgang von Goethe',
                                   Decimal('0.76069'), Decimal('1.12047'), Decimal('0.00841'),
                                   Decimal('0.08245'), Decimal('36751.0'), Decimal('9293.0'),
                                   Decimal('0.00000'), Decimal('0.00000'), Decimal('0.00000'),
                                   Decimal('0.00000'), Decimal('0.0'), Decimal('0.0'), u'de',
                                   u'', u'German poetry',
                                   u'http://www.gutenberg.org/files/21000/21000-h/21000-h.htm',
                                   u'Public domain in the USA.', u'', u'',
                                   u'http://www.gutenberg.org/ebooks/21000', u'', u'', u'',
                                   u'2007-04-06')]

        expected_result_Melville = [(u'Moby Dick; Or, The Whale', u'Herman Melville',
                                     Decimal('0.62059'), Decimal('2.24768'), Decimal('0.00923'),
                                     Decimal('0.10468'), Decimal('260447.0'), Decimal('20825.0'),
                                     Decimal('0.00000'), Decimal('0.00000'), Decimal('0.00000'),
                                     Decimal('0.00000'), Decimal('0.0'), Decimal('0.0'), u'en',
                                     u'', u'Whaling -- Fiction',
                                     u'http://www.gutenberg.org/files/2701/2701-h/2701-h.htm',
                                     u'Public domain in the USA.', u'', u'',
                                     u'http://www.gutenberg.org/ebooks/2701', u'', u'', u'',
                                     u'2001-07-01')]

        expected_result_Defoe = [(u'The Life and Adventures of Robinson Crusoe',
                                  u'Daniel Defoe', Decimal('0.54545'), Decimal('2.44881'),
                                  Decimal('0.00879'), Decimal('0.09605'), Decimal('141776.0'),
                                  Decimal('7643.0'), Decimal('0.00000'), Decimal('0.00000'),
                                  Decimal('0.00000'), Decimal('0.00000'), Decimal('0.0'),
                                  Decimal('0.0'), u'en', u'', u'Shipwreck survival -- Fiction',
                                  u'http://www.gutenberg.org/files/521/521-h/521-h.htm',
                                  u'Public domain in the USA.', u'', u'',
                                  u'http://www.gutenberg.org/ebooks/521', u'', u'', u'',
                                  u'1996-05-01')]

        expected_result_Baudelaire = [(u'Les Fleurs du Mal', u'Charles Baudelaire', Decimal('0.74097'),
                                       Decimal('1.32195'), Decimal('0.00444'), Decimal('0.04306'),
                                       Decimal('31525.0'), Decimal('8177.0'), Decimal('0.00000'),
                                       Decimal('0.00000'),
                                       Decimal('0.00000'), Decimal('0.00000'), Decimal('0.0'), Decimal('0.0'),
                                       u'fr', u'', u'Poetry',
                                       u'http://www.gutenberg.org/files/6099/6099-h/6099-h.htm',
                                       u'Public domain in the USA.', u'', u'',
                                       u'http://www.gutenberg.org/ebooks/6099', u'', u'', u'',
                                       u'2004-07-01')]

        expected_result_Saavedra = [(u'Don Quijote', u'Miguel de Cervantes Saavedra', Decimal('0.64185'),
                                     Decimal('1.85563'), Decimal('0.01072'), Decimal('0.12811'),
                                     Decimal('449755.0'), Decimal('27284.0'), Decimal('0.00000'),
                                     Decimal('0.00000'), Decimal('0.00000'), Decimal('0.00000'),
                                     Decimal('0.0'), Decimal('0.0'), u'es', u'',
                                     u'Spain -- Social life and customs -- 16th century -- Fiction',
                                     u'http://www.gutenberg.org/files/2000/2000-h/2000-h.htm',
                                     u'Public domain in the USA.', u'', u'',
                                     u'http://www.gutenberg.org/ebooks/2000', u'', u'', u'',
                                     u'1999-12-01')]

        expected_result_Descartes = [(u'Meditationes de prima philosophia', u'Ren\xe9 Descartes',
                                      Decimal('0.57913'), Decimal('2.70417'), Decimal('0.02310'),
                                      Decimal('0.22193'), Decimal('28207.0'), Decimal('6085.0'),
                                      Decimal('0.00000'), Decimal('0.00000'), Decimal('0.00000'),
                                      Decimal('0.00000'), Decimal('0.0'), Decimal('0.0'), u'la', u'',
                                      u'First philosophy',
                                      u'http://www.gutenberg.org/files/23306/23306-h/23306-h.htm',
                                      u'Public domain in the USA.', u'', u'',
                                      u'http://www.gutenberg.org/ebooks/23306', u'', u'', u'',
                                      u'2007-11-03')]

        <<db-connection>>
        mycursor = MY_DB.cursor()
        mycursor.execute("DROP DATABASE IF EXISTS library_test;")
        my_args = ["lol", "test/", "db/library_test.db"]
        analyse_books(my_args, "library_test")
        mycursor = MY_DB.cursor()
        mycursor.execute("USE library_test;")
        query_Xueqin = (query_pattern + ' where author="Xueqin Cao"')
        mycursor.execute(query_Xueqin)
        self.assertEqual(mycursor.fetchall(), expected_result_Xueqin)
        query_Collodi = (query_pattern + ' where author="Carlo Collodi"')
        mycursor.execute(query_Collodi)
        self.assertEqual(mycursor.fetchall(), expected_result_Collodi)
        query_Goethe = (query_pattern + ' where author="Johann Wolfgang von Goethe"')
        mycursor.execute(query_Goethe)
        self.assertEqual(mycursor.fetchall(), expected_result_Goethe)
        query_Melville = (query_pattern + ' where author="Herman Melville"')
        mycursor.execute(query_Melville)
        self.assertEqual(mycursor.fetchall(), expected_result_Melville)
        query_Defoe = (query_pattern + ' where author="Daniel Defoe"')
        mycursor.execute(query_Defoe)
        self.assertEqual(mycursor.fetchall(), expected_result_Defoe)
        query_Baudelaire = (query_pattern + ' where author="Charles Baudelaire"')
        mycursor.execute(query_Baudelaire)
        self.assertEqual(mycursor.fetchall(), expected_result_Baudelaire)
        query_Saavedra = (query_pattern + ' where author="Miguel de Cervantes Saavedra"')
        mycursor.execute(query_Saavedra)
        self.assertEqual(mycursor.fetchall(), expected_result_Saavedra)
        query_Descartes = (query_pattern + ' where author="René Descartes"')
        mycursor.execute(query_Descartes)
        self.assertEqual(mycursor.fetchall(), expected_result_Descartes)
        mycursor = MY_DB.cursor()
        mycursor.execute("drop database library_test;")

if __name__ == '__main__':
    unittest.main(failfast=True)
  #+end_src

  #+RESULTS:

** Creating benchmark

#+BEGIN_SRC python :noweb yes :tangle create_benchmark.py :exports code
'''
Create benchmark based on epubs
'''

import json
import os
from corpus_analysis import analyse_book

DATA = {}
DATA['books'] = []

for dirpath, __, files in os.walk('test'):
    for ebook in files:
        try:
            my_book, word_curve_fit, zh_character_curve_fit = analyse_book(dirpath
                                                                           + '/'
                                                                           + ebook)
        except TypeError as ex:
            print ex
            continue
        DATA['books'].append({"path": dirpath + "/" + ebook,
                              "author": my_book.author,
                              "title": my_book.title,
                              "epub_type": my_book.epub_type,
                              "subject": my_book.subject,
                              "rights": my_book.rights,
                              "relation": my_book.relation,
                              "publisher": my_book.publisher,
                              "identifier": my_book.identifier,
                              "epub_format": my_book.epub_format,
                              "description": my_book.description,
                              "contributor": my_book.contributor,
                              "date": my_book.date,
                              "language": my_book.language,
                              "word_count": my_book.word_count,
                              "unique_words": my_book.unique_words,
                              "zh_character_count": my_book.character_count,
                              "unique_zh_characters": my_book.unique_characters,
                              "word_curve_fit_slope":
                              word_curve_fit['slope'],
                              "word_curve_fit_intercept":
                              word_curve_fit['intercept'],
                              "word_curve_fit_std_error_slope":
                              word_curve_fit['std_error_slope'],
                              "word_curve_fit_std_error_intercept":
                              word_curve_fit['std_error_intercept'],
                              "zh_character_curve_fit_slope":
                              zh_character_curve_fit['slope'],
                              "zh_character_curve_fit_intercept":
                              zh_character_curve_fit['intercept'],
                              "zh_character_curve_fit_std_error_slope":
                              zh_character_curve_fit['std_error_slope'],
                              "zh_character_curve_fit_std_error_intercept":
                              zh_character_curve_fit['std_error_intercept']})

with open('benchmarks.json', 'w') as outfile:
    json.dump(DATA, outfile)
#+end_src

** Downloading books for benchmark

#+BEGIN_SRC shell :exports code :tangle download_benchmark.sh
mkdir test/db
mkdir test/books
wget https://www.gutenberg.org/ebooks/24264.epub.noimages?session_id=13a48cb17a2a788bd0df32eb9d11b2cc90e5ffb6 -O test/books/hongloumeng.epub
wget https://www.gutenberg.org/ebooks/6099.epub.noimages?session_id=e525c6c0f4f2faf96f365aabedf179ef08f4f236 -O test/books/lesfleursdumal.epub
wget https://www.gutenberg.org/ebooks/21000.epub.noimages?session_id=e525c6c0f4f2faf96f365aabedf179ef08f4f236 -O test/books/faust.epub
wget https://www.gutenberg.org/ebooks/23306.epub.noimages?session_id=13a48cb17a2a788bd0df32eb9d11b2cc90e5ffb6 -O test/books/meditationes.epub
wget https://www.gutenberg.org/ebooks/2000.epub.noimages?session_id=13a48cb17a2a788bd0df32eb9d11b2cc90e5ffb6 -O test/books/Quijote.epub
wget https://www.gutenberg.org/ebooks/521.epub.noimages?session_id=13a48cb17a2a788bd0df32eb9d11b2cc90e5ffb6 -O test/books/crusoe.epub
wget https://www.gutenberg.org/ebooks/2701.epub.noimages?session_id=37b8b8ef79424fa1e6b7a18eb4b341d5de076f03 -O test/books/moby.epub
wget https://www.gutenberg.org/ebooks/500.epub.noimages?session_id=37b8b8ef79424fa1e6b7a18eb4b341d5de076f03 -O test/books/pinocchio.epub
   #+end_src

   #+RESULTS:

** TypeError: Improper input: N=2 must not exceed M=1
   sweep_values = lexical_sweep(my_book.tokens, samples=1)

   sweep_values = lexical_sweep(my_book.tokens, samples=2)
   OptimizeWarning: Covariance of the parameters could not be estimated

* SQL DB
#+header: :engine mysql
#+header: :dbuser root
#+header: :dbpassword root
#+header: :database fiction
#+begin_src sql
SELECT DISTINCT Language FROM main;
#+end_src

#+RESULTS:
| Tables_in_fiction |
|-------------------|
| hashes            |
| main              |
| main_edited       |

#+NAME: db-connection
#+begin_src python :noweb yes :session python :exports code
MY_DB = mysql.connector.connect(
    host="localhost",
    user="root",
    passwd="root",
    charset='utf8'
)
#+end_src

#+Name: database-insertion
#+begin_src python :noweb yes :session python :exports code
def insert_book_db(book, word_curve_fit, zh_character_curve_fit, db="library"):
    '''
    Insert data into db
    '''
    mycursor = MY_DB.cursor()
    mycursor.execute("use " + db + ";")
    sql = """INSERT IGNORE corpus (title,
    author,
    slope,
    intercept,
    std_error_slope,
    std_error_intercept,
    word_count,
    unique_words,
    zhslope,
    zhintercept,
    zhstd_error_slope,
    zhstd_error_intercept,
    character_count,
    unique_characters,
    language,
    epub_type,
    subject,
    source,
    rights,
    relation,
    publisher,
    identifier,
    epub_format,
    description,
    contributor,
    date
    ) VALUES (%s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s,
    %s)"""
    val = (book.title,
           book.author,
           float(word_curve_fit['slope']),
           float(word_curve_fit['intercept']),
           float(word_curve_fit['std_error_slope']),
           float(word_curve_fit['std_error_intercept']),
           float(book.word_count),
           float(book.unique_words),
           float(zh_character_curve_fit['slope']),
           float(zh_character_curve_fit['intercept']),
           float(zh_character_curve_fit['std_error_slope']),
           float(zh_character_curve_fit['std_error_intercept']),
           float(book.character_count),
           float(book.unique_characters),
           book.language,
           book.epub_type,
           book.subject,
           book.source,
           book.rights,
           book.relation,
           book.publisher,
           book.identifier,
           book.epub_format,
           book.description,
           book.contributor,
           book.date)
    mycursor.execute(sql, val)
    MY_DB.commit()
    print("1 record inserted, ID:", mycursor.lastrowid)
#+end_src

#+RESULTS:
#+Name: database-creation
#+begin_src python :noweb yes :session python :exports code
def create_database(db="library"):
    '''
    Create database if it doesn't exists yet.
    '''
    mycursor = MY_DB.cursor()
    mycursor.execute("CREATE DATABASE IF NOT EXISTS " + db + ";")
    mycursor.execute(
        "ALTER DATABASE " + db + " CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;")
    mycursor.execute("USE " + db + ";")
    mycursor.execute(
        """ CREATE TABLE IF NOT EXISTS corpus (id INT AUTO_INCREMENT PRIMARY KEY,
        title VARCHAR(255),
        author VARCHAR(255),
        slope DECIMAL(10,5),
        intercept DECIMAL(10,5),
        std_error_slope DECIMAL(10,5),
        std_error_intercept DECIMAL(10,5),
        word_count DECIMAL(20,1),
        unique_words DECIMAL(20,1),
        zhslope DECIMAL(10,5),
        zhintercept DECIMAL(10,5),
        zhstd_error_slope DECIMAL(10,5),
        zhstd_error_intercept DECIMAL(10,5),
        character_count DECIMAL(15,1),
        unique_characters DECIMAL(15,1),
        language VARCHAR(255),
        epub_type VARCHAR(255),
        subject VARCHAR(255),
        source VARCHAR(255),
        rights VARCHAR(255),
        relation VARCHAR(255),
        publisher VARCHAR(255),
        identifier VARCHAR(255),
        epub_format VARCHAR(255),
        description VARCHAR(510),
        contributor VARCHAR(255),
        date VARCHAR(255)) """)
    mycursor.execute(
        "ALTER TABLE corpus CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;")
    try:
        mycursor.execute(
            "ALTER TABLE corpus ADD CONSTRAINT unique_book UNIQUE (title,author);")
    except Exception as ex:
        print ex
#+end_src

#+NAME: is-book-in-db
#+begin_src python :noweb yes :session python :exports code :results output
def is_book_in_db(title, author):
    '''
    Check if book is in database.
    '''
    mycursor = MY_DB.cursor()
    mycursor.execute("CREATE DATABASE IF NOT EXISTS library;")
    mycursor.execute("USE library;")
    query = ('SELECT * from corpus where title="' + str(title)
             + '" and author="' + str(author) + '"')
    mycursor.execute(query)
    mycursor.fetchall()
    if mycursor.rowcount == 1:
        print ("Book " + str(title)
               + ", by " + str(author)
               + " already in database. Next.")
        return True
    return False
#+end_src

#+RESULTS: does-book-exist-db
: ELECT * from corpus where title="opus" and author="paco"
: 1
: Book opus, by paco already in database. Next.
#+NAME: db-backup
#+begin_src python :noweb yes :session python :exports code
def runbackup(hostname,
              mysql_user,
              mysql_password,
              db_loc="test/db/library_test.db"):
    '''
    Write sql file.
    '''
    try:
        backup = subprocess.Popen("mysqldump -h"
                                  + hostname + " -u"
                                  + mysql_user + " -p'"
                                  + mysql_password + "' --databases library > "
                                  + db_loc, shell=True)
        # Wait for completion
        backup.communicate()
        if backup.returncode != 0:
            sys.exit(1)
        else:
            print("Backup done for", hostname)
    except Exception as ex:
        # Check for errors
        print ex
        print("Backup failed for", hostname)
#+end_src
* Fitting points to function
  The purpose of this section is to fit all the different points to a function
  | Minimum length (characters) |         R^2 |
  |-----------------------------+-------------|
  |                           0 | 0.743868489 |
  |                       20000 |        0.71 |
  |                             |             |
  #+BEGIN_SRC python
for i in xrange(0,lexicalVariety,1000):
  print(i)
  #+END_SRC

  #+RESULTS:

#+BEGIN_SRC gnuplot :exports both :file sweep.png
set multiplot
set encoding utf8
set title "Lexical variety Vs Length"
set xlabel "Length in characters"
set ylabel "Unique characters"
set logscale x
set nologscale y
plot '/home/rcl/readability-metric/test/0936.tsv' title 'Jipin Jiading' linecolor 1, \
     '/home/rcl/readability-metric/test/1077-4000.tsv' title 'Cixi Quanzhuan' linecolor 2
     #'/home/rcl/readability-metric/zh-TW.tsv' title 'Chinese' linecolor 3
unset multiplot
#+END_SRC

#+RESULTS:
[[file:sweep.png]]


#+BEGIN_SRC gnuplot :exports both :file test.png
set multiplot
set encoding utf8
set title "Lexical variety Vs Length"
set xlabel "Length in characters"
set ylabel "Unique characters"
set logscale x
set nologscale y
plot '/home/rcl/readability-metric/zh-TW.tsv' title 'Jipin Jiading' linecolor 1, \
     #'/home/rcl/readability-metric/zh-TW.tsv' title 'Cixi Quanzhuan' linecolor 2
     #'/home/rcl/readability-metric/zh-TW.tsv' title 'Chinese' linecolor 3
unset multiplot
#+END_SRC

#+RESULTS:
[[file:test.png]]

#+BEGIN_SRC R :file R.png :results output graphics
dat <- read.csv("~/readability-metric/zh-TW.tsv", header=FALSE, sep="\t")
x = dat[, 1]
y = dat[, 2]

Estimate = lm(y ~ x)
logEstimate = lm(y ~ log(x))

plot(x,predict(Estimate),type='l',col='blue')
lines(x,predict(logEstimate),col='red')
plot(x, y, log ="x",
        type="p",
        pch = 1,
        xlab="Length (characters)",
        ylab="Unique characters (characters)")
#+END_SRC

#+RESULTS:
[[file:R.png]]

#+begin_src R :file 3.png :results output graphics
library(lattice)
xyplot(1:10 ~ 1:10)
#+end_src

#+RESULTS:
[[file:3.png]]
* Plotting

#+RESULTS:

Perfect. It plots the first two columns and doesn't give an error about all the rest.
#+BEGIN_SRC gnuplot
reset
set title "Lexical variety Vs Length"
set xlabel "Length in words"
set ylabel "Unique words"
set logscale x
set logscale y
es_filelist=system("ls es*.tsv")
fr_filelist=system("ls fr*.tsv")
pt_filelist=system("ls p*.tsv")
plot  for [filename in es_filelist] filename title 'Spanish' linecolor 1, \
      for [filename in fr_filelist] filename title 'French' linecolor 2, \
      for [filename in pt_filelist] filename title 'Portuguese' linecolor 3, \
      'ar.tsv' title 'Arabic' linecolor 4, \
      'zh-TW.tsv' title 'Chinese' linecolor 5
#+END_SRC

#+RESULTS:

#+BEGIN_SRC gnuplot
reset
set title "Lexical variety Vs Length"
set xlabel "Length in characters"
set ylabel "Unique characters"
set logscale x
set nologscale y
plot 'zh-TW.tsv' title 'Chinese' linecolor 1
#+END_SRC

#+RESULTS:

#+BEGIN_SRC gnuplot
reset
set title "Lexical variety Vs Length"
set xlabel "Length in characters"
set ylabel "Unique characters"
set logscale x
set logscale y
plot 'ar.tsv' title 'Arabic' linecolor 1
#+END_SRC

#+BEGIN_SRC gnuplot
reset
set multiplot
set title "Lexical variety Vs Length"
set xlabel "Length in words"
set ylabel "Unique words"
#set logscale x
#set logscale y
set logscale x
set logscale y
filelist=system("ls *.tsv")
#plot  for [filename in filelist] filename title filename
plot 'spanish.tsv' title 'Spanish' linecolor 1, \
     'french.tsv' title 'French' linecolor 2, \
     'portuguese.tsv' title 'Portuguese' linecolor 3, \
     'ar.tsv' title 'Arabic' linecolor 4, \
     for [filename in filelist] filename title filename linecolor 5
unset multiplot
#+END_SRC

#+RESULTS:
[[file:all.png]]
* Tagging
  The purpose of this section is to tag the lists containing the analysis with the canon to which they belong, if appropriate.
  #+begin_src bash :tangle canon-tagging.sh :exports code
canon="/home/rcl/readability-metric/canon/chinese.txt"
analized="/home/rcl/readability-metric/tagging/zh-TW.tsv"
list=""
while read -r author_canon title_canon; do
        list+=$author_canon
        list+=" "
done < "$canon"
unique_authors=$(tr ' ' '\n' <<< $list | sort -u)
echo $unique_authors
while read -r filesize lexicalVariety intercept slope language author_list title_list type subject source rights relation publisher identifier format contibutor date; do
    flag=0
    while read -r author_canon title_canon; do
        if [ "$author_list" == "$author_canon" ] && [ "$title_list" == "$title_canon" ]; then
            #printf '%s %s Canon match!!\n' "$author_list" "$title_list"
            flag=1
        fi
    done < "$canon"
    for word in $unique_authors; do
        if [ "$author_list" == "$word" ] && [ "$flag" != 1 ]; then
            #printf '%s %s Extended canon match!!\n' "$author_list" "$title_list"
        fi
    done
done < "$analized"
  #+end_src

  #+begin_src bash
linewriting="/home/rcl/readability-metric/linewriting.txt"
touch $linewriting
echo "roland coeurjoly" > $linewriting
echo "chun zhang" >> $linewriting

while read line; do
    if [[ $line = *"chun zhang"* ]]; then
        #echo "substring found!"
        echo
    fi
done < "$linewriting"
less $linewriting
  #+end_src
  #+begin_src python :results output
# -*- coding: utf-8 -*-
import numpy
import csv
canon_file="/home/rcl/readability-metric/canon/chinese.txt"
analysis_file="/home/rcl/readability-metric/tagging/zh-TW.tsv"
canon = numpy.array(list(csv.reader(open(canon_file, "rb"), delimiter=" "))).astype("object")
analysis = numpy.array(list(csv.reader(open(analysis_file, "rb"), delimiter="\t"))).astype("object")
print canon[90][0]
print analysis[90][5]
  #+end_src
  #+RESULTS:
  : 古龍
  : 東野圭吾
#+begin_src python :results output
import json

with open("benchmarks.json", "r") as test_cases:
    benchmarks = json.load(test_cases)
    for benchmark in benchmarks['books']:
        print benchmark['path'].encode('utf-8')
        print benchmark['author'].encode('utf-8')
        print benchmark['title'].encode('utf-8')
        print benchmark['epub_type'].encode('utf-8')
        print benchmark['word_curve_fit_slope']
        print benchmark['zh_character_curve_fit_slope']
        print benchmark['word_count']
        print benchmark['unique_words']
#+end_src


#+RESULTS:
#+begin_example
test/pg23306.epub
René Descartes
Meditationes de prima philosophia

0.803463675366
0
28207
6085
test/pg21000.epub
Johann Wolfgang von Goethe
Faust: Eine Tragödie

0.831561333002
0
36751
9293
test/pg24264.epub
Xueqin Cao
紅樓夢

0.69794400829
373.751162525
662992
21113
test/pg6099.epub
Charles Baudelaire
Les Fleurs du Mal

0.834087803731
0
31525
8177
test/pg2000.epub
Miguel de Cervantes Saavedra
Don Quijote

0.740139477978
0
449755
27284
test/pg521.epub
Daniel Defoe
The Life and Adventures of Robinson Crusoe

0.708038727522
0
141776
7643
test/Las conversaciones privadas de Hitler - Adolf Hitler.epub
Adolf Hitler
Las conversaciones privadas de Hitler

0.774981251067
0
308320
28381
#+end_example
#+begin_src emacs-lisp
(require 'virtualenvwrapper)
(setq venv-location "/home/rcl/readability-metric/env/")
#+end_src

#+RESULTS:
: /home/rcl/readability-metric/env/

#+RESULTS:
|                 |
|                 |
| /usr/bin/python |
#+begin_src python :results output :session python
import sys
print('\n'.join(sys.path))
print(sys.executable)
#+end_src

#+RESULTS:
#+begin_example
/home/rcl/readability-metric/lib/python2.7
/home/rcl/readability-metric/lib/python2.7/plat-x86_64-linux-gnu
/home/rcl/readability-metric/lib/python2.7/lib-tk
/home/rcl/readability-metric/lib/python2.7/lib-old
/home/rcl/readability-metric/lib/python2.7/lib-dynload
/usr/lib/python2.7
/usr/lib/python2.7/plat-x86_64-linux-gnu
/usr/lib/python2.7/lib-tk
/home/rcl/readability-metric/local/lib/python2.7/site-packages
/home/rcl/readability-metric/lib/python2.7/site-packages
/home/rcl/readability-metric/bin/python
#+end_example
* Profiling
