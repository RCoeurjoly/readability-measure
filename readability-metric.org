#+TITLE: Readability metric based on analysis of lexical variety
#+AUTHOR: Roland Coeurjoly
#+EMAIL: rolandcoeurjoly@gmail.com
#+Date:
#+OPTIONS: ^:nil toc:nil H:4
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usepackage{attrib}
#+LATEX_HEADER: \Plainauthor{Roland Coeurjoly}
#+LATEX_HEADER: \author{Roland Coeurjoly}
#+LATEX_HEADER: \title{Readability metric based on analysis of lexical variety}
#+LATEX_HEADER: \Shorttitle{Pending}
#+LATEX_HEADER: \Keywords{readability metric, readability test, readability, formula, comprehensible input, extensive reading, vocabulary, literate programming, reproducible research, emacs}
#+LATEX_HEADER: \Address{Pending}
#+LATEX_HEADER: \Abstract{We present a readability metric, capable of being applied to books written in multiple languages and easy to compute, therefore lending itself to be applied to large corpora composed of thousands of books. It uses length of text (metricd in words) versus unique words to compute the rate at which the author introduces new vocabulary in a certain book. This rate can then be used to rank the book with respect to others. This readability metric is only suitable to texts of at least 10.000 (ten thousand) words. It is therefore used primarely for the analysis of }
#+STARTUP: oddeven
* Summary
  The main idea of this paper is that the rate at which an author introduces new words into a book can function as a readability metric.
  This rate takes into account the number of unique words and the length in words of a book.
  So, in a simplified case, between two books with the same length, the one with fewer unique words is easiest to read, all other factors being equal.
  This readability metric is valuable to people who, having a large corpus of books, want to find those that are easiest to read in comparison with the rest of the corpus.
  In other words, helps you find comprehensible input.
  In this project we are going to develop a readability metric based on corpus analysis.
** What is a readability metric?
   A readability metric is a set of equations used to quantify how easy a given text is to read.
   In this project, we focus on long texts, with more that 2000 words or tokens. This is in contrast with other readability metrics which analyze sentences.
* Literature review
  Alexander Arguelles proposes the following:
* Approach
  We are going to approach the problem from a corpus linguistics perspective. This means that we are going to analyze a lot of books and take some general conclusions about them.
  The basic premise used in this project is that vocabulary and readability are correlated negatively, i.e., the larger the vocabulary, the more difficult to understand.

  We will use a literate programming approach using GNU/Emacs org mode to make our results as reproducible as possible.
* Corpora
** French
  #+BEGIN_SRC shell :exports code :tangle scripts/french-corpus.sh
for i in {1..11000}
do
    wget -O $1$i.epub bibliothequenumerique.tv5monde.com/download/epub/$i
    find ${1} -name "*.epub" -size -88k -delete
done
  #+END_SRC

  #+RESULTS:
** Chinese
   The following is the perfect script for downloading Chinese books from haodoo (好讀).
It removes those in vertical format with -R "V*.epub".
  #+BEGIN_SRC shell :exports code :tangle scripts/chinese-corpus.sh
wget -np -P $1 -r -A .epub -R "V*.epub" http://haodoo.net/PDB/
  #+END_SRC

#+BEGIN_SRC shell :exports code
find ~/readability-metric/corpus/Chinese/haodoo.net/PDB/ -mindepth 1 -type f -name "*.epub" -exec printf x \; | wc -c
#+END_SRC

#+RESULTS:
: 3699

#+BEGIN_SRC shell :exports code
ls -ltu ~/readability-metric/corpus/Chinese/haodoo.net/PDB/A/*435.epub
#+END_SRC

#+RESULTS:
: -rw-rw-r-- 1 rcl rcl 130460 jul 19 16:04 /home/rcl/readability-metric/corpus/Chinese/haodoo.net/PDB/A/435.epub
** Arabic
   [[http://www.hindawi.
org/][http://www.hindawi.org/]]
   #+BEGIN_SRC shell :exports code :tangle scripts/arabic-corpus.sh
wget -np -P $1 -r -A .epub http://www.hindawi.org/books/
   #+END_SRC
** Russian
   #+BEGIN_SRC shell :exports code
wget -np -P ~/readability-metric/corpus/Russian/ -r -A .epub https://www.rulit.me/
   #+END_SRC
** All
   The Gutenberg project is also a good source of books.
   We could create different files for each language with length and lexical variety.
   Also, a convenient feature of the Gutenberg library is that it has ebooks with images and without.
   We download without images to save bandwidth.
   #+BEGIN_SRC shell :exports code
wget -w 2 -m -H "http://www.gutenberg.org/robot/harvest?filetypes[]=epub.noimages"
   #+END_SRC
* Python module
** Usage
** Main
 #+NAME: main
 #+BEGIN_SRC python :noweb yes :session python :exports code
def correct_dirpath(dirpath):
    if dirpath.endswith('/'):
        return dirpath
    return dirpath + '/'


def get_size(filepath, unit='M'):
    if unit == 'K':
        return os.path.getsize(filepath) >> 10
    if unit == 'M':
        return os.path.getsize(filepath) >> 20
    if unit == 'G':
        return os.path.getsize(filepath) >> 30


def analyse_file(ebookpath, my_col):
    """
    Analyse single book
    """
    if ebookpath.endswith(".epub"):
        try:
            ebook = ebookpath
            print("Checking if book " + ebook + " is in database")
            my_book = Book(ebookpath)
            if is_book_in_mongodb(my_book, my_col):
                return False
            if get_size(ebookpath) < 10:
                print("Reading ebook " + ebook)
                my_book = Book(ebookpath, samples=10)
            else:
                print("Book " + ebook + " too big. Only metadata is read")
            print("Writing to database")
            my_col.insert_one(my_book.__dict__, my_col)
            return True
        except (KeyError,
                TypeError,
                lxml.etree.XMLSyntaxError,
                ebooklib.epub.EpubException) as ex:
            print(ex)
            return False
    print("Only epubs can be analysed")
    return False


def analyse_directory(corpus_path, db, json_export):
    '''
    Main function: open and read all epub files in directory.
    Analyze them and populate data in database
    :param argv: command line args.
    '''
    books_analyzed = 1
    my_client, __, my_col = mongo_connection(db)
    for dirpath, __, files in os.walk(corpus_path):
        for ebook in files:
            result = analyse_file(correct_dirpath(dirpath) + ebook, my_col)
            if result:
                print("Books analysed: " + str(books_analyzed + 1))
                books_analyzed += 1
            if books_analyzed % 25 == 0:
                print("Performing export")
                export_mongo(db, json_export)
    print("Performing final export")
    export_mongo(db, json_export)
    print("Closing db")
    my_client.close()


def main(argv):
    parser = argparse.ArgumentParser()
    group = parser.add_mutually_exclusive_group()
    group.add_argument("-c", "--corpus-dir", type=str, help="the corpus directory")
    group.add_argument("-b", "--book", type=str, help="the book to be analyzed")
    parser.add_argument("-j", "--json", type=str, help="the exported json")
    parser.add_argument("-d", "--database", type=str, help="the database")
    args = parser.parse_args()
    if args.book is not None:
        __, __, my_col = mongo_connection(args.database)
        return analyse_file(args.book, my_col)
    elif args.corpus_dir is not None:
        analyse_directory(args.corpus_dir, args.database, args.json)

if __name__ == '__main__':
    main(sys.argv)
 #+END_SRC

 #+RESULTS: main

 #+RESULTS: epub-handling
** Lexical sweep and curve fitting
   We can only do the curve fitting with books longer than 10000 tokens. This is because, to begin with, books don't exhibit logarithmic behavior until they reach around 4 thousand words.
   Moreover, we need to have enough samples to be able to feed the piece of software that does the curve fitting.
#+NAME: lexical-sweep
#+BEGIN_SRC python :noweb yes :session python :exports code
def lexical_sweep_map(start, stop, step, text):
    return list(map(lambda x: [x, len(set(text[0:x]))], range(int(start),
                                                         int(stop),
                                                         int(step))))


def lexical_sweep_list_comprehension(start, stop, step, text):
    return [[x, len(set(text[0:x]))] for x in range(int(start),
                                                    int(stop),
                                                    int(step))]


def lexical_sweep_for_loop(start, stop, step, text):
    return list(map(lambda x: [x, len(set(text[0:x]))], range(int(start),
                                                         int(stop),
                                                         int(step))))


def lexical_sweep(text, slicing_function=lexical_sweep_map, samples=10):
    '''
    Lexical sweep.
    '''
    try:
        log_behaviour_start = 5000
        sweep_values = []
        log_behaviour_range = len(text) - log_behaviour_start
        log_step = log_behaviour_range/(samples - 1)
        if len(text) > 10000 and samples >= 2:
            sweep_values = slicing_function(log_behaviour_start,
                                            len(text) + 1,
                                            log_step,
                                            text)
            return sweep_values
        return False
    except AttributeError as ex:
        print(ex)
        return False
     #+END_SRC

#+NAME: curve-fit
#+BEGIN_SRC python :noweb yes :session python :exports code
def extract_fit_parameters(self, analysis_type, sweep_values):
    '''
    Curve fit.
    '''
    log_x = True
    function = linear_func
    if analysis_type == "words":
        log_y = True
    elif analysis_type == "characters":
        log_y = False
    if sweep_values:
        array = list(zip(*sweep_values))
        if log_x:
            xarr = log(array[0])
        else:
            xarr = array[0]
        if log_y:
            yarr = log(array[1])
        else:
            yarr = array[1]
        initial_a = 0
        initial_b = 0
        popt, pcov = curve_fit(function, xarr, yarr, (initial_a, initial_b))
        slope = popt[0]
        intercept = popt[1]
        perr = np.sqrt(np.diag(pcov))
        std_error_slope = perr[0]
        std_error_intercept = perr[1]
        fit = {'samples': len(sweep_values),
               'intercept': intercept,
               'slope': slope,
               'std_error_intercept': std_error_intercept,
               'std_error_slope': std_error_slope}
        setattr(self,
                analysis_type + "_fit",
                fit)
#+END_SRC

#+RESULTS: lexical-sweep

Empirically, We have found
#+NAME: fit-functions
#+begin_src python :noweb yes :session python :exports code
def linear_func(variable, slope, y_intercept):
    '''
    Linear model.
    '''
    return slope*variable + y_intercept


def log_func(variable, coefficient, x_intercept):
    '''
    Logarithmic model.
    '''
    return coefficient*log(variable) + x_intercept


def log_log_func(variable, coefficient, intercept):
    '''
    Log-log model.
    '''
    return math.e**(coefficient*log(variable) + intercept)
#+end_src

** Book class

   We then proceed to open the epub and extract all metadata.
   As stated in the [[https://ebooklib.readthedocs.io/en/latest/tutorial.html#reading-epub][package documentation]], only creator, title and language are required metadata fields.
   The rest is optional, so we catch them with care.

   We then use BeautifulSoup to remove all html marks.
*** Book class architecture
#+NAME: book-class
#+BEGIN_SRC python :noweb yes :session python :exports code
class Book(object):
    '''
    Book class
    '''
    <<constructor>>

    <<metadata>>

    <<tokenization>>

    <<freq-dist>>

    <<text-extraction>>

    <<language-detection>>

    <<curve-fit>>

    <<delete>>
    #+end_src

*** Constructor
    We don't extract all text in constructor because it is expensive and we want to check first if it exists in database.
#+NAME: constructor
#+BEGIN_SRC python :noweb yes :session python :exports code
def __init__(self, epub_filename, slicing_function=lexical_sweep_map, samples=0):
    '''
    Init.
    '''
    # pylint: disable=too-many-statements
    # There is a lot of metadata but it is repetitive and non problematic.
    try:
        print("Extracting metadata")
        self.extract_metadata(epub_filename)
        if samples:
            print("Extracting text")
            self.extract_text()
            print("Detecting language")
            self.detect_language()
            print("Tokenization")
            self.tokenize()
            print("Calculating word sweep values")
            sweep_values = lexical_sweep(self.tokens, slicing_function, samples)
            print("Word fit")
            self.extract_fit_parameters("words", sweep_values)
            if self.language == "zh" or self.language == "zh_Hant":
                print("Calculating character sweep values")
                sweep_values = lexical_sweep(self.zh_characters, slicing_function, samples)
                print("Character fit")
                self.extract_fit_parameters("characters", sweep_values)
            self.delete_heavy_attributes()
    except AttributeError:
        pass
#+END_SRC
*** Extracting metadata
#+NAME: metadata
#+BEGIN_SRC python :noweb yes :session python :exports code
def extract_metadata(self, epub_filename):
    '''
    Extraction of metadata
    '''
    self.filepath = epub_filename
    epub_file = epub.read_epub(self.filepath)
    metadata_fields = ['creator',
                       'title',
                       'subject',
                       'source',
                       'rights',
                       'relation',
                       'publisher',
                       'identifier',
                       'description',
                       'coverage',
                       'contributor',
                       'date']
    for metadata_field in metadata_fields:
        try:
            setattr(self,
                    metadata_field,
                    epub_file.get_metadata('DC', metadata_field)[0][0])
        except (IndexError, AttributeError):
            pass
    metadata_to_attribute = [['original_language', 'language'],
                             ['epub_type', 'type'],
                             ['epub_format', 'format']]
    for attribute, metadata_field in metadata_to_attribute:
        try:
            setattr(self,
                    attribute,
                    epub_file.get_metadata('DC', metadata_field)[0][0])
        except (IndexError, AttributeError):
            pass
#+end_src

*** Extracting text from ebook
#+NAME: text-extraction
#+BEGIN_SRC python :noweb yes :session python :exports code
def extract_text(self):
    '''
    Extract all text from the book.
    '''
    book = epub.read_epub(self.filepath)
    cleantext = ""
    html_filtered = ""
    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            raw_html = item.get_content()
            <<html-filtering>>
    cleantext = clean_non_printable(html_filtered)
    self.text = cleantext
#+END_SRC

#+RESULTS: text-extraction
**** Cleaning the html
#+NAME: html-filtering
#+BEGIN_SRC python :noweb yes :session python :exports code
html_filtered += BeautifulSoup(raw_html, "lxml").text
#+END_SRC

#+RESULTS: html-cleaning
**** Removing invalid utf-8

#+NAME: printable-set
#+BEGIN_SRC python :noweb yes :session python :exports code
PRINTABLE = {
    # 'Cc',
    'Cf',
    'Cn',
    'Co',
    'Cs',
    'LC',
    'Ll',
    'Lm',
    'Lo',
    'Lt',
    'Lu',
    'Mc',
    'Me',
    'Mn',
    'Nd',
    'Nl',
    'No',
    'Pc',
    'Pd',
    'Pe',
    'Pf',
    'Pi',
    'Po',
    'Ps',
    'Sc',
    'Sk',
    'Sm',
    'So',
    'Zl',
    'Zp',
    'Zs'}
     #+end_src

#+NAME: utf8-cleaning
#+BEGIN_SRC python :noweb yes :session python :exports code
def clean_non_printable(text):
    '''
    Remove all non printable characters from string.
    '''
    return ''.join(character for character in text
                   if unicodedata.category(character) in PRINTABLE)
#+END_SRC
MongoDB doesnt like storing dots
#+NAME: dot-cleaning
#+BEGIN_SRC python :noweb yes :session python :exports code
def clean_dots(dictionary):
    '''
    Remove dot form dictionary.
    '''
    del dictionary['.']
#+END_SRC

**** Language detection
#+NAME: language-detection
#+begin_src python :noweb yes :session python :exports code
def detect_language(self):
    '''
    We don't trust the epub metadata regarding language tags
    so we do our own language detection
    '''
    if not hasattr(self, 'text'):
        self.extract_text()
    self.language = Text(self.text).language.code
#+end_src

*** Tokenization
    If the language is Chinese, appart from doing the tokenization, we also metric individual characters.
#+NAME: tokenization
#+BEGIN_SRC python :noweb yes :session python :exports code
def tokenize(self):
    '''
    Tokenization.
    '''
    try:
        if self.language == 'zh' or self.language == 'zh_Hant':
            self.zh_characters = ''.join(character for character in self.text
                                         if u'\u4e00' <= character <= u'\u9fff')
            self.character_count = len(self.zh_characters)
            self.unique_characters = len(set(self.zh_characters))
        self.tokens = Text(self.text).words
        self.word_count = len(self.tokens)
        self.unique_words = len(set(self.tokens))
    except ValueError as ex:
        print(ex)
        self.tokens = []
#+END_SRC
*** Frequency distributions
#+NAME: freq-dist
#+BEGIN_SRC python :noweb yes :session python :exports code
def get_freq_dist(self):
    '''
    Frequency distribution for both .
    '''
    if not self.tokens:
        self.tokenize()
    if self.language == 'zh' or self.language == 'zh_Hant':
        self.zh_char_freq_dist = dict(FreqDist(self.zh_characters))
        try:
            del self.zh_char_freq_dist['.']
        except KeyError as ex:
            print(ex)
    self.freq_dist = dict(FreqDist(self.tokens))
    try:
        del self.freq_dist['.']
    except KeyError as ex:
        print(ex)
#+END_SRC
*** Delete
#+NAME: delete
#+BEGIN_SRC python :noweb yes :session python :exports code
def delete_heavy_attributes(self):
    '''
    Delete heavy attributes.
    '''
    del self.text
    del self.tokens
    try:
        del self.zh_characters
    except AttributeError:
        pass
#+END_SRC
** Learnable words
** Imports
   We import some packages to make our life easier:
   - ebooklib: to process epubs
   - BeautifulSoup: to process the html in epubs
   - langdetect to detect language. We use this because based on experience epub language tags are not very reliable
   - ntlk: to do natural language processing
#+NAME: imports
#+BEGIN_SRC python :session python :results none :exports code
import unicodedata
import sys
import os
import math
import subprocess
import re
import lxml
import ebooklib
import pymongo
import argparse
from ebooklib import epub
from bs4 import BeautifulSoup
from scipy.optimize import curve_fit
import numpy as np
from numpy.lib.scimath import log as log
from polyglot.text import Text
from nltk import FreqDist
#+END_SRC

** Architecture
   In a first instance, we want to extract the following information from each ebook:
  - Author
  - Title
  - Length in number of words
  - Number of unique words
  It would be nice to create a file for each language (according to metadata).
  The logic would be the following:
  Try adding the results to a file suffixed with the language code.
  If that throws an exception, create the file and add the results
#+BEGIN_SRC python :noweb yes :tangle corpus_analysis.py :exports code
# -*- coding: utf-8 -*-
'''
corpus-analysis.py: readability metric for epub ebooks.
Version 1.0
Copyright (C) 2019  Roland Coeurjoly <rolandcoeurjoly@gmail.com>
'''
# Imports
<<imports>>
# Constants
<<printable-set>>
# Curve fitting functions


<<lexical-sweep>>


<<fit-functions>>
# Classes
# Book Class


<<book-class>>
# Functions


<<utf8-cleaning>>


<<dot-cleaning>>
# Database functions
# MongoDB


<<mongodb_connection>>


<<insert_book_mongo>>


<<check_book_mongo>>


<<backup_mongo>>


<<export_mongo>>
# Main function


<<main>>
#+END_SRC

#+RESULTS:
: None

** Vocabulary coverage
   #+NAME: vocabulary_coverage
   #+begin_src python :noweb yes :exports code :session sahj :tangle vocabulary_coverage.py :results output
'''
Random
'''
from nltk import FreqDist
import corpus_analysis


MY_BOOK = corpus_analysis.Book("./test/pinocchio.epub")
MY_BOOK.tokenize()
MY_FREQDIST = FreqDist(MY_BOOK.tokens)
print(MY_BOOK.word_count)
percentage = 0
cumulative_word_count = 0
coverage = 1
print(MY_FREQDIST.most_common(coverage)[coverage - 1][1])
margin_unknowable_list = MY_FREQDIST.most_common(MY_BOOK.word_count - 1) - MY_FREQDIST.most_common(int(round((MY_BOOK.word_count - 1)*0.98)))
last_word_frequency = MY_FREQDIST.most_common(coverage)[coverage - 1][1]
coverage += 1
cumulative_word_count += last_word_frequency
percentage = (cumulative_word_count*100/MY_BOOK.word_count)
print(margin_unknowable_list)
   #+end_src

   #+RESULTS: vocabulary_coverage
   : 52544
   : 3345
   : Traceback (most recent call last):
   :   File "<stdin>", line 1, in <module>
   :   File "/tmp/babel-2FdH2m/python-VLLu9V", line 16, in <module>
   :     margin_unknowable_list = MY_FREQDIST.most_common(MY_BOOK.word_count - 1) - MY_FREQDIST.most_common(int(round((MY_BOOK.word_count - 1)*0.98)))
   : TypeError: unsupported operand type(s) for -: 'list' and 'list'

* Testing
  To execute a particular test:
  #+begin_src shell :shebang #!/bin/bash -i :results output
python test_corpus_analysis.py MyTest.test_metadata
python test_corpus_analysis.py MyTest.test_language
python test_corpus_analysis.py MyTest.test_tokens
python test_corpus_analysis.py MyTest.test_fit
python test_corpus_analysis.py MyTest.test_db_writing
  #+end_src

** Unit tests
#+BEGIN_SRC python :exports code :noweb yes :tangle test_corpus_analysis.py
# -*- coding: utf-8 -*-
'''
Unit testing for the corpus analysis
'''
import pymongo
import unittest
import json
from ebooklib import epub
from corpus_analysis import Book, lexical_sweep, linear_func, analyse_directory

class MyTest(unittest.TestCase):
    '''
    Class
    '''
    maxDiff = None

    def test_metadata(self):
        '''
        Given a certain book, test metadata
        '''
        metadata = ['epub_type',
                    'subject',
                    'source',
                    'rights',
                    'relation',
                    'publisher',
                    'identifier',
                    'epub_format',
                    'description',
                    'coverage',
                    'contributor',
                    'date']

        with open("benchmarks.json", "r") as test_cases:
            benchmarks = json.load(test_cases)
            for benchmark in benchmarks['books']:
                my_book = Book(benchmark['filepath'])
                self.assertEqual(my_book.creator, benchmark['creator'])
                self.assertEqual(my_book.title, benchmark['title'])
                for key in benchmark.keys():
                    if key in metadata:
                        attr = getattr(my_book, key)
                        self.assertEqual(attr, benchmark[key])
                print("Metadata for " + benchmark['title'] + " OK")

    def test_language(self):
        '''
        Given a certain book, test language
        '''
        with open("benchmarks.json", "r") as test_cases:
            benchmarks = json.load(test_cases)
            for benchmark in benchmarks['books']:
                my_book = Book(benchmark['filepath'])
                my_book.detect_language()
                self.assertEqual(my_book.language, benchmark['language'])
                print("Language for " + benchmark['title'] + " OK")

    def test_tokens(self):
        '''
        Given a certain book, test tokens
        '''
        tokens = ['word_count',
                  'unique_words',
                  'character_count',
                  'unique_characters']

        with open("benchmarks.json", "r") as test_cases:
            benchmarks = json.load(test_cases)
            for benchmark in benchmarks['books']:
                my_book = Book(benchmark['filepath'])
                my_book.detect_language()
                my_book.tokenize()
                for key in benchmark.keys():
                    if key in tokens:
                        attr = getattr(my_book, key)
                        self.assertEqual(attr, benchmark[key])
                print("Tokens for " + benchmark['title'] + " OK")

    def test_fit(self):
        '''
        Given a certain book, test fit
        '''
        with open("benchmarks.json", "r") as test_cases:
            benchmarks = json.load(test_cases)
            for benchmark in benchmarks['books']:
                my_book = Book(benchmark['filepath'], samples=10)
                self.assertEqual(my_book.words_fit, benchmark['words_fit'])
                print("Fit for " + benchmark['title'] + " OK")

    def test_db_writing(self):
        '''
        Write all books to database
        '''
        my_args = ["assets/", "library_test", "dump/my_json.json"]
        # Drop database
        myclient = pymongo.MongoClient("mongodb://localhost:27017/")
        mydb = myclient["library_test"]
        mycol = mydb["corpus"]
        mycol.drop()
        analyse_directory(my_args[0], my_args[1], my_args[2])
        with open("benchmarks.json", "r") as test_cases:
            benchmarks = json.load(test_cases)
            for benchmark in benchmarks['books']:
                for result in mycol.find({}, {"_id":False}):
                    if benchmark['title'] == result['title']:
                        self.assertEqual(result, benchmark)
                        print("Database write for " + benchmark['title'] + " OK")

if __name__ == '__main__':
    unittest.main(failfast=True)
  #+end_src

  #+RESULTS:
** Creating benchmark

#+BEGIN_SRC python :noweb yes :tangle create_benchmark.py :exports code
'''
Create benchmark based on epubs
'''

import json
import os
import corpus_analysis
from corpus_analysis import correct_dirpath
DATA = {}
DATA['books'] = []

with open('benchmarks.json', 'w') as outfile:
    for dirpath, __, files in os.walk('assets/'):
        for ebook in files:
            ebook_path = correct_dirpath(dirpath) + ebook
            print("Reading book")
            my_book = corpus_analysis.Book(ebook_path, samples=10)
            print("Book read")
            DATA['books'].append(my_book.__dict__)
            outfile.write('\n')

with open('benchmarks.json', 'w') as outfile:
    json.dump(DATA, outfile, indent=2)
#+end_src

** Downloading books for benchmark

#+BEGIN_SRC shell :exports code :tangle scripts/download_benchmark.sh
mkdir test/db
mkdir test/books
wget https://www.gutenberg.org/ebooks/24264.epub.noimages?session_id=13a48cb17a2a788bd0df32eb9d11b2cc90e5ffb6 -O assets/hongloumeng.epub
wget https://www.gutenberg.org/ebooks/6099.epub.noimages?session_id=e525c6c0f4f2faf96f365aabedf179ef08f4f236 -O assets/lesfleursdumal.epub
wget https://www.gutenberg.org/ebooks/21000.epub.noimages?session_id=e525c6c0f4f2faf96f365aabedf179ef08f4f236 -O assets/faust.epub
wget https://www.gutenberg.org/ebooks/23306.epub.noimages?session_id=13a48cb17a2a788bd0df32eb9d11b2cc90e5ffb6 -O assets/meditationes.epub
wget https://www.gutenberg.org/ebooks/2000.epub.noimages?session_id=13a48cb17a2a788bd0df32eb9d11b2cc90e5ffb6 -O assets/Quijote.epub
wget https://www.gutenberg.org/ebooks/521.epub.noimages?session_id=13a48cb17a2a788bd0df32eb9d11b2cc90e5ffb6 -O assets/crusoe.epub
wget https://www.gutenberg.org/ebooks/2701.epub.noimages?session_id=37b8b8ef79424fa1e6b7a18eb4b341d5de076f03 -O assets/moby.epub
wget https://www.gutenberg.org/ebooks/500.epub.noimages?session_id=37b8b8ef79424fa1e6b7a18eb4b341d5de076f03 -O assets/pinocchio.epub
   #+end_src

   #+RESULTS:

** TypeError: Improper input: N=2 must not exceed M=1
   sweep_values = lexical_sweep(my_book.tokens, samples=1)

   sweep_values = lexical_sweep(my_book.tokens, samples=2)
   OptimizeWarning: Covariance of the parameters could not be estimated

* Arguelles Analysis
** Python
*** Whole length
#+NAME: moby
#+BEGIN_SRC python :exports code :session readability_metric :results output
from corpus_analysis import Book

moby_dick = Book("test/moby.epub")
moby_dick.extract_text()
moby_dick.tokenize()
print(moby_dick.title)
with open('moby.tsv', 'w+') as my_file:
    my_file.write(str(moby_dick.word_count) + "\t" + str(moby_dick.unique_words) + "\n")
#+END_SRC

#+RESULTS: moby
: Moby Dick; Or, The Whale

#+NAME: pinocchio
#+BEGIN_SRC python :exports code :session readability_metric :results output
from corpus_analysis import Book

pinocchio = Book("test/pinocchio.epub")
pinocchio.extract_text()
pinocchio.tokenize()
print(pinocchio.title)

with open('pinocchio.tsv', 'w') as my_file:
    my_file.write(str(pinocchio.word_count) + "\t" + str(pinocchio.unique_words) + "\n")
#+END_SRC
#+RESULTS: pinocchio
: The Adventures of Pinocchio
*** Sweep
#+NAME: moby_sweep
#+BEGIN_SRC python :exports code :session readability_metric :results output
from corpus_analysis import Book

moby_dick = Book("test/moby.epub")
moby_dick.extract_text()
moby_dick.tokenize()
print(moby_dick.title)
sweep_values = lexical_sweep(moby_dick.tokens, samples=100, log_x=False, log_y=False)
with open('moby_sweep.tsv', 'w') as my_file:
    for sweep_value in sweep_values:
        my_file.write(str(sweep_value[0]) + "\t" + str(sweep_value[1]) + "\n")
#+END_SRC

#+RESULTS: moby_sweep
: Moby Dick; Or, The Whale

#+NAME: pinocchio_sweep
#+BEGIN_SRC python :exports code :session readability_metric :results output
from corpus_analysis import Book, lexical_sweep

pinocchio = Book("test/pinocchio.epub")
pinocchio.extract_text()
pinocchio.tokenize()
print(pinocchio.title)
sweep_values = lexical_sweep(pinocchio.tokens, samples=100, log_x=False, log_y=False)
with open('pinocchio_sweep.tsv', 'w') as my_file:
    for sweep_value in sweep_values:
        my_file.write(str(sweep_value[0]) + "\t" + str(sweep_value[1]) + "\n")
#+END_SRC

#+RESULTS: pinocchio_sweep
: The Adventures of Pinocchio
** Plot
#+NAME: moby_pinocchio_plot
#+BEGIN_SRC gnuplot :var pinocchio_title=pinocchio moby_title=moby :exports both moby_pinocchio.png
reset
set xrange [0:300000]
set yrange [0:25000]
set key autotitle columnhead
set style line 1 lw 4 lc rgb '#990042' ps 2 pt 6 pi 5
set style line 2 lw 3 lc rgb '#31f120' ps 2 pt 12 pi 3
set title "Lexical variety Vs Length"
set title pinocchio_title
set xlabel "Length in words"
set ylabel "Unique words"
plot "moby.tsv" ls 1 title moby_title, \
     "pinocchio.tsv" ls 2 title pinocchio_title
#+END_SRC

#+RESULTS: moby_pinocchio_plot

#+RESULTS:

#+NAME: moby_pinocchio_sweep_plot
#+BEGIN_SRC gnuplot :var pinocchio_title=pinocchio_sweep moby_title=moby_sweep :exports both :file moby_pinocchio.png
reset
set xrange [4000:400000]
set yrange [1000:40000]
set logscale x
set logscale y
set style line 1 lw 4 lc rgb '#990042' ps 2 pt 6 pi 5
set style line 2 lw 3 lc rgb '#31f120' ps 2 pt 12 pi 3
set title pinocchio_title
set title "Pinocchio and Moby Dick comparison"
#set title "Lexical variety Vs Length"
set xlabel "Length in words"
set ylabel "Unique words"
plot "moby_sweep.tsv" ls 1 title moby_title, \
     "pinocchio_sweep.tsv" ls 2 title pinocchio_title
#+END_SRC

#+RESULTS: moby_pinocchio_sweep_plot
[[file:moby_pinocchio.png]]

#+RESULTS:

#+begin_src gnuplot :exports both file.png
reset

set title "Putting it All Together"

set xlabel "X"
set xrange [-8:8]
set xtics -8,2,8


set ylabel "Y"
set yrange [-20:70]
set ytics -20,10,70

2f(x) = x**2
g(x) = x**3
h(x) = 10*sqrt(abs(x))
i(x) = 15*sin(x)

plot f(x) w lp lw 1, g(x) w p lw 2, h(x) w l lw 3, i(x) w l lw 4
#+end_src

#+RESULTS:

* MongoDB
** Usage
   Connect to data base:
   #+begin_src shell :shebang #!/bin/bash -i :results output
mongo
   #+end_src

   Once inside mongo, select database and filter, for example:
   #+begin_quote
use library
db.corpus.find({}, {"title": 1, _id: 0, creator: 1}).pretty().sort( { "characters_fit.slope": -1 } )
   #+end_quote
** Connect
#+NAME: mongodb_connection
#+begin_src python :noweb yes :session python :exports code
def mongo_connection(database, client="mongodb://localhost:27017/", collection="corpus"):
    myclient = pymongo.MongoClient(client)
    mydb = myclient[database]
    mycol = mydb[collection]
    return myclient, mydb, mycol
#+end_src
** Insert
   #+NAME: insert_book_mongo
   #+begin_src python :noweb yes :session python :exports code
def insert_book_mongo(book, collection):
    collection.insert_one(book.__dict__)
   #+end_src

** Check
#+NAME: check_book_mongo
#+begin_src python :noweb yes :session python :exports code
def is_book_in_mongodb(book, collection):
    try:
        myquery = {"creator": book.creator, "title": book.title}
        mydoc = collection.find_one(myquery)
        if mydoc:
            return True
        return False
    except AttributeError:
        return True
#+end_src

#+begin_src python :results output :session python

import pymongo

myclient = pymongo.MongoClient("mongodb://localhost:27017/")
mydb = myclient["library"]
mycol = mydb["corpus"]

myquery = { "creator": "Carl Collodi", "title": "The Adventures of Pinocchio"}

mydoc = mycol.find_one(myquery, {"creator":True, "title":True, "_id":False})
if mydoc:
    print("Found")
else:
    print("Not found")
print(mydoc)
#+end_src

#+RESULTS:
: Not found
: None

** Backup
#+NAME: backup_mongo
#+begin_src python :noweb yes :session python :exports code
def backup_mongo(db):
    '''
    Write mongo file as json.
    '''
    try:
        backup = subprocess.Popen(["mongodump", "--host", "localhost", "--db",
                                   db])
        # Wait for completion
        backup.communicate()
        if backup.returncode != 0:
            sys.exit(1)
        else:
            print("Dump done for " + db)
    except OSError as ex:
        # Check for errors
        print(ex)
        print("Dump failed for " + db)
#+end_src

#+RESULTS: backup_mongo

** Export to JSON
#+NAME: export_mongo
#+begin_src python :noweb yes :session python :exports code
def export_mongo(db, destination):
    '''
    Write mongo file as json.
    '''
    try:
        backup = subprocess.Popen(["mongoexport", "--host", "localhost", "--db",
                                   db, "--collection", "corpus", "-o", destination, "--jsonArray", "--pretty"])
        # Wait for completion
        backup.communicate()
        if backup.returncode != 0:
            sys.exit(1)
        else:
            print("Export done for " + db)
    except OSError as ex:
        # Check for errors
        print(ex)
        print("Export failed for " + db)
#+end_src

#+RESULTS: export_mongo

** Close
** Issues
*** InvalidDocument: key 'edition.Most' must not contain '.'
    #+begin_src python :results output :session python
from corpus_analysis import Book

my_book = Book("./assets/hongloumeng.epub")
my_book.get_freq_dist()
dir(my_book)
    #+end_src

    #+RESULTS:
    : False
    : False
* Fitting points to function
  The purpose of this section is to fit all the different points to a function
  | Minimum length (characters) |         R^2 |
  |-----------------------------+-------------|
  |                           0 | 0.743868489 |
  |                       20000 |        0.71 |
  |                             |             |
  #+BEGIN_SRC python
for i in xrange(0,lexicalVariety,1000):
  print(i)
  #+END_SRC

  #+RESULTS:

#+BEGIN_SRC gnuplot :exports both :file sweep.png
set multiplot
set encoding utf8
set title "Lexical variety Vs Length"
set xlabel "Length in characters"
set ylabel "Unique characters"
set logscale x
set nologscale y
plot '/home/rcl/readability-metric/test/0936.tsv' title 'Jipin Jiading' linecolor 1, \
     '/home/rcl/readability-metric/test/1077-4000.tsv' title 'Cixi Quanzhuan' linecolor 2
     #'/home/rcl/readability-metric/zh-TW.tsv' title 'Chinese' linecolor 3
unset multiplot
#+END_SRC

#+RESULTS:
[[file:sweep.png]]


#+BEGIN_SRC gnuplot :exports both :file test.png
set multiplot
set encoding utf8
set title "Lexical variety Vs Length"
set xlabel "Length in characters"
set ylabel "Unique characters"
set logscale x
set nologscale y
plot '/home/rcl/readability-metric/zh-TW.tsv' title 'Jipin Jiading' linecolor 1, \
     #'/home/rcl/readability-metric/zh-TW.tsv' title 'Cixi Quanzhuan' linecolor 2
     #'/home/rcl/readability-metric/zh-TW.tsv' title 'Chinese' linecolor 3
unset multiplot
#+END_SRC

#+RESULTS:
[[file:test.png]]

#+BEGIN_SRC R :file R.png :results output graphics
dat <- read.csv("~/readability-metric/zh-TW.tsv", header=FALSE, sep="\t")
x = dat[, 1]
y = dat[, 2]

Estimate = lm(y ~ x)
logEstimate = lm(y ~ log(x))

plot(x,predict(Estimate),type='l',col='blue')
lines(x,predict(logEstimate),col='red')
plot(x, y, log ="x",
        type="p",
        pch = 1,
        xlab="Length (characters)",
        ylab="Unique characters (characters)")
#+END_SRC

#+RESULTS:
[[file:R.png]]

#+begin_src R :file 3.png :results output graphics
library(lattice)
xyplot(1:10 ~ 1:10)
#+end_src

#+RESULTS:
[[file:3.png]]
* Plotting

#+RESULTS:

Perfect. It plots the first two columns and doesn't give an error about all the rest.
#+BEGIN_SRC gnuplot
reset
set title "Lexical variety Vs Length"
set xlabel "Length in words"
set ylabel "Unique words"
set logscale x
set logscale y
es_filelist=system("ls es*.tsv")
fr_filelist=system("ls fr*.tsv")
pt_filelist=system("ls p*.tsv")
plot  for [filename in es_filelist] filename title 'Spanish' linecolor 1, \
      for [filename in fr_filelist] filename title 'French' linecolor 2, \
      for [filename in pt_filelist] filename title 'Portuguese' linecolor 3, \
      'ar.tsv' title 'Arabic' linecolor 4, \
      'zh-TW.tsv' title 'Chinese' linecolor 5
#+END_SRC

#+RESULTS:

#+BEGIN_SRC gnuplot
reset
set title "Lexical variety Vs Length"
set xlabel "Length in characters"
set ylabel "Unique characters"
set logscale x
set nologscale y
plot 'zh-TW.tsv' title 'Chinese' linecolor 1
#+END_SRC

#+RESULTS:

#+BEGIN_SRC gnuplot
reset
set title "Lexical variety Vs Length"
set xlabel "Length in characters"
set ylabel "Unique characters"
set logscale x
set logscale y
plot 'ar.tsv' title 'Arabic' linecolor 1
#+END_SRC

#+BEGIN_SRC gnuplot
reset
set multiplot
set title "Lexical variety Vs Length"
set xlabel "Length in words"
set ylabel "Unique words"
#set logscale x
#set logscale y
set logscale x
set logscale y
filelist=system("ls *.tsv")
#plot  for [filename in filelist] filename title filename
plot 'spanish.tsv' title 'Spanish' linecolor 1, \
     'french.tsv' title 'French' linecolor 2, \
     'portuguese.tsv' title 'Portuguese' linecolor 3, \
     'ar.tsv' title 'Arabic' linecolor 4, \
     for [filename in filelist] filename title filename linecolor 5
unset multiplot
#+END_SRC

#+RESULTS:
[[file:all.png]]
* Tagging
  The purpose of this section is to tag the lists containing the analysis with the canon to which they belong, if appropriate.
  #+begin_src bash :tangle scripts/canon-tagging.sh :exports code
canon="/home/rcl/readability-metric/canon/chinese.txt"
analized="/home/rcl/readability-metric/tagging/zh-TW.tsv"
list=""
while read -r author_canon title_canon; do
        list+=$author_canon
        list+=" "
done < "$canon"
unique_authors=$(tr ' ' '\n' <<< $list | sort -u)
echo $unique_authors
while read -r filesize lexicalVariety intercept slope language author_list title_list type subject source rights relation publisher identifier format contibutor date; do
    flag=0
    while read -r author_canon title_canon; do
        if [ "$author_list" == "$author_canon" ] && [ "$title_list" == "$title_canon" ]; then
            #printf '%s %s Canon match!!\n' "$author_list" "$title_list"
            flag=1
        fi
    done < "$canon"
    for word in $unique_authors; do
        if [ "$author_list" == "$word" ] && [ "$flag" != 1 ]; then
            #printf '%s %s Extended canon match!!\n' "$author_list" "$title_list"
        fi
    done
done < "$analized"
  #+end_src

  #+begin_src bash
linewriting="/home/rcl/readability-metric/linewriting.txt"
touch $linewriting
echo "roland coeurjoly" > $linewriting
echo "chun zhang" >> $linewriting

while read line; do
    if [[ $line = *"chun zhang"* ]]; then
        #echo "substring found!"
        echo
    fi
done < "$linewriting"
less $linewriting
  #+end_src
  #+begin_src python :results output
# -*- coding: utf-8 -*-
import numpy
import csv
canon_file="/home/rcl/readability-metric/canon/chinese.txt"
analysis_file="/home/rcl/readability-metric/tagging/zh-TW.tsv"
canon = numpy.array(list(csv.reader(open(canon_file, "rb"), delimiter=" "))).astype("object")
analysis = numpy.array(list(csv.reader(open(analysis_file, "rb"), delimiter="\t"))).astype("object")
print canon[90][0]
print analysis[90][5]
  #+end_src
  #+RESULTS:
  : 古龍
  : 東野圭吾
#+begin_src python :results output
import json

with open("benchmarks.json", "r") as test_cases:
    benchmarks = json.load(test_cases)
    for benchmark in benchmarks['books']:
        print benchmark['filepath']
        print benchmark['creator']
        print benchmark['title']
        print benchmark['epub_type']
        print benchmark['word_curve_fit_slope']
        print benchmark['zh_character_curve_fit_slope']
        print benchmark['word_count']
        print benchmark['unique_words']
#+end_src


#+RESULTS:
#+begin_example
test/pg23306.epub
René Descartes
Meditationes de prima philosophia

0.803463675366
0
28207
6085
test/pg21000.epub
Johann Wolfgang von Goethe
Faust: Eine Tragödie

0.831561333002
0
36751
9293
test/pg24264.epub
Xueqin Cao
紅樓夢

0.69794400829
373.751162525
662992
21113
test/pg6099.epub
Charles Baudelaire
Les Fleurs du Mal

0.834087803731
0
31525
8177
test/pg2000.epub
Miguel de Cervantes Saavedra
Don Quijote

0.740139477978
0
449755
27284
test/pg521.epub
Daniel Defoe
The Life and Adventures of Robinson Crusoe

0.708038727522
0
141776
7643
test/Las conversaciones privadas de Hitler - Adolf Hitler.epub
Adolf Hitler
Las conversaciones privadas de Hitler

0.774981251067
0
308320
28381
#+end_example
#+begin_src emacs-lisp
(require 'virtualenvwrapper)
(setq venv-location "/home/rcl/readability-metric/env/")
#+end_src

#+RESULTS:
: /home/rcl/readability-metric/env/

#+RESULTS:
|                 |
|                 |
| /usr/bin/python |
#+begin_src python :results output :session python
import sys
print('\n'.join(sys.path))
print(sys.executable)
#+end_src

#+RESULTS:
#+begin_example
/home/rcl/readability-metric/lib/python2.7
/home/rcl/readability-metric/lib/python2.7/plat-x86_64-linux-gnu
/home/rcl/readability-metric/lib/python2.7/lib-tk
/home/rcl/readability-metric/lib/python2.7/lib-old
/home/rcl/readability-metric/lib/python2.7/lib-dynload
/usr/lib/python2.7
/usr/lib/python2.7/plat-x86_64-linux-gnu
/usr/lib/python2.7/lib-tk
/home/rcl/readability-metric/local/lib/python2.7/site-packages
/home/rcl/readability-metric/lib/python2.7/site-packages
/home/rcl/readability-metric/bin/python
#+end_example
